{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNG39NIOSEDLPBZKtqiNP1Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uSnGVL-zodRO"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Structured Data in RAG Systems - Part 1: Setup and Introduction\n","\n","This notebook is Part 1 of our structured data processing series, accompanying\n","Chapter 4 of \"Mastering Retrieval Augmented Generation\". We'll establish our\n","development environment and create sample data for subsequent sections.\n","\n","## What's in this Series\n","1. Part 1 (Current): Setup and Sample Data Creation\n","2. Part 2: CSV Processing and Validation\n","3. Part 3: Directory Management\n","4. Part 4: Hierarchical Data (JSON/XML)\n","5. Part 5: Performance Optimization\n","\"\"\"\n","\n","# First, let's install all necessary packages\n","!pip install pandas numpy chardet xmltodict pytest-cov"]},{"cell_type":"code","source":["\"\"\"## Section 1: Understanding Our Tools\n","\n","Before we start working with structured data, let's import our necessary libraries\n","and understand what each one does in our data processing toolkit.\n","\"\"\"\n","\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import chardet\n","import xml.etree.ElementTree as ET\n","import xmltodict\n","from typing import List, Optional, Any, Dict\n","from datetime import datetime\n","import logging\n","import io\n","import hashlib\n","from pathlib import Path\n","from typing import Optional\n","\n","\n","# Set up logging for our operations\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Section 2: Creating Our Working Directory\n","\n","We'll create a directory to store our sample files that demonstrate different\n","structured data scenarios we'll encounter in real-world applications.\n","\"\"\"\n","\n","# Create directory for our samples\n","!mkdir -p rag_sample_data\n","\n","def create_clean_csv():\n","    \"\"\"\n","    Creates a clean, well-formatted CSV file for basic demonstrations.\n","    This represents an ideal scenario with consistent data.\n","    \"\"\"\n","    sales_data = \"\"\"\n","Date,Product,Quantity,Revenue,Region\n","2025-01-01,Widget A,100,5000,North\n","2025-01-02,Widget B,150,8250,South\n","2025-01-03,Widget A,120,6000,East\n","2025-01-04,Widget C,80,4800,West\n","2025-01-05,Widget B,200,11000,North\n","\"\"\".strip()\n","\n","    with open('rag_sample_data/sales_clean.csv', 'w') as f:\n","        f.write(sales_data)\n","    logger.info(\"Created clean CSV sample\")\n","\n","def create_messy_csv():\n","    \"\"\"\n","    Creates a CSV file with common real-world challenges:\n","    - Different delimiter (semicolon)\n","    - Missing values\n","    - Inconsistent data patterns\n","    \"\"\"\n","    messy_data = \"\"\"\n","Product;Price;Stock;LastUpdated;Supplier\n","Widget A;49.99;100;2025-01-01;Acme Corp\n","Widget B;55.00;;2025-01-02;TechSupply\n","Widget C;75.50;75;;GlobalParts\n","Widget D;;;2025-01-04;Acme Corp\n","\"\"\".strip()\n","\n","    with open('rag_sample_data/inventory_messy.csv', 'w') as f:\n","        f.write(messy_data)\n","    logger.info(\"Created messy CSV sample\")\n","\n","def create_sample_json():\n","    \"\"\"\n","    Creates a JSON file with nested structures and various data types.\n","    This demonstrates hierarchical data relationships.\n","    \"\"\"\n","    product_catalog = {\n","        \"catalog\": {\n","            \"last_updated\": \"2025-01-07T10:00:00Z\",\n","            \"categories\": [\n","                {\n","                    \"name\": \"Electronics\",\n","                    \"products\": [\n","                        {\n","                            \"id\": \"E001\",\n","                            \"name\": \"Smart Watch\",\n","                            \"price\": 199.99,\n","                            \"specifications\": {\n","                                \"battery\": \"24h\",\n","                                \"waterproof\": True,\n","                                \"sensors\": [\"heart rate\", \"GPS\"]\n","                            }\n","                        }\n","                    ]\n","                }\n","            ]\n","        }\n","    }\n","\n","    with open('rag_sample_data/product_catalog.json', 'w') as f:\n","        json.dump(product_catalog, f, indent=2)\n","    logger.info(\"Created JSON sample\")\n","\n","def create_sample_xml():\n","    \"\"\"\n","    Creates an XML file demonstrating structured document format.\n","    Shows nested elements and attributes.\n","    \"\"\"\n","    xml_data = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n","<inventory last_updated=\"2025-01-07\">\n","    <warehouse location=\"North\">\n","        <product>\n","            <sku>W001</sku>\n","            <name>Widget Pro</name>\n","            <stock>150</stock>\n","        </product>\n","    </warehouse>\n","</inventory>\n","\"\"\"\n","\n","    with open('rag_sample_data/inventory.xml', 'w') as f:\n","        f.write(xml_data)\n","    logger.info(\"Created XML sample\")\n","\n","# Create all our sample files\n","create_clean_csv()\n","create_messy_csv()\n","create_sample_json()\n","create_sample_xml()\n","\n","# Verify creation and show file sizes\n","print(\"\\nSample files created in 'rag_sample_data' directory:\")\n","for file in os.listdir('rag_sample_data'):\n","    size = os.path.getsize(os.path.join('rag_sample_data', file))\n","    print(f\"- {file}: {size} bytes\")\n","\n","\"\"\"## What's Next?\n","\n","In Part 2 of this series, we'll implement a robust CSV processor that can handle:\n","- Automatic encoding detection\n","- Different delimiters\n","- Missing value handling\n","- Data validation\n","- Error recovery\n","\n","We'll use the sample files we've created here to test our implementation against\n","both ideal and challenging scenarios.\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"nEBIw0oZubAC","executionInfo":{"status":"ok","timestamp":1739023687917,"user_tz":-330,"elapsed":126,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"9bc4e317-ca74-4a4d-9860-4f93c83f7352"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample files created in 'rag_sample_data' directory:\n","- product_catalog.json: 492 bytes\n","- inventory_messy.csv: 181 bytes\n","- sales_clean.csv: 209 bytes\n","- inventory.xml: 272 bytes\n"]},{"output_type":"execute_result","data":{"text/plain":["\"## What's Next?\\n\\nIn Part 2 of this series, we'll implement a robust CSV processor that can handle:\\n- Automatic encoding detection\\n- Different delimiters\\n- Missing value handling\\n- Data validation\\n- Error recovery\\n\\nWe'll use the sample files we've created here to test our implementation against\\nboth ideal and challenging scenarios.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Structured Data in RAG Systems - Part 2: CSV Processing\n","\n","This notebook demonstrates how to build a robust CSV processor for RAG systems.\n","We'll create a comprehensive solution that can handle common challenges in\n","real-world CSV files.\n","\n","Note: Make sure you've run Part 1 first to create the sample data files we'll use here.\n","\"\"\"\n","\n","# First, let's install and import our required packages\n","!pip install pandas numpy chardet\n","\n","import pandas as pd\n","import numpy as np\n","import chardet\n","import os\n","import json\n","from typing import List, Optional, Dict, Any\n","from datetime import datetime\n","import logging\n","\n","# Set up logging for better visibility into our operations\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","class EnhancedCSVLoader:\n","    \"\"\"\n","    A robust CSV loader designed for RAG systems that can handle:\n","    - Different encodings\n","    - Various delimiters\n","    - Missing values\n","    - Data validation\n","    - Error recovery\n","    \"\"\"\n","\n","    def __init__(self, file_path: str):\n","        \"\"\"\n","        Initialize the CSV loader with configuration options.\n","\n","        Args:\n","            file_path: Path to the CSV file to process\n","        \"\"\"\n","        self.file_path = file_path\n","        self.metadata = {}\n","        self.delimiter = None\n","        self.encoding = None\n","\n","    def detect_file_properties(self) -> dict:\n","        \"\"\"\n","        Automatically detect CSV file properties including encoding and delimiter.\n","        This helps handle files from various sources reliably.\n","\n","        Returns:\n","            Dictionary containing detected file properties\n","        \"\"\"\n","        try:\n","            with open(self.file_path, 'rb') as file:\n","                # Read a sample of the file for detection\n","                raw_data = file.read(10000)  # Read first 10KB\n","\n","                # Detect encoding\n","                result = chardet.detect(raw_data)\n","                self.encoding = result['encoding']\n","\n","                # Convert to string for delimiter detection\n","                sample_text = raw_data.decode(self.encoding)\n","\n","                # Count potential delimiters\n","                delimiters = [',', ';', '\\t', '|']\n","                delimiter_counts = {d: sample_text.count(d) for d in delimiters}\n","\n","                # Choose the most common delimiter\n","                self.delimiter = max(delimiter_counts.items(), key=lambda x: x[1])[0]\n","\n","                return {\n","                    'encoding': self.encoding,\n","                    'delimiter': self.delimiter,\n","                    'confidence': result['confidence']\n","                }\n","        except Exception as e:\n","            logger.error(f\"Error detecting file properties: {str(e)}\")\n","            raise\n","\n","    def validate_data(self, df: pd.DataFrame) -> dict:\n","        \"\"\"\n","        Perform validation checks on the loaded data.\n","\n","        Args:\n","            df: Pandas DataFrame containing the loaded CSV data\n","\n","        Returns:\n","            Dictionary containing validation results and statistics\n","        \"\"\"\n","        try:\n","            validation = {\n","                'total_rows': len(df),\n","                'total_columns': len(df.columns),\n","                'missing_values': df.isnull().sum().to_dict(),\n","                'column_types': df.dtypes.astype(str).to_dict(),\n","                'duplicate_rows': df.duplicated().sum(),\n","                'column_statistics': {}\n","            }\n","\n","            # Calculate statistics for numeric columns\n","            numeric_columns = df.select_dtypes(include=[np.number]).columns\n","            for col in numeric_columns:\n","                validation['column_statistics'][col] = {\n","                    'mean': float(df[col].mean()),  # Convert numpy types to Python types\n","                    'std': float(df[col].std()),\n","                    'min': float(df[col].min()),\n","                    'max': float(df[col].max())\n","                }\n","\n","            # Check for potential data quality issues\n","            validation['quality_checks'] = {\n","                'empty_columns': [col for col in df.columns if df[col].isnull().all()],\n","                'high_missing_ratio': [\n","                    col for col in df.columns\n","                    if df[col].isnull().sum() / len(df) > 0.5\n","                ],\n","                'constant_columns': [\n","                    col for col in df.columns\n","                    if df[col].nunique() == 1\n","                ]\n","            }\n","\n","            return validation\n","\n","        except Exception as e:\n","            logger.error(f\"Error during data validation: {str(e)}\")\n","            raise\n","\n","    def load(self) -> tuple[pd.DataFrame, dict]:\n","        \"\"\"\n","        Load and process the CSV file, returning both the data and metadata.\n","        Implements comprehensive error handling and data validation.\n","\n","        Returns:\n","            Tuple containing:\n","            - Pandas DataFrame with the loaded data\n","            - Dictionary with metadata and validation results\n","        \"\"\"\n","        try:\n","            # First check if file exists\n","            if not os.path.exists(self.file_path):\n","                raise FileNotFoundError(f\"File not found: {self.file_path}\")\n","\n","            # Detect file properties\n","            properties = self.detect_file_properties()\n","            logger.info(f\"Detected file properties: {properties}\")\n","\n","            # Read CSV with detected properties\n","            df = pd.read_csv(\n","                self.file_path,\n","                encoding=self.encoding,\n","                delimiter=self.delimiter,\n","                on_bad_lines='warn'\n","            )\n","\n","            # Validate the data\n","            validation_results = self.validate_data(df)\n","            logger.info(\"Data validation completed\")\n","\n","            # Collect metadata\n","            self.metadata = {\n","                'file_properties': properties,\n","                'validation': validation_results,\n","                'file_size': os.path.getsize(self.file_path),\n","                'last_modified': datetime.fromtimestamp(\n","                    os.path.getmtime(self.file_path)\n","                ).isoformat(),\n","                'columns': list(df.columns)\n","            }\n","\n","            return df, self.metadata\n","\n","        except Exception as e:\n","            logger.error(f\"Error loading CSV: {str(e)}\")\n","            raise\n","\n","\"\"\"## Testing Our Implementation\n","\n","Let's test our CSV loader with both the clean and messy sample files we created\n","in Part 1. This will demonstrate how it handles different scenarios.\n","\"\"\"\n","\n","def test_csv_loader():\n","    \"\"\"\n","    Test the CSV loader with different sample files and analyze the results.\n","    \"\"\"\n","    try:\n","        # Test 1: Process the clean CSV\n","        print(\"Testing Clean CSV Processing\")\n","        print(\"-\" * 50)\n","\n","        loader = EnhancedCSVLoader(\"rag_sample_data/sales_clean.csv\")\n","        df, metadata = loader.load()\n","\n","        print(\"Clean CSV Preview:\")\n","        print(df.head())\n","        print(\"\\nValidation Results:\")\n","        print(json.dumps(metadata['validation'], indent=2))\n","\n","        # Test 2: Process the messy CSV\n","        print(\"\\nTesting Messy CSV Processing\")\n","        print(\"-\" * 50)\n","\n","        loader = EnhancedCSVLoader(\"rag_sample_data/inventory_messy.csv\")\n","        df, metadata = loader.load()\n","\n","        print(\"Messy CSV Preview:\")\n","        print(df.head())\n","        print(\"\\nValidation Results:\")\n","        print(json.dumps(metadata['validation'], indent=2))\n","\n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","\n","# Run our tests\n","test_csv_loader()\n","\n","\"\"\"## What's Next?\n","\n","In Part 3, we'll explore directory management for handling collections of\n","structured data files. We'll build upon the CSV processing capabilities\n","we've developed here to handle multiple files efficiently.\"\"\""],"metadata":{"id":"Oae_CMtVwxcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Structured Data in RAG Systems - Part 3: Directory Management\n","\n","This notebook focuses on managing collections of structured data files in RAG systems.\n","We'll build a robust system for organizing, tracking, and processing multiple files\n","efficiently. This is particularly important when dealing with large document collections\n","or regularly updated datasets.\n","\n","Make sure you've run Parts 1 and 2 first, as we'll build upon the CSV processing\n","capabilities we developed there.\n","\"\"\"\n","\n","# Import required libraries\n","import os\n","import json\n","import pandas as pd\n","import hashlib\n","from datetime import datetime\n","from typing import List, Dict, Optional, Any\n","import logging\n","from pathlib import Path\n","import shutil\n","\n","# Set up logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Section 1: Building a Document Collection Manager\n","\n","First, let's create a comprehensive system for managing collections of structured\n","data files. This will help us organize files, track changes, and maintain metadata\n","about our document collection.\n","\"\"\"\n","\n","class DocumentCollectionManager:\n","    \"\"\"\n","    Manages collections of structured data files with features for:\n","    - Organizing files by type and category\n","    - Tracking file changes and versions\n","    - Maintaining a searchable index\n","    - Processing files in batches\n","    \"\"\"\n","\n","    def __init__(self, root_dir: str):\n","        \"\"\"\n","        Initialize the document collection manager.\n","\n","        Args:\n","            root_dir: Root directory for the document collection\n","        \"\"\"\n","        self.root_dir = Path(root_dir)\n","        self.file_index = {}\n","        self.metadata = {}\n","\n","        # Create directory structure if it doesn't exist\n","        self._initialize_directory_structure()\n","\n","    def _initialize_directory_structure(self):\n","        \"\"\"\n","        Creates the necessary directory structure for organizing files.\n","        We'll create separate directories for different file types and\n","        maintain an index directory for metadata.\n","        \"\"\"\n","        # Create main directories\n","        directories = [\n","            'csv_files',\n","            'json_files',\n","            'xml_files',\n","            'index',\n","            'archive'\n","        ]\n","\n","        for dir_name in directories:\n","            dir_path = self.root_dir / dir_name\n","            dir_path.mkdir(parents=True, exist_ok=True)\n","            logger.info(f\"Initialized directory: {dir_path}\")\n","\n","    def _calculate_file_hash(self, file_path: Path) -> str:\n","        \"\"\"\n","        Calculate a hash of the file contents for change detection.\n","        This helps us track when files have been modified.\n","\n","        Args:\n","            file_path: Path to the file\n","\n","        Returns:\n","            SHA-256 hash of the file contents\n","        \"\"\"\n","        hasher = hashlib.sha256()\n","        with open(file_path, 'rb') as f:\n","            for chunk in iter(lambda: f.read(4096), b''):\n","                hasher.update(chunk)\n","        return hasher.hexdigest()\n","\n","    def _get_file_metadata(self, file_path: Path) -> dict:\n","        \"\"\"\n","        Extract comprehensive metadata about a file.\n","        This includes file properties, modification times, and content hash.\n","\n","        Args:\n","            file_path: Path to the file\n","\n","        Returns:\n","            Dictionary containing file metadata\n","        \"\"\"\n","        stats = file_path.stat()\n","        return {\n","            'filename': file_path.name,\n","            'extension': file_path.suffix.lower(),\n","            'size': stats.st_size,\n","            'created': datetime.fromtimestamp(stats.st_ctime).isoformat(),\n","            'modified': datetime.fromtimestamp(stats.st_mtime).isoformat(),\n","            'content_hash': self._calculate_file_hash(file_path),\n","            'relative_path': str(file_path.relative_to(self.root_dir))\n","        }\n","\n","    def add_file(self, source_path: str, category: Optional[str] = None) -> dict:\n","        \"\"\"\n","        Add a new file to the collection, organizing it appropriately.\n","\n","        Args:\n","            source_path: Path to the file to add\n","            category: Optional category for organizing files\n","\n","        Returns:\n","            Metadata about the added file\n","        \"\"\"\n","        source_path = Path(source_path)\n","        file_ext = source_path.suffix.lower()\n","\n","        # Determine target directory based on file type\n","        if file_ext == '.csv':\n","            target_dir = self.root_dir / 'csv_files'\n","        elif file_ext == '.json':\n","            target_dir = self.root_dir / 'json_files'\n","        elif file_ext == '.xml':\n","            target_dir = self.root_dir / 'xml_files'\n","        else:\n","            raise ValueError(f\"Unsupported file type: {file_ext}\")\n","\n","        # Add category subdirectory if specified\n","        if category:\n","            target_dir = target_dir / category\n","            target_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Copy file to collection\n","        target_path = target_dir / source_path.name\n","        shutil.copy2(source_path, target_path)\n","\n","        # Extract and store metadata\n","        metadata = self._get_file_metadata(target_path)\n","        self.file_index[str(target_path)] = metadata\n","\n","        # Save updated index\n","        self._save_index()\n","\n","        logger.info(f\"Added file: {source_path.name} to {target_dir}\")\n","        return metadata\n","\n","    def _save_index(self):\n","        \"\"\"\n","        Save the file index to disk for persistence.\n","        This maintains a record of all files and their metadata.\n","        \"\"\"\n","        index_path = self.root_dir / 'index' / 'file_index.json'\n","        with open(index_path, 'w') as f:\n","            json.dump(self.file_index, f, indent=2)\n","\n","    def scan_directory(self) -> dict:\n","        \"\"\"\n","        Scan the entire directory structure and update the file index.\n","        This helps maintain an accurate record of all files in the collection.\n","\n","        Returns:\n","            Statistics about the document collection\n","        \"\"\"\n","        total_files = 0\n","        total_size = 0\n","        file_types = {}\n","\n","        # Scan all subdirectories\n","        for root, _, files in os.walk(self.root_dir):\n","            root_path = Path(root)\n","\n","            # Skip index and archive directories\n","            if 'index' in root_path.parts or 'archive' in root_path.parts:\n","                continue\n","\n","            for file in files:\n","                file_path = root_path / file\n","\n","                # Update file index\n","                self.file_index[str(file_path)] = self._get_file_metadata(file_path)\n","\n","                # Update statistics\n","                total_files += 1\n","                total_size += file_path.stat().st_size\n","                file_types[file_path.suffix] = file_types.get(file_path.suffix, 0) + 1\n","\n","        # Save updated index\n","        self._save_index()\n","\n","        return {\n","            'total_files': total_files,\n","            'total_size': total_size,\n","            'file_types': file_types,\n","            'last_scan': datetime.now().isoformat()\n","        }\n","\n","\"\"\"## Section 2: Testing Our Implementation\n","\n","Let's test our document collection manager with the sample files we created\n","in Part 1. This will demonstrate how it handles different file types and\n","maintains organization.\n","\"\"\"\n","\n","def test_document_manager():\n","    \"\"\"\n","    Test the document collection manager with our sample files.\n","    This demonstrates the key features of file organization and tracking.\n","    \"\"\"\n","    # Create a test collection\n","    collection_dir = Path(\"rag_test_collection\")\n","    manager = DocumentCollectionManager(collection_dir)\n","\n","    # Add our sample files\n","    sample_files = [\n","        (\"rag_sample_data/sales_clean.csv\", \"sales\"),\n","        (\"rag_sample_data/inventory_messy.csv\", \"inventory\"),\n","        (\"rag_sample_data/product_catalog.json\", \"products\"),\n","        (\"rag_sample_data/inventory.xml\", \"inventory\")\n","    ]\n","\n","    print(\"Adding sample files to collection...\")\n","    for file_path, category in sample_files:\n","        metadata = manager.add_file(file_path, category)\n","        print(f\"\\nAdded {metadata['filename']}:\")\n","        print(json.dumps(metadata, indent=2))\n","\n","    # Scan the directory and show statistics\n","    print(\"\\nCollection Statistics:\")\n","    stats = manager.scan_directory()\n","    print(json.dumps(stats, indent=2))\n","\n","# Run our tests\n","test_document_manager()\n","\n","\"\"\"## Section 3: Batch Processing Implementation\n","\n","Now let's implement batch processing capabilities for handling multiple files\n","efficiently. This is particularly useful when working with large collections\n","or when files need regular updates.\n","\"\"\"\n","\n","class BatchProcessor:\n","    \"\"\"\n","    Processes multiple structured data files in batches with support for:\n","    - Parallel processing\n","    - Progress tracking\n","    - Error handling and recovery\n","    \"\"\"\n","\n","    def __init__(self, collection_manager: DocumentCollectionManager):\n","        \"\"\"\n","        Initialize the batch processor.\n","\n","        Args:\n","            collection_manager: Instance of DocumentCollectionManager\n","        \"\"\"\n","        self.collection_manager = collection_manager\n","        self.processing_stats = {\n","            'processed': 0,\n","            'failed': 0,\n","            'skipped': 0\n","        }\n","\n","    def process_files(self, file_type: str, processor_func: callable) -> dict:\n","        \"\"\"\n","        Process all files of a specific type using the provided function.\n","\n","        Args:\n","            file_type: File extension to process (e.g., '.csv')\n","            processor_func: Function to apply to each file\n","\n","        Returns:\n","            Processing statistics and results\n","        \"\"\"\n","        results = []\n","\n","        # Get all files of specified type\n","        files_to_process = [\n","            path for path in self.collection_manager.file_index\n","            if Path(path).suffix.lower() == file_type\n","        ]\n","\n","        logger.info(f\"Starting batch processing of {len(files_to_process)} files\")\n","\n","        # Process each file\n","        for file_path in files_to_process:\n","            try:\n","                # Process the file\n","                result = processor_func(file_path)\n","                results.append({\n","                    'file': file_path,\n","                    'status': 'success',\n","                    'result': result\n","                })\n","                self.processing_stats['processed'] += 1\n","\n","            except Exception as e:\n","                logger.error(f\"Error processing {file_path}: {str(e)}\")\n","                results.append({\n","                    'file': file_path,\n","                    'status': 'failed',\n","                    'error': str(e)\n","                })\n","                self.processing_stats['failed'] += 1\n","\n","        return {\n","            'stats': self.processing_stats,\n","            'results': results\n","        }\n","\n","\"\"\"## Example: Batch Processing CSV Files\n","\n","Let's demonstrate batch processing by analyzing all CSV files in our collection.\n","We'll calculate summary statistics for each file.\n","\"\"\"\n","\n","def csv_analyzer(file_path: str) -> dict:\n","    \"\"\"\n","    Analyze a CSV file and return summary statistics.\n","    This is an example processor function for batch processing.\n","\n","    Args:\n","        file_path: Path to the CSV file\n","\n","    Returns:\n","        Dictionary containing analysis results\n","    \"\"\"\n","    # Use our EnhancedCSVLoader from Part 2\n","    from Part2 import EnhancedCSVLoader\n","\n","    loader = EnhancedCSVLoader(file_path)\n","    df, metadata = loader.load()\n","\n","    # Calculate additional statistics\n","    analysis = {\n","        'row_count': len(df),\n","        'column_count': len(df.columns),\n","        'numeric_columns': {\n","            col: {\n","                'mean': df[col].mean(),\n","                'std': df[col].std(),\n","                'min': df[col].min(),\n","                'max': df[col].max()\n","            }\n","            for col in df.select_dtypes(include=[np.number]).columns\n","        }\n","    }\n","\n","    return analysis\n","\n","# Test batch processing\n","collection_dir = Path(\"rag_test_collection\")\n","manager = DocumentCollectionManager(collection_dir)\n","processor = BatchProcessor(manager)\n","\n","print(\"Starting batch analysis of CSV files...\")\n","results = processor.process_files('.csv', csv_analyzer)\n","print(\"\\nProcessing Results:\")\n","print(json.dumps(results, indent=2))\n","\n","\"\"\"## What's Next?\n","\n","In Part 4, we'll explore processing hierarchical data formats (JSON and XML),\n","building upon our directory management capabilities. We'll implement specialized\n","processors for these formats and show how to integrate them with our batch\n","processing system.\"\"\""],"metadata":{"id":"r7GMfYi0w62n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Structured Data in RAG Systems - Part 4: Hierarchical Data Processing\n","\n","This notebook focuses on processing hierarchical data formats (JSON and XML) in RAG systems.\n","We'll build robust processors that can handle complex nested structures while maintaining\n","data relationships and context. This is particularly important when working with API\n","responses, configuration files, and structured documents.\n","\n","Make sure you've run the previous parts first, as we'll build upon concepts introduced\n","there and use some of the sample files we created.\n","\"\"\"\n","\n","# First, let's install and import our required packages\n","!pip install pandas numpy xmltodict jsonpath-ng\n","\n","import json\n","import xml.etree.ElementTree as ET\n","import xmltodict\n","from typing import List, Dict, Any, Optional\n","import logging\n","from pathlib import Path\n","from jsonpath_ng import jsonpath, parse\n","import pandas as pd\n","from datetime import datetime\n","\n","# Set up logging for better visibility into our operations\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Understanding Hierarchical Data Structures\n","\n","Before we dive into implementation, let's understand why hierarchical data requires\n","special handling. Consider our product catalog JSON from Part 1:\n","\n","1. It has multiple levels of nesting (catalog → categories → products)\n","2. It contains mixed data types (strings, numbers, booleans, arrays)\n","3. Some fields might be optional or have varying structures\n","4. Relationships between elements are important for context\n","\n","Our implementation needs to handle these complexities while making the data\n","accessible for RAG systems.\n","\"\"\"\n","\n","class HierarchicalDataProcessor:\n","    \"\"\"\n","    A comprehensive processor for hierarchical data formats (JSON and XML).\n","    Handles complex nested structures while preserving relationships and context.\n","    \"\"\"\n","\n","    def __init__(self, file_path: str):\n","        \"\"\"\n","        Initialize the hierarchical data processor.\n","\n","        Args:\n","            file_path: Path to the JSON or XML file to process\n","        \"\"\"\n","        self.file_path = Path(file_path)\n","        self.file_type = self.file_path.suffix.lower()\n","        self.data = None\n","        self.metadata = {}\n","\n","        if self.file_type not in ['.json', '.xml']:\n","            raise ValueError(f\"Unsupported file type: {self.file_type}\")\n","\n","    def _calculate_depth(self, obj: Any, current_depth: int = 0) -> int:\n","        \"\"\"\n","        Calculate the maximum nesting depth of a hierarchical structure.\n","        This helps us understand the complexity of our data.\n","\n","        Args:\n","            obj: The object to analyze\n","            current_depth: Current nesting level\n","\n","        Returns:\n","            Maximum nesting depth found\n","        \"\"\"\n","        if isinstance(obj, dict):\n","            if not obj:\n","                return current_depth\n","            return max(self._calculate_depth(v, current_depth + 1) for v in obj.values())\n","        elif isinstance(obj, list):\n","            if not obj:\n","                return current_depth\n","            return max(self._calculate_depth(item, current_depth + 1) for item in obj)\n","        else:\n","            return current_depth\n","\n","    def _extract_schema(self, obj: Any, path: str = '') -> Dict:\n","        \"\"\"\n","        Extract the implicit schema from the data structure.\n","        This helps understand the structure and relationships in our data.\n","\n","        Args:\n","            obj: The object to analyze\n","            path: Current path in the hierarchy\n","\n","        Returns:\n","            Dictionary describing the data structure\n","        \"\"\"\n","        if isinstance(obj, dict):\n","            return {\n","                'type': 'object',\n","                'properties': {\n","                    k: self._extract_schema(v, f\"{path}.{k}\" if path else k)\n","                    for k, v in obj.items()\n","                }\n","            }\n","        elif isinstance(obj, list):\n","            if obj:\n","                # Analyze the first item as an example\n","                return {\n","                    'type': 'array',\n","                    'items': self._extract_schema(obj[0], f\"{path}[]\")\n","                }\n","            return {'type': 'array', 'items': {}}\n","        else:\n","            return {'type': type(obj).__name__}\n","\n","    def load(self) -> tuple[Any, dict]:\n","        \"\"\"\n","        Load and process the hierarchical data file.\n","\n","        Returns:\n","            Tuple containing:\n","            - The processed data structure\n","            - Dictionary with metadata and analysis results\n","        \"\"\"\n","        try:\n","            # Read and parse the file based on its type\n","            if self.file_type == '.json':\n","                with open(self.file_path, 'r', encoding='utf-8') as f:\n","                    self.data = json.load(f)\n","            else:  # XML\n","                with open(self.file_path, 'r', encoding='utf-8') as f:\n","                    self.data = xmltodict.parse(f.read())\n","\n","            # Analyze the structure\n","            depth = self._calculate_depth(self.data)\n","            schema = self._extract_schema(self.data)\n","\n","            # Collect metadata\n","            self.metadata = {\n","                'file_info': {\n","                    'name': self.file_path.name,\n","                    'size': self.file_path.stat().st_size,\n","                    'last_modified': datetime.fromtimestamp(\n","                        self.file_path.stat().st_mtime\n","                    ).isoformat()\n","                },\n","                'structure': {\n","                    'max_depth': depth,\n","                    'schema': schema\n","                }\n","            }\n","\n","            return self.data, self.metadata\n","\n","        except Exception as e:\n","            logger.error(f\"Error processing {self.file_path}: {str(e)}\")\n","            raise\n","\n","    def query(self, path_expr: str) -> List[Any]:\n","        \"\"\"\n","        Query the data structure using JSONPath expressions.\n","        This provides a flexible way to extract specific information.\n","\n","        Args:\n","            path_expr: JSONPath expression to evaluate\n","\n","        Returns:\n","            List of matching elements\n","        \"\"\"\n","        if self.data is None:\n","            raise ValueError(\"Data not loaded. Call load() first.\")\n","\n","        jsonpath_expr = parse(path_expr)\n","        return [match.value for match in jsonpath_expr.find(self.data)]\n","\n","    def to_tabular(self, path_expr: str, columns: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Convert a portion of the hierarchical data to tabular format.\n","        This is useful when working with structured sections of the data.\n","\n","        Args:\n","            path_expr: JSONPath expression to locate the records\n","            columns: List of column names to extract\n","\n","        Returns:\n","            Pandas DataFrame containing the extracted data\n","        \"\"\"\n","        records = self.query(path_expr)\n","\n","        # Extract specified columns from each record\n","        tabular_data = []\n","        for record in records:\n","            row = {}\n","            for col in columns:\n","                try:\n","                    # Handle nested paths in column names\n","                    if '.' in col:\n","                        parts = col.split('.')\n","                        value = record\n","                        for part in parts:\n","                            value = value.get(part, None)\n","                    else:\n","                        value = record.get(col, None)\n","                    row[col] = value\n","                except AttributeError:\n","                    row[col] = None\n","            tabular_data.append(row)\n","\n","        return pd.DataFrame(tabular_data)\n","\n","\"\"\"## Testing Our Implementation\n","\n","Let's test our hierarchical data processor with the sample files we created in Part 1.\n","This will demonstrate how it handles different types of nested structures.\n","\"\"\"\n","\n","def test_hierarchical_processor():\n","    \"\"\"\n","    Test the hierarchical data processor with our sample files.\n","    This demonstrates processing of both JSON and XML data.\n","    \"\"\"\n","    try:\n","        # Test 1: Process JSON catalog\n","        print(\"Testing JSON Processing\")\n","        print(\"-\" * 50)\n","\n","        json_processor = HierarchicalDataProcessor(\"rag_sample_data/product_catalog.json\")\n","        json_data, json_metadata = json_processor.load()\n","\n","        print(\"JSON Structure Analysis:\")\n","        print(json.dumps(json_metadata['structure'], indent=2))\n","\n","        # Extract product information using JSONPath\n","        print(\"\\nProduct Information:\")\n","        products = json_processor.query(\"$.catalog.categories[*].products[*]\")\n","        for product in products:\n","            print(f\"- {product['name']}: ${product['price']}\")\n","\n","        # Convert products to tabular format\n","        columns = ['id', 'name', 'price']\n","        df = json_processor.to_tabular(\n","            \"$.catalog.categories[*].products[*]\",\n","            columns\n","        )\n","        print(\"\\nTabular Product Data:\")\n","        print(df)\n","\n","        # Test 2: Process XML inventory\n","        print(\"\\nTesting XML Processing\")\n","        print(\"-\" * 50)\n","\n","        xml_processor = HierarchicalDataProcessor(\"rag_sample_data/inventory.xml\")\n","        xml_data, xml_metadata = xml_processor.load()\n","\n","        print(\"XML Structure Analysis:\")\n","        print(json.dumps(xml_metadata['structure'], indent=2))\n","\n","        # Query warehouse information\n","        warehouses = xml_processor.query(\"$.inventory.warehouse\")\n","        print(\"\\nWarehouse Information:\")\n","        for warehouse in warehouses:\n","            print(f\"Location: {warehouse['@location']}\")\n","            if 'product' in warehouse:\n","                products = warehouse['product']\n","                if not isinstance(products, list):\n","                    products = [products]\n","                for product in products:\n","                    print(f\"- {product['name']}: {product['stock']} in stock\")\n","\n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","\n","# Run our tests\n","test_hierarchical_processor()\n","\n","\"\"\"## Building a Search Index\n","\n","For RAG systems, we often need to search through hierarchical data efficiently.\n","Let's implement a search index that makes our nested data searchable.\n","\"\"\"\n","\n","class HierarchicalSearchIndex:\n","    \"\"\"\n","    Creates a searchable index for hierarchical data structures.\n","    This makes it efficient to find information across nested documents.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the search index.\"\"\"\n","        self.index = {}\n","        self.documents = {}\n","\n","    def _extract_text(self, obj: Any, path: List[str] = None) -> List[tuple[str, List[str]]]:\n","        \"\"\"\n","        Recursively extract text content from nested structures.\n","        Maintains the path to each piece of text for context.\n","\n","        Args:\n","            obj: Object to process\n","            path: Current path in the hierarchy\n","\n","        Returns:\n","            List of (text, path) tuples\n","        \"\"\"\n","        if path is None:\n","            path = []\n","\n","        results = []\n","\n","        if isinstance(obj, dict):\n","            for key, value in obj.items():\n","                current_path = path + [key]\n","                if isinstance(value, (dict, list)):\n","                    results.extend(self._extract_text(value, current_path))\n","                else:\n","                    results.append((str(value), current_path))\n","        elif isinstance(obj, list):\n","            for i, item in enumerate(obj):\n","                current_path = path + [f\"[{i}]\"]\n","                results.extend(self._extract_text(item, current_path))\n","        else:\n","            results.append((str(obj), path))\n","\n","        return results\n","\n","    def add_document(self, doc_id: str, content: Any):\n","        \"\"\"\n","        Add a document to the search index.\n","\n","        Args:\n","            doc_id: Unique identifier for the document\n","            content: The hierarchical data structure to index\n","        \"\"\"\n","        # Store the original document\n","        self.documents[doc_id] = content\n","\n","        # Extract and index all text content\n","        text_items = self._extract_text(content)\n","\n","        for text, path in text_items:\n","            # Create tokens from the text\n","            tokens = text.lower().split()\n","\n","            # Index each token\n","            for token in tokens:\n","                if token not in self.index:\n","                    self.index[token] = {}\n","                if doc_id not in self.index[token]:\n","                    self.index[token][doc_id] = []\n","                self.index[token][doc_id].append(path)\n","\n","    def search(self, query: str) -> Dict[str, List[List[str]]]:\n","        \"\"\"\n","        Search the index for documents matching the query.\n","\n","        Args:\n","            query: Search terms\n","\n","        Returns:\n","            Dictionary mapping document IDs to lists of matching paths\n","        \"\"\"\n","        tokens = query.lower().split()\n","        results = {}\n","\n","        # Find documents containing all search terms\n","        for token in tokens:\n","            if token in self.index:\n","                for doc_id, paths in self.index[token].items():\n","                    if doc_id not in results:\n","                        results[doc_id] = paths\n","                    else:\n","                        # Combine paths for multiple matching terms\n","                        results[doc_id].extend(paths)\n","\n","        return results\n","\n","\"\"\"## Testing the Search Index\n","\n","Let's test our search capabilities with our sample hierarchical data.\n","\"\"\"\n","\n","def test_search_index():\n","    \"\"\"Test the hierarchical search index with our sample documents.\"\"\"\n","    # Create and populate the search index\n","    search_index = HierarchicalSearchIndex()\n","\n","    # Add our JSON and XML samples\n","    json_processor = HierarchicalDataProcessor(\"rag_sample_data/product_catalog.json\")\n","    xml_processor = HierarchicalDataProcessor(\"rag_sample_data/inventory.xml\")\n","\n","    json_data, _ = json_processor.load()\n","    xml_data, _ = xml_processor.load()\n","\n","    search_index.add_document(\"product_catalog\", json_data)\n","    search_index.add_document(\"inventory\", xml_data)\n","\n","    # Perform some test searches\n","    test_queries = [\n","        \"watch\",\n","        \"warehouse\",\n","        \"stock\",\n","        \"GPS\"\n","    ]\n","\n","    print(\"Search Results:\")\n","    print(\"-\" * 50)\n","\n","    for query in test_queries:\n","        print(f\"\\nSearching for: {query}\")\n","        results = search_index.search(query)\n","\n","        for doc_id, paths in results.items():\n","            print(f\"\\nDocument: {doc_id}\")\n","            print(\"Matching paths:\")\n","            for path in paths:\n","                print(f\"- {' → '.join(path)}\")\n","\n","# Run the search index test\n","test_search_index()\n","\n","\"\"\"## What's Next?\n","\n","In Part 5, we'll focus on performance optimization and testing. We'll explore:\n","1. Efficient processing of large hierarchical datasets\n","2. Caching strategies for improved performance\n","3. Comprehensive testing approaches\n","4. Integration with RAG system components\"\"\""],"metadata":{"id":"UWe8kADy3b53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Structured Data in RAG Systems - Part 5: Performance Optimization and Testing\n","\n","This notebook demonstrates practical approaches to optimizing and testing structured\n","data processing in RAG systems. We'll focus on real-world performance improvements\n","and reliable testing strategies.\n","\n","Note: Make sure you've run the previous parts to have the sample data available.\n","\"\"\"\n","\n","# Install required packages\n","!pip install pytest pandas numpy memory_profiler\n","\n","import pandas as pd\n","import numpy as np\n","import json\n","import time\n","from typing import Any, Dict, List, Optional\n","from pathlib import Path\n","import logging\n","from datetime import datetime\n","import hashlib\n","import os\n","\n","# Set up logging with a clear format\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Section 1: Performance Monitoring\n","\n","First, let's create a simple but effective performance monitoring system. This will\n","help us measure the impact of our optimizations.\n","\"\"\"\n","\n","class PerformanceMonitor:\n","    \"\"\"\n","    Tracks execution time and memory usage of operations.\n","    Provides insights into performance bottlenecks.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the performance monitor.\"\"\"\n","        self.metrics = {}\n","        self.start_times = {}\n","\n","    def start_operation(self, operation_name: str):\n","        \"\"\"\n","        Start timing an operation.\n","\n","        Args:\n","            operation_name: Name of the operation to track\n","        \"\"\"\n","        self.start_times[operation_name] = time.time()\n","\n","    def end_operation(self, operation_name: str) -> float:\n","        \"\"\"\n","        End timing an operation and record its duration.\n","\n","        Args:\n","            operation_name: Name of the operation to record\n","\n","        Returns:\n","            Duration of the operation in seconds\n","        \"\"\"\n","        if operation_name not in self.start_times:\n","            raise ValueError(f\"Operation {operation_name} was not started\")\n","\n","        duration = time.time() - self.start_times[operation_name]\n","\n","        if operation_name not in self.metrics:\n","            self.metrics[operation_name] = []\n","\n","        self.metrics[operation_name].append(duration)\n","        return duration\n","\n","    def get_statistics(self, operation_name: str) -> Dict:\n","        \"\"\"\n","        Get performance statistics for an operation.\n","\n","        Args:\n","            operation_name: Name of the operation to analyze\n","\n","        Returns:\n","            Dictionary containing performance statistics\n","        \"\"\"\n","        if operation_name not in self.metrics:\n","            raise ValueError(f\"No metrics found for {operation_name}\")\n","\n","        durations = self.metrics[operation_name]\n","        return {\n","            'count': len(durations),\n","            'mean_duration': np.mean(durations),\n","            'min_duration': min(durations),\n","            'max_duration': max(durations)\n","        }\n","\n","\"\"\"Let's test our performance monitor with a simple operation:\"\"\"\n","\n","def test_performance_monitor():\n","    \"\"\"Test the performance monitoring system.\"\"\"\n","    monitor = PerformanceMonitor()\n","\n","    # Test a simple operation\n","    monitor.start_operation(\"test_op\")\n","    time.sleep(1)  # Simulate work\n","    duration = monitor.end_operation(\"test_op\")\n","\n","    print(f\"Operation duration: {duration:.2f} seconds\")\n","    print(\"\\nOperation statistics:\")\n","    print(json.dumps(monitor.get_statistics(\"test_op\"), indent=2))\n","\n","# Run the test\n","test_performance_monitor()\n","\n","\"\"\"## Section 2: Optimized File Processing\n","\n","Now let's create an optimized file processor that includes caching and\n","chunked processing capabilities.\n","\"\"\"\n","\n","class OptimizedFileProcessor:\n","    \"\"\"\n","    Processes files efficiently using caching and chunked reading.\n","    Designed for handling large data files in RAG systems.\n","    \"\"\"\n","\n","    def __init__(self, file_path: str, chunk_size: int = 10000):\n","        \"\"\"\n","        Initialize the optimized file processor.\n","\n","        Args:\n","            file_path: Path to the file to process\n","            chunk_size: Size of chunks for reading large files\n","        \"\"\"\n","        self.file_path = Path(file_path)\n","        self.chunk_size = chunk_size\n","        self.monitor = PerformanceMonitor()\n","\n","        # Create cache directory if it doesn't exist\n","        self.cache_dir = Path('file_cache')\n","        self.cache_dir.mkdir(exist_ok=True)\n","\n","    def _calculate_file_hash(self) -> str:\n","        \"\"\"\n","        Calculate a hash of the file content for cache validation.\n","        Uses chunked reading to handle large files efficiently.\n","\n","        Returns:\n","            SHA-256 hash of the file content\n","        \"\"\"\n","        hasher = hashlib.sha256()\n","        with open(self.file_path, 'rb') as f:\n","            while chunk := f.read(8192):  # Read in 8KB chunks\n","                hasher.update(chunk)\n","        return hasher.hexdigest()\n","\n","    def _get_cache_path(self, file_hash: str) -> Path:\n","        \"\"\"Get the cache file path for a given hash.\"\"\"\n","        return self.cache_dir / f\"{file_hash}.cache\"\n","\n","    def process_file(self) -> pd.DataFrame:\n","        \"\"\"\n","        Process a file with caching and performance monitoring.\n","\n","        Returns:\n","            Processed DataFrame\n","        \"\"\"\n","        self.monitor.start_operation('total_processing')\n","\n","        try:\n","            # Check cache\n","            file_hash = self._calculate_file_hash()\n","            cache_path = self._get_cache_path(file_hash)\n","\n","            if cache_path.exists():\n","                logger.info(\"Using cached result\")\n","                df = pd.read_pickle(cache_path)\n","                self.monitor.end_operation('total_processing')\n","                return df\n","\n","            logger.info(\"Cache miss - processing file\")\n","\n","            # Process in chunks\n","            chunks = []\n","            for chunk_num, chunk in enumerate(pd.read_csv(self.file_path, chunksize=self.chunk_size)):\n","                logger.info(f\"Processing chunk {chunk_num + 1}\")\n","                chunks.append(chunk)\n","\n","            # Combine chunks\n","            df = pd.concat(chunks, ignore_index=True)\n","\n","            # Cache the result\n","            df.to_pickle(cache_path)\n","\n","            duration = self.monitor.end_operation('total_processing')\n","            logger.info(f\"Processing completed in {duration:.2f} seconds\")\n","\n","            return df\n","\n","        except Exception as e:\n","            logger.error(f\"Error processing file: {str(e)}\")\n","            raise\n","\n","\"\"\"Let's test our optimized file processor with a sample CSV:\"\"\"\n","\n","def test_optimized_processor():\n","    \"\"\"Test the optimized file processor with sample data.\"\"\"\n","    # Create a test CSV file\n","    test_data = pd.DataFrame({\n","        'id': range(100),\n","        'value': np.random.randn(100)\n","    })\n","\n","    test_file = Path('test_data.csv')\n","    test_data.to_csv(test_file, index=False)\n","\n","    try:\n","        # Process the file twice to test caching\n","        processor = OptimizedFileProcessor(test_file)\n","\n","        print(\"First run (no cache):\")\n","        result1 = processor.process_file()\n","        print(f\"Result shape: {result1.shape}\")\n","\n","        print(\"\\nSecond run (with cache):\")\n","        result2 = processor.process_file()\n","        print(f\"Result shape: {result2.shape}\")\n","\n","    finally:\n","        # Clean up test file\n","        test_file.unlink(missing_ok=True)\n","\n","# Run the test\n","test_optimized_processor()\n","\n","\"\"\"## Section 3: Performance Testing Framework\n","\n","Let's create a framework for testing performance under different conditions.\n","\"\"\"\n","\n","class PerformanceTester:\n","    \"\"\"\n","    Runs performance tests on data processing operations.\n","    Helps compare different optimization strategies.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the performance tester.\"\"\"\n","        self.results = {}\n","\n","    def run_test(self, name: str, func: callable, iterations: int = 3):\n","        \"\"\"\n","        Run a performance test multiple times.\n","\n","        Args:\n","            name: Name of the test\n","            func: Function to test\n","            iterations: Number of times to run the test\n","        \"\"\"\n","        durations = []\n","\n","        for i in range(iterations):\n","            logger.info(f\"Running {name} - iteration {i + 1}\")\n","            start_time = time.time()\n","\n","            try:\n","                func()\n","                duration = time.time() - start_time\n","                durations.append(duration)\n","            except Exception as e:\n","                logger.error(f\"Error in test {name}: {str(e)}\")\n","                raise\n","\n","        self.results[name] = {\n","            'mean_duration': np.mean(durations),\n","            'min_duration': min(durations),\n","            'max_duration': max(durations),\n","            'iterations': iterations\n","        }\n","\n","    def compare_results(self):\n","        \"\"\"Print a comparison of test results.\"\"\"\n","        print(\"\\nPerformance Test Results:\")\n","        print(\"-\" * 50)\n","\n","        for name, metrics in self.results.items():\n","            print(f\"\\n{name}:\")\n","            print(f\"  Mean duration: {metrics['mean_duration']:.2f} seconds\")\n","            print(f\"  Min duration: {metrics['min_duration']:.2f} seconds\")\n","            print(f\"  Max duration: {metrics['max_duration']:.2f} seconds\")\n","\n","\"\"\"Let's use our performance testing framework to compare different processing approaches:\"\"\"\n","\n","def test_performance_comparison():\n","    \"\"\"Compare performance of different processing strategies.\"\"\"\n","    # Create test data\n","    test_data = pd.DataFrame({\n","        'id': range(1000),\n","        'value': np.random.randn(1000)\n","    })\n","\n","    test_file = Path('perf_test.csv')\n","    test_data.to_csv(test_file, index=False)\n","\n","    try:\n","        tester = PerformanceTester()\n","\n","        # Test different chunk sizes\n","        for chunk_size in [100, 500, 1000]:\n","            processor = OptimizedFileProcessor(test_file, chunk_size=chunk_size)\n","            tester.run_test(\n","                f\"Chunk size {chunk_size}\",\n","                processor.process_file\n","            )\n","\n","        # Compare results\n","        tester.compare_results()\n","\n","    finally:\n","        # Clean up\n","        test_file.unlink(missing_ok=True)\n","\n","# Run the performance comparison\n","test_performance_comparison()\n","\n","\"\"\"## Key Insights\n","\n","Through our optimization and testing work, we've learned several important lessons:\n","\n","1. Caching significantly improves performance for unchanged files\n","2. Chunked processing helps manage memory usage with large files\n","3. Performance monitoring helps identify optimization opportunities\n","4. Regular testing ensures optimizations don't introduce bugs\n","\n","These techniques can be applied to improve the performance of any RAG system\n","that processes structured data.\"\"\""],"metadata":{"id":"PoVmz84_48U5"},"execution_count":null,"outputs":[]}]}