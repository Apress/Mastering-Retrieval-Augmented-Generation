{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmicY/zm2u74ZzEqfr0Y1u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"Dc_fRntUlI-9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hplgqqF2j9OB"},"outputs":[],"source":["!pip install langchain langchain-openai tiktoken faiss-cpu sentence-transformers\n","\n","import os\n","import re\n","import json\n","import numpy as np\n","from typing import List, Dict, Any, Optional, Tuple\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","import tiktoken\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","from langchain.chains import LLMChain\n","from langchain_core.documents import Document"]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"4_J3vKbOlKLs"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")"],"metadata":{"id":"24jIBM2elKXu","executionInfo":{"status":"ok","timestamp":1740847276126,"user_tz":-330,"elapsed":11,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: Basic Few-Shot Templates**"],"metadata":{"id":"uhEjXMQnmQnu"}},{"cell_type":"code","source":["print(\"Section 1: Basic Few-Shot Templates\")\n","\n","# Basic few-shot template for question answering\n","basic_few_shot_template = \"\"\"\n","Answer the question based on the context. If the answer isn't in the context, say \"I don't have enough information.\"\n","\n","Example 1:\n","Context: The Golden Gate Bridge was completed in 1937. It has a total length of 8,981 feet.\n","Question: When was the Golden Gate Bridge completed?\n","Answer: The Golden Gate Bridge was completed in 1937.\n","\n","Example 2:\n","Context: The Eiffel Tower is 330 meters tall and was completed in 1889.\n","Question: How deep is the foundation of the Eiffel Tower?\n","Answer: I don't have enough information.\n","\n","Now, answer the following:\n","Context: {context}\n","Question: {question}\n","Answer:\n","\"\"\"\n","\n","print(\"Basic few-shot template:\")\n","print(basic_few_shot_template)\n","print(f\"Token count: {count_tokens(basic_few_shot_template)}\")\n","\n","# Test with a sample context and question\n","sample_context = \"The Python programming language was created by Guido van Rossum and first released in 1991.\"\n","sample_question = \"Who created the Python programming language?\"\n","\n","formatted_prompt = basic_few_shot_template.format(\n","    context=sample_context,\n","    question=sample_question\n",")\n","\n","print(\"\\nFormatted prompt:\")\n","print(formatted_prompt)\n","\n","print_separator()"],"metadata":{"id":"usG-jsiVmQ5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: Example Categories for RAG Applications**"],"metadata":{"id":"sTUOSjrtmdMA"}},{"cell_type":"code","source":["print(\"Section 2: Example Categories for RAG Applications\")\n","\n","# Source attribution examples\n","attribution_examples = [\n","    {\n","        \"context\": \"[Doc1] The Python programming language was created by Guido van Rossum in 1991.\",\n","        \"question\": \"Who created Python?\",\n","        \"answer\": \"Python was created by Guido van Rossum in 1991 [Doc1].\"\n","    },\n","    {\n","        \"context\": \"[Doc1] The first Harry Potter book was published in 1997.\\n[Doc2] J.K. Rowling is the author of the Harry Potter series.\",\n","        \"question\": \"When was the first Harry Potter book published and who wrote it?\",\n","        \"answer\": \"The first Harry Potter book was published in 1997 [Doc1] and was written by J.K. Rowling [Doc2].\"\n","    }\n","]\n","\n","# Incomplete information examples\n","incomplete_info_examples = [\n","    {\n","        \"context\": \"[Doc1] JavaScript was created by Brendan Eich in 1995.\",\n","        \"question\": \"When was TypeScript released?\",\n","        \"answer\": \"I don't have enough information in the provided context to answer when TypeScript was released.\"\n","    },\n","    {\n","        \"context\": \"[Doc1] The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World.\",\n","        \"question\": \"How tall is the Great Pyramid of Giza?\",\n","        \"answer\": \"The context doesn't provide information about the height of the Great Pyramid of Giza.\"\n","    }\n","]\n","\n","# Contradiction resolution examples\n","contradiction_examples = [\n","    {\n","        \"context\": \"[Doc1] Some studies suggest coffee may increase heart disease risk.\\n[Doc2] Recent research indicates moderate coffee consumption may reduce heart disease risk.\",\n","        \"question\": \"Does coffee increase heart disease risk?\",\n","        \"answer\": \"The information in the context presents conflicting views. According to [Doc1], some studies suggest coffee may increase heart disease risk, while [Doc2] indicates recent research shows moderate coffee consumption may actually reduce heart disease risk. The scientific consensus appears to be evolving on this topic.\"\n","    },\n","    {\n","        \"context\": \"[Doc1] The film 'Inception' was released in 2010.\\n[Doc2] The movie 'Inception' came out in 2009.\",\n","        \"question\": \"When was Inception released?\",\n","        \"answer\": \"There is conflicting information in the provided context. [Doc1] states that 'Inception' was released in 2010, while [Doc2] states it came out in 2009. Without additional information, I cannot determine which date is correct.\"\n","    }\n","]\n","\n","# Synthesis examples\n","synthesis_examples = [\n","    {\n","        \"context\": \"[Doc1] Machine learning is a subset of artificial intelligence.\\n[Doc2] Deep learning is a type of machine learning that uses neural networks with multiple layers.\\n[Doc3] Neural networks are computational models inspired by the human brain.\",\n","        \"question\": \"Explain the relationship between AI, machine learning, and deep learning.\",\n","        \"answer\": \"Based on the provided context, artificial intelligence is a broad field that includes machine learning as a subset [Doc1]. Deep learning is a specific type of machine learning that utilizes neural networks with multiple layers [Doc2]. These neural networks are computational models inspired by the human brain [Doc3].\"\n","    }\n","]\n","\n","# Function to demonstrate example usage\n","def print_example_category(category_name, examples):\n","    print(f\"\\n{category_name} Examples:\")\n","    for i, example in enumerate(examples):\n","        print(f\"Example {i+1}:\")\n","        print(f\"Context: {example['context']}\")\n","        print(f\"Question: {example['question']}\")\n","        print(f\"Answer: {example['answer']}\")\n","        print(\"-\" * 30)\n","\n","# Print example categories\n","print_example_category(\"Source Attribution\", attribution_examples)\n","print_example_category(\"Incomplete Information\", incomplete_info_examples)\n","print_example_category(\"Contradiction Resolution\", contradiction_examples)\n","print_example_category(\"Synthesis\", synthesis_examples)\n","\n","print_separator()"],"metadata":{"id":"id_-NM7WmdW4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Creating Templates with Diverse Examples**"],"metadata":{"id":"mo_d0BlbmxhF"}},{"cell_type":"code","source":["print(\"Section 3: Creating Templates with Diverse Examples\")\n","\n","# Template with diverse examples\n","diverse_examples_template = \"\"\"\n","Answer based on the provided information.\n","\n","Example 1: [Direct answer with citation]\n","Context: [Doc1] Einstein published the theory of relativity in 1905.\n","Question: When did Einstein publish the theory of relativity?\n","Answer: Einstein published the theory of relativity in 1905 [Doc1].\n","\n","Example 2: [Synthesis across documents]\n","Context: [Doc1] The Great Depression began with the stock market crash in 1929.\n","[Doc2] The Great Depression lasted until about 1939.\n","Question: What was the Great Depression?\n","Answer: The Great Depression was an economic downturn that began with the stock market crash in 1929 [Doc1] and lasted until about 1939 [Doc2].\n","\n","Example 3: [Missing information]\n","Context: [Doc1] The Python programming language was created by Guido van Rossum.\n","Question: When was Python 3.0 released?\n","Answer: I don't have enough information to answer when Python 3.0 was released.\n","\n","Context: {context}\n","Question: {question}\n","Answer:\n","\"\"\"\n","\n","print(\"Template with diverse examples:\")\n","print(diverse_examples_template)\n","print(f\"Token count: {count_tokens(diverse_examples_template)}\")\n","\n","# Function to format examples into a template string\n","def format_examples_into_template(examples, max_examples=2):\n","    formatted_examples = \"\"\n","\n","    for i, example in enumerate(examples[:max_examples]):\n","        formatted_examples += f\"\\nExample {i+1}:\\n\"\n","        formatted_examples += f\"Context: {example['context']}\\n\"\n","        formatted_examples += f\"Question: {example['question']}\\n\"\n","        formatted_examples += f\"Answer: {example['answer']}\\n\"\n","\n","    return formatted_examples\n","\n","# Create a template with attribution and incomplete info examples\n","custom_examples = attribution_examples[:1] + incomplete_info_examples[:1]\n","formatted_examples = format_examples_into_template(custom_examples)\n","\n","custom_template = f\"\"\"\n","Answer the question based on the provided context. If the information isn't available, say so clearly.\n","{formatted_examples}\n","Context: {{context}}\n","Question: {{question}}\n","Answer:\n","\"\"\"\n","\n","print(\"\\nCustom template with selected examples:\")\n","print(custom_template)\n","\n","print_separator()"],"metadata":{"id":"KOnWXqkRmxul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Dynamic Example Selection**"],"metadata":{"id":"G68wQPdlm7d4"}},{"cell_type":"code","source":["print(\"Section 4: Dynamic Example Selection\")\n","\n","# Create an example library organized by query types\n","example_library = {\n","    \"factual\": attribution_examples,\n","    \"comparative\": [\n","        {\n","            \"context\": \"[Doc1] Python is dynamically typed. [Doc2] Java is statically typed.\",\n","            \"question\": \"Compare Python and Java typing systems.\",\n","            \"answer\": \"Based on the context, Python is dynamically typed [Doc1] while Java is statically typed [Doc2].\"\n","        },\n","        {\n","            \"context\": \"[Doc1] Electric cars run on batteries and electric motors. [Doc2] Gasoline cars use internal combustion engines.\",\n","            \"question\": \"What's the difference between electric and gasoline cars?\",\n","            \"answer\": \"According to the context, electric cars run on batteries and electric motors [Doc1], while gasoline cars use internal combustion engines [Doc2].\"\n","        }\n","    ],\n","    \"cause_effect\": [\n","        {\n","            \"context\": \"[Doc1] Greenhouse gases trap heat in the Earth's atmosphere. [Doc2] Increased greenhouse gas emissions have led to global temperature rise.\",\n","            \"question\": \"Why is the Earth's temperature rising?\",\n","            \"answer\": \"Based on the context, the Earth's temperature is rising because increased greenhouse gas emissions [Doc2] trap heat in the Earth's atmosphere [Doc1].\"\n","        }\n","    ],\n","    \"not_found\": incomplete_info_examples,\n","    \"contradictory\": contradiction_examples,\n","    \"synthesis\": synthesis_examples\n","}\n","\n","# Function to detect query type\n","def detect_query_type(query: str) -> str:\n","    \"\"\"Detect the type of query based on keywords and structure.\"\"\"\n","    query = query.lower()\n","\n","    # Check for comparison questions\n","    if any(term in query for term in [\"compare\", \"difference\", \"versus\", \"vs\", \"similarities\", \"differences\"]):\n","        return \"comparative\"\n","\n","    # Check for cause and effect questions\n","    if any(term in query for term in [\"why\", \"cause\", \"effect\", \"result\", \"impact\", \"lead to\"]):\n","        return \"cause_effect\"\n","\n","    # Check for synthesis questions (typically broader questions)\n","    if any(term in query for term in [\"explain\", \"describe\", \"elaborate\", \"summarize\"]):\n","        return \"synthesis\"\n","\n","    # Default to factual\n","    return \"factual\"\n","\n","# Function to select examples based on query type\n","def select_examples(query: str, max_examples: int = 2) -> List[Dict]:\n","    \"\"\"Select appropriate examples based on query type.\"\"\"\n","    query_type = detect_query_type(query)\n","\n","    # Get examples for the detected type, or fall back to factual\n","    selected_examples = example_library.get(query_type, example_library[\"factual\"])\n","\n","    # Limit to max_examples\n","    return selected_examples[:max_examples]\n","\n","# Test with different query types\n","test_queries = [\n","    \"Who invented the light bulb?\",\n","    \"Compare renewable and non-renewable energy sources.\",\n","    \"Why does climate change happen?\",\n","    \"Explain the water cycle and its importance.\"\n","]\n","\n","for query in test_queries:\n","    query_type = detect_query_type(query)\n","    examples = select_examples(query)\n","\n","    print(f\"\\nQuery: {query}\")\n","    print(f\"Detected type: {query_type}\")\n","    print(f\"Selected {len(examples)} examples:\")\n","\n","    for i, example in enumerate(examples):\n","        print(f\"  Example {i+1}: Question: {example['question']}\")\n","\n","print_separator()"],"metadata":{"id":"A88nhO7Jm7pU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Advanced Example Selection with Semantic Similarity**"],"metadata":{"id":"qk0367IXnFD1"}},{"cell_type":"code","source":["print(\"Section 5: Advanced Example Selection with Semantic Similarity\")\n","\n","try:\n","    from sentence_transformers import SentenceTransformer\n","\n","    # This is optional and will only run if sentence-transformers is installed\n","    print(\"Demonstrating semantic similarity-based example selection...\")\n","\n","    # Initialize model for semantic similarity (lightweight model)\n","    try:\n","        model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n","\n","        # Flatten all examples for similarity comparison\n","        all_examples = []\n","        for category, examples in example_library.items():\n","            for example in examples:\n","                example['category'] = category\n","                all_examples.append(example)\n","\n","        def select_examples_by_similarity(query: str, examples: List[Dict], top_k: int = 2):\n","            \"\"\"Select examples most similar to the query.\"\"\"\n","            if not examples:\n","                return []\n","\n","            # Get embeddings\n","            query_embedding = model.encode(query)\n","            question_embeddings = model.encode([ex['question'] for ex in examples])\n","\n","            # Calculate similarities\n","            similarities = [np.dot(query_embedding, q_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(q_emb))\n","                            for q_emb in question_embeddings]\n","\n","            # Get top-k examples by similarity\n","            top_indices = np.argsort(similarities)[-top_k:][::-1]\n","\n","            return [examples[i] for i in top_indices]\n","\n","        # Test semantic similarity selection\n","        test_query = \"What are the environmental impacts of deforestation?\"\n","        similar_examples = select_examples_by_similarity(test_query, all_examples, top_k=2)\n","\n","        print(f\"\\nQuery: {test_query}\")\n","        print(\"Selected examples by semantic similarity:\")\n","\n","        for i, example in enumerate(similar_examples):\n","            print(f\"Example {i+1} (Category: {example['category']}):\")\n","            print(f\"Question: {example['question']}\")\n","            print(f\"Context: {example['context']}\")\n","            print(\"-\" * 30)\n","\n","    except Exception as e:\n","        print(f\"Couldn't initialize semantic model: {e}\")\n","        print(\"Continuing with keyword-based example selection...\")\n","\n","except ImportError:\n","    print(\"Sentence Transformers not installed. Skipping semantic similarity example.\")\n","    print(\"To enable, install with: pip install sentence-transformers\")\n","\n","print_separator()"],"metadata":{"id":"Fo8iEjbCnFPz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Balancing Example Count with Context Limitations**"],"metadata":{"id":"W69gkibVnY7w"}},{"cell_type":"code","source":["print(\"Section 6: Balancing Example Count with Context Limitations\")\n","\n","def build_prompt_with_examples(query: str, context: str, max_tokens: int = 3000) -> str:\n","    \"\"\"\n","    Build a prompt with dynamically selected examples while respecting token limits.\n","\n","    Args:\n","        query: The user's question\n","        context: The retrieved context to include\n","        max_tokens: Maximum tokens allowed for the prompt\n","\n","    Returns:\n","        A formatted prompt with examples\n","    \"\"\"\n","    # Base template without examples\n","    base_template = \"\"\"\n","    Answer the question based on the provided context. If the information isn't in the context, say so clearly.\n","\n","    {examples}\n","\n","    Context: {context}\n","    Question: {question}\n","    Answer:\n","    \"\"\"\n","\n","    # Calculate tokens for fixed parts\n","    fixed_template = base_template.format(examples=\"\", context=context, question=query)\n","    fixed_tokens = count_tokens(fixed_template)\n","\n","    # Calculate available tokens for examples\n","    available_for_examples = max_tokens - fixed_tokens\n","\n","    # Select examples\n","    selected_examples = select_examples(query)\n","\n","    # Format examples into text\n","    examples_text = \"\"\n","    current_example_tokens = 0\n","\n","    for i, example in enumerate(selected_examples):\n","        example_text = f\"\\nExample {i+1}:\\n\"\n","        example_text += f\"Context: {example['context']}\\n\"\n","        example_text += f\"Question: {example['question']}\\n\"\n","        example_text += f\"Answer: {example['answer']}\\n\"\n","\n","        example_tokens = count_tokens(example_text)\n","\n","        # Check if adding this example would exceed our token budget\n","        if current_example_tokens + example_tokens <= available_for_examples:\n","            examples_text += example_text\n","            current_example_tokens += example_tokens\n","        else:\n","            # Stop adding examples if we'd exceed the limit\n","            break\n","\n","    # Format final prompt\n","    final_prompt = base_template.format(\n","        examples=examples_text,\n","        context=context,\n","        question=query\n","    )\n","\n","    return final_prompt, count_tokens(final_prompt)\n","\n","# Test with different context lengths\n","short_context = \"The Amazon rainforest is located in South America. It spans across Brazil, Peru, Colombia, and several other countries.\"\n","long_context = \"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations and 3,344 formally acknowledged indigenous territories. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have 'Amazonas' as the name of a political-administrative region. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\n","\n","test_query = \"Where is the Amazon rainforest located?\"\n","\n","short_prompt, short_tokens = build_prompt_with_examples(test_query, short_context)\n","long_prompt, long_tokens = build_prompt_with_examples(test_query, long_context)\n","\n","print(f\"Short context prompt (tokens: {short_tokens}):\")\n","print(short_prompt[:300] + \"...\")\n","\n","print(f\"\\nLong context prompt (tokens: {long_tokens}):\")\n","print(long_prompt[:300] + \"...\")\n","\n","print(\"\\nExample count comparison:\")\n","short_example_count = short_prompt.count(\"Example \")\n","long_example_count = long_prompt.count(\"Example \")\n","\n","print(f\"Short context allows for {short_example_count} examples\")\n","print(f\"Long context allows for {long_example_count} examples\")\n","\n","print_separator()"],"metadata":{"id":"kY7FNseRnZGY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 7: Example Rotation Strategies**"],"metadata":{"id":"8P_xNo0mnj3E"}},{"cell_type":"code","source":["print(\"Section 7: Example Rotation Strategies\")\n","\n","class ExampleRotator:\n","    def __init__(self, example_library):\n","        self.example_library = example_library\n","        self.usage_counts = {category: [0] * len(examples)\n","                             for category, examples in example_library.items()}\n","\n","    def rotate_examples(self, query, strategy=\"round_robin\", max_examples=2):\n","        \"\"\"\n","        Select examples using various rotation strategies.\n","\n","        Strategies:\n","        - round_robin: Rotate through examples evenly\n","        - query_type: Select examples matching query type\n","        - diversity: Select examples from different categories\n","        \"\"\"\n","        if strategy == \"round_robin\":\n","            # Flatten and rotate examples\n","            all_examples = []\n","            for category, examples in self.example_library.items():\n","                all_examples.extend([(category, i, ex) for i, ex in enumerate(examples)])\n","\n","            # Sort by usage count\n","            all_examples.sort(key=lambda x: self.usage_counts[x[0]][x[1]])\n","\n","            # Select least used examples\n","            selected = all_examples[:max_examples]\n","\n","            # Update usage counts\n","            for category, idx, _ in selected:\n","                self.usage_counts[category][idx] += 1\n","\n","            return [ex for _, _, ex in selected]\n","\n","        elif strategy == \"query_type\":\n","            # Match by query type\n","            query_type = detect_query_type(query)\n","            examples = self.example_library.get(query_type, self.example_library[\"factual\"])\n","\n","            # Get indices sorted by usage count\n","            indices = sorted(range(len(examples)),\n","                            key=lambda i: self.usage_counts[query_type][i])\n","\n","            # Select and update usage\n","            selected = [examples[i] for i in indices[:max_examples]]\n","            for i in indices[:max_examples]:\n","                self.usage_counts[query_type][i] += 1\n","\n","            return selected\n","\n","        elif strategy == \"diversity\":\n","            # Select from different categories\n","            categories = list(self.example_library.keys())\n","            selected = []\n","\n","            for i in range(min(max_examples, len(categories))):\n","                category = categories[i % len(categories)]\n","                examples = self.example_library[category]\n","\n","                if not examples:\n","                    continue\n","\n","                # Find least used example\n","                idx = min(range(len(examples)),\n","                         key=lambda i: self.usage_counts[category][i])\n","\n","                selected.append(examples[idx])\n","                self.usage_counts[category][idx] += 1\n","\n","            return selected\n","\n","        else:\n","            # Default to query type matching\n","            return select_examples(query, max_examples)\n","\n","# Create rotator\n","rotator = ExampleRotator(example_library)\n","\n","# Test rotation strategies\n","strategies = [\"round_robin\", \"query_type\", \"diversity\"]\n","test_queries = [\n","    \"What is photosynthesis?\",\n","    \"Compare renewable and fossil fuels.\",\n","    \"Why do seasons change?\",\n","    \"Explain artificial intelligence.\"\n","]\n","\n","for strategy in strategies:\n","    print(f\"\\nStrategy: {strategy}\")\n","\n","    for query in test_queries:\n","        examples = rotator.rotate_examples(query, strategy=strategy)\n","\n","        print(f\"\\nQuery: {query}\")\n","        print(f\"Selected examples:\")\n","\n","        for i, example in enumerate(examples):\n","            question = example['question']\n","            category = next((cat for cat, exs in example_library.items()\n","                           if example in exs), \"unknown\")\n","            print(f\"  Example {i+1}: [{category}] {question}\")\n","\n","print_separator()"],"metadata":{"id":"345lf-aznkK4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 8: Putting It All Together - Complete RAG Prompt with Examples**"],"metadata":{"id":"VNQredsqnto_"}},{"cell_type":"code","source":["print(\"Section 8: Putting It All Together - Complete RAG Prompt with Examples\")\n","\n","def create_complete_rag_prompt(query, context_docs, max_tokens=3500):\n","    \"\"\"\n","    Create a complete RAG prompt with dynamically selected examples.\n","\n","    Args:\n","        query: The user question\n","        context_docs: List of Document objects with retrieved information\n","        max_tokens: Maximum tokens for the prompt\n","\n","    Returns:\n","        Formatted prompt with examples and context\n","    \"\"\"\n","    # 1. Process retrieved documents into context string\n","    context = \"\\n\\n\".join([f\"[Doc {i+1}] {doc.page_content}\"\n","                           for i, doc in enumerate(context_docs)])\n","\n","    # 2. Detect query type and select appropriate examples\n","    query_type = detect_query_type(query)\n","    examples = select_examples(query)\n","\n","    # 3. Format template with placeholders\n","    template = \"\"\"\n","    You are an AI assistant answering questions based on the provided context.\n","\n","    {examples}\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    INSTRUCTIONS:\n","    - Answer based ONLY on the information in the CONTEXT\n","    - If the CONTEXT doesn't contain the answer, say \"I don't have enough information to answer this question\"\n","    - Cite sources using [Doc X] notation\n","    - For contradictory information, acknowledge the conflict and present both perspectives\n","\n","    ANSWER:\n","    \"\"\"\n","\n","    # 4. Calculate token budgets\n","    template_without_context_examples = template.format(\n","        examples=\"\", context=\"\", question=query\n","    )\n","    fixed_tokens = count_tokens(template_without_context_examples)\n","    context_tokens = count_tokens(context)\n","\n","    # Calculate remaining tokens for examples\n","    available_for_examples = max_tokens - fixed_tokens - context_tokens\n","\n","    # 5. Format examples to fit token limit\n","    examples_text = \"\"\n","    current_example_tokens = 0\n","\n","    for i, example in enumerate(examples):\n","        example_text = f\"\\nExample {i+1}:\\n\"\n","        example_text += f\"Context: {example['context']}\\n\"\n","        example_text += f\"Question: {example['question']}\\n\"\n","        example_text += f\"Answer: {example['answer']}\\n\"\n","\n","        example_tokens = count_tokens(example_text)\n","\n","        if current_example_tokens + example_tokens <= available_for_examples:\n","            examples_text += example_text\n","            current_example_tokens += example_tokens\n","        else:\n","            break\n","\n","    # 6. Construct final prompt\n","    final_prompt = template.format(\n","        examples=examples_text,\n","        context=context,\n","        question=query\n","    )\n","\n","    # 7. Check if we need to truncate context (emergency backup)\n","    total_tokens = count_tokens(final_prompt)\n","    if total_tokens > max_tokens:\n","        # Need to reduce context\n","        available_for_context = max_tokens - (total_tokens - context_tokens)\n","\n","        # Simple truncation (in practice, you'd want smarter truncation)\n","        encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","        context_tokens_list = encoder.encode(context)\n","        truncated_context_tokens = context_tokens_list[:available_for_context]\n","        truncated_context = encoder.decode(truncated_context_tokens)\n","\n","        # Rebuild prompt\n","        final_prompt = template.format(\n","            examples=examples_text,\n","            context=truncated_context,\n","            question=query\n","        )\n","\n","    return final_prompt\n","\n","# Create sample documents\n","sample_docs = [\n","    Document(page_content=\"Climate change is the long-term alteration of temperature and typical weather patterns. It is primarily caused by human activities, especially the burning of fossil fuels.\",\n","             metadata={\"source\": \"climate_science_journal\"}),\n","    Document(page_content=\"The effects of climate change include rising sea levels, more frequent extreme weather events, and shifts in plant and animal ranges.\",\n","             metadata={\"source\": \"environmental_report\"}),\n","    Document(page_content=\"Renewable energy sources like solar and wind power can help reduce greenhouse gas emissions that contribute to climate change.\",\n","             metadata={\"source\": \"energy_policy_paper\"})\n","]\n","\n","# Test the complete RAG prompt\n","test_query = \"What causes climate change and what are its effects?\"\n","complete_prompt = create_complete_rag_prompt(test_query, sample_docs)\n","\n","print(\"Complete RAG prompt with examples:\")\n","print(complete_prompt)\n","print(f\"\\nTotal token count: {count_tokens(complete_prompt)}\")\n","\n","# Try with OpenAI if API key is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        llm = OpenAI(temperature=0)\n","        response = llm.invoke(complete_prompt)\n","\n","        print(\"\\nGenerated response:\")\n","        print(response)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","else:\n","    print(\"\\nNo OpenAI API key found. Skipping response generation.\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")"],"metadata":{"id":"Vsu6SLHXntx1"},"execution_count":null,"outputs":[]}]}