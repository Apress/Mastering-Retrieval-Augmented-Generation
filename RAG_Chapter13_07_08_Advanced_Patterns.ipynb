{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPzfm62hHpzX4TMKqGF7oj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**# INSTALLATION**"],"metadata":{"id":"bupdwBe4JAAa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZmswLR9IpTe"},"outputs":[],"source":["!pip install langchain langchain-openai langchain-community langchain-core langchain-experimental crewai crewai-tools pandas numpy python-dotenv tiktoken openai\n","!pip install plotly matplotlib seaborn redis sqlite3 psutil prometheus-client asyncio aiohttp\n","\n","# IMPORTS\n","import os\n","import json\n","import time\n","import asyncio\n","import sqlite3\n","import hashlib\n","import threading\n","from datetime import datetime, timedelta\n","from typing import Dict, List, Any, Optional, Union, Callable, Tuple\n","from dataclasses import dataclass, field\n","from enum import Enum\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import seaborn as sns\n","\n","# LangChain imports\n","from langchain.agents import AgentExecutor, create_react_agent, create_structured_chat_agent\n","from langchain.agents.agent_types import AgentType\n","from langchain.agents.initialize import initialize_agent\n","from langchain.tools import BaseTool, tool\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.tools import Tool\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import AgentAction, AgentFinish\n","from langchain.callbacks.base import BaseCallbackHandler\n","from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n","from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n","\n","# CrewAI imports\n","from crewai import Agent, Task, Crew, Process\n","from crewai.tools import BaseTool as CrewBaseTool\n","\n","# Monitoring and metrics\n","import psutil\n","from prometheus_client import Counter, Histogram, Gauge, start_http_server\n","\n","# Load environment variables\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# Set your OpenAI API key here\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n","\n","print(\"=\"*60)\n","print(\"NOTEBOOK 13.4: PRODUCTION DEPLOYMENT AND ADVANCED PATTERNS\")\n","print(\"=\"*60)"]},{"cell_type":"markdown","source":["**# SECTION 1: LANGCHAIN AGENTS (13.7.1)**"],"metadata":{"id":"y8RjnKZYJYuh"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 1: LANGCHAIN AGENTS IMPLEMENTATION\")\n","print(\"=\"*60)\n","\n","class AgentTypeEnum(Enum):\n","    \"\"\"Types of agents for different use cases (renamed to avoid conflicts)\"\"\"\n","    REACT = \"react\"\n","    CONVERSATIONAL = \"conversational\"\n","    PANDAS_DATAFRAME = \"pandas_dataframe\"\n","    CUSTOM = \"custom\"\n","\n","class AgentMetrics:\n","    \"\"\"Metrics collection for agent performance monitoring\"\"\"\n","\n","    def __init__(self, use_prometheus: bool = False):\n","        # Initialize internal metrics (always available)\n","        self.execution_history = []\n","        self.performance_stats = {}\n","        self.use_prometheus = use_prometheus\n","\n","        # Initialize Prometheus metrics only if requested and available\n","        if use_prometheus:\n","            try:\n","                # Check if metrics already exist and clear registry if needed\n","                from prometheus_client import REGISTRY\n","                # Clear existing metrics to avoid duplicates\n","                collectors_to_remove = []\n","                for collector in list(REGISTRY._collector_to_names.keys()):\n","                    if hasattr(collector, '_name') and any(name in collector._name for name in ['agent_requests', 'agent_duration', 'tool_usage', 'active_agents']):\n","                        collectors_to_remove.append(collector)\n","\n","                for collector in collectors_to_remove:\n","                    try:\n","                        REGISTRY.unregister(collector)\n","                    except KeyError:\n","                        pass  # Already unregistered\n","\n","                # Initialize Prometheus metrics\n","                self.agent_requests = Counter('agent_requests_total', 'Total agent requests', ['agent_type', 'status'])\n","                self.agent_duration = Histogram('agent_duration_seconds', 'Agent execution time', ['agent_type'])\n","                self.tool_usage = Counter('tool_usage_total', 'Tool usage count', ['tool_name', 'status'])\n","                self.active_agents = Gauge('active_agents', 'Number of active agents')\n","\n","            except Exception as e:\n","                print(f\"âš ï¸ Prometheus metrics initialization failed: {e}\")\n","                print(\"ðŸ“Š Falling back to internal metrics only\")\n","                self.use_prometheus = False\n","                self._init_fallback_metrics()\n","        else:\n","            self._init_fallback_metrics()\n","\n","    def _init_fallback_metrics(self):\n","        \"\"\"Initialize fallback metrics when Prometheus is not available\"\"\"\n","        self.agent_requests = {\"total\": 0, \"by_type\": {}, \"by_status\": {}}\n","        self.agent_duration = {\"total_time\": 0, \"count\": 0, \"by_type\": {}}\n","        self.tool_usage = {\"total\": 0, \"by_tool\": {}, \"by_status\": {}}\n","        self.active_agents = {\"count\": 0}\n","\n","    def record_execution(self, agent_type: str, duration: float, success: bool, tools_used: List[str]):\n","        \"\"\"Record agent execution metrics\"\"\"\n","        status = \"success\" if success else \"failure\"\n","\n","        # Update Prometheus metrics if available\n","        if self.use_prometheus:\n","            try:\n","                self.agent_requests.labels(agent_type=agent_type, status=status).inc()\n","                self.agent_duration.labels(agent_type=agent_type).observe(duration)\n","\n","                for tool in tools_used:\n","                    self.tool_usage.labels(tool_name=tool, status=status).inc()\n","            except Exception as e:\n","                print(f\"âš ï¸ Prometheus metrics update failed: {e}\")\n","        else:\n","            # Update fallback metrics\n","            self.agent_requests[\"total\"] += 1\n","            self.agent_requests[\"by_type\"][agent_type] = self.agent_requests[\"by_type\"].get(agent_type, 0) + 1\n","            self.agent_requests[\"by_status\"][status] = self.agent_requests[\"by_status\"].get(status, 0) + 1\n","\n","            self.agent_duration[\"total_time\"] += duration\n","            self.agent_duration[\"count\"] += 1\n","            if agent_type not in self.agent_duration[\"by_type\"]:\n","                self.agent_duration[\"by_type\"][agent_type] = {\"total_time\": 0, \"count\": 0}\n","            self.agent_duration[\"by_type\"][agent_type][\"total_time\"] += duration\n","            self.agent_duration[\"by_type\"][agent_type][\"count\"] += 1\n","\n","            for tool in tools_used:\n","                self.tool_usage[\"total\"] += 1\n","                self.tool_usage[\"by_tool\"][tool] = self.tool_usage[\"by_tool\"].get(tool, 0) + 1\n","                tool_status_key = f\"{tool}_{status}\"\n","                self.tool_usage[\"by_status\"][tool_status_key] = self.tool_usage[\"by_status\"].get(tool_status_key, 0) + 1\n","\n","        # Update internal metrics\n","        execution_record = {\n","            \"timestamp\": datetime.now(),\n","            \"agent_type\": agent_type,\n","            \"duration\": duration,\n","            \"success\": success,\n","            \"tools_used\": tools_used\n","        }\n","        self.execution_history.append(execution_record)\n","\n","        # Update performance stats\n","        if agent_type not in self.performance_stats:\n","            self.performance_stats[agent_type] = {\n","                \"total_executions\": 0,\n","                \"successful_executions\": 0,\n","                \"average_duration\": 0,\n","                \"tool_usage\": {}\n","            }\n","\n","        stats = self.performance_stats[agent_type]\n","        stats[\"total_executions\"] += 1\n","        if success:\n","            stats[\"successful_executions\"] += 1\n","\n","        # Update average duration\n","        stats[\"average_duration\"] = (\n","            (stats[\"average_duration\"] * (stats[\"total_executions\"] - 1) + duration) /\n","            stats[\"total_executions\"]\n","        )\n","\n","        # Update tool usage stats\n","        for tool in tools_used:\n","            stats[\"tool_usage\"][tool] = stats[\"tool_usage\"].get(tool, 0) + 1\n","\n","    def get_performance_summary(self) -> Dict[str, Any]:\n","        \"\"\"Get comprehensive performance summary\"\"\"\n","        total_executions = len(self.execution_history)\n","        successful_executions = sum(1 for record in self.execution_history if record[\"success\"])\n","\n","        if total_executions == 0:\n","            return {\"total_executions\": 0, \"success_rate\": 0}\n","\n","        success_rate = successful_executions / total_executions\n","        avg_duration = sum(record[\"duration\"] for record in self.execution_history) / total_executions\n","\n","        # Agent type breakdown\n","        agent_type_stats = {}\n","        for agent_type, stats in self.performance_stats.items():\n","            agent_type_stats[agent_type] = {\n","                \"success_rate\": stats[\"successful_executions\"] / stats[\"total_executions\"],\n","                \"average_duration\": stats[\"average_duration\"],\n","                \"total_executions\": stats[\"total_executions\"],\n","                \"most_used_tools\": sorted(stats[\"tool_usage\"].items(), key=lambda x: x[1], reverse=True)[:5]\n","            }\n","\n","        return {\n","            \"total_executions\": total_executions,\n","            \"success_rate\": success_rate,\n","            \"average_duration\": avg_duration,\n","            \"agent_type_breakdown\": agent_type_stats,\n","            \"recent_performance\": self._get_recent_performance()\n","        }\n","\n","    def _get_recent_performance(self) -> Dict[str, Any]:\n","        \"\"\"Get recent performance metrics (last 24 hours)\"\"\"\n","        cutoff_time = datetime.now() - timedelta(hours=24)\n","        recent_records = [r for r in self.execution_history if r[\"timestamp\"] > cutoff_time]\n","\n","        if not recent_records:\n","            return {\"recent_executions\": 0, \"recent_success_rate\": 0}\n","\n","        recent_success_rate = sum(1 for r in recent_records if r[\"success\"]) / len(recent_records)\n","        recent_avg_duration = sum(r[\"duration\"] for r in recent_records) / len(recent_records)\n","\n","        return {\n","            \"recent_executions\": len(recent_records),\n","            \"recent_success_rate\": recent_success_rate,\n","            \"recent_average_duration\": recent_avg_duration\n","        }\n","\n","# Advanced LangChain Agent Factory\n","class ProductionAgentFactory:\n","    \"\"\"Factory for creating production-ready LangChain agents\"\"\"\n","\n","    def __init__(self, llm_model: str = \"gpt-3.5-turbo\", metrics: AgentMetrics = None):\n","        self.llm = ChatOpenAI(model=llm_model, temperature=0.1)\n","        self.metrics = metrics or AgentMetrics()\n","        self.tools_registry = {}\n","        self.agent_cache = {}\n","\n","    def register_tool(self, tool: Tool):\n","        \"\"\"Register a tool in the factory\"\"\"\n","        self.tools_registry[tool.name] = tool\n","        print(f\"âœ… Registered tool: {tool.name}\")\n","\n","    def create_react_agent(self, tools: List[Tool], memory: bool = True) -> AgentExecutor:\n","        \"\"\"Create a ReAct agent with monitoring and error handling\"\"\"\n","\n","        # Create memory if requested\n","        agent_memory = None\n","        if memory:\n","            agent_memory = ConversationBufferMemory(\n","                memory_key=\"chat_history\",\n","                return_messages=True\n","            )\n","\n","        # Create custom prompt template\n","        react_prompt = PromptTemplate.from_template(\"\"\"\n","You are an intelligent assistant with access to tools. Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [{tool_names}]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Available tools: {tools}\n","\n","Question: {input}\n","Thought: {agent_scratchpad}\n","        \"\"\")\n","\n","        # Create the agent\n","        agent = create_react_agent(\n","            llm=self.llm,\n","            tools=tools,\n","            prompt=react_prompt\n","        )\n","\n","        # Create executor with monitoring callback\n","        monitoring_callback = ProductionCallbackHandler(self.metrics, AgentTypeEnum.REACT.value)\n","\n","        executor = AgentExecutor(\n","            agent=agent,\n","            tools=tools,\n","            memory=agent_memory,\n","            verbose=True,\n","            max_iterations=10,\n","            max_execution_time=300,  # 5 minutes timeout\n","            callbacks=[monitoring_callback],\n","            handle_parsing_errors=True,\n","            return_intermediate_steps=True\n","        )\n","\n","        return executor\n","\n","    def create_structured_chat_agent(self, tools: List[Tool], system_message: str = None) -> AgentExecutor:\n","        \"\"\"Create a structured chat agent for complex conversations\"\"\"\n","\n","        # Default system message\n","        if system_message is None:\n","            system_message = \"\"\"You are a helpful AI assistant with access to tools.\n","            You can use tools to gather information, perform calculations, and complete tasks.\n","            Always explain your reasoning and provide clear, helpful responses.\n","\n","            You have access to the following tools:\n","            {tools}\n","\n","            Use the following format:\n","\n","            Question: the input question you must answer\n","            Thought: you should always think about what to do\n","            Action: the action to take, should be one of [{tool_names}]\n","            Action Input: the input to the action\n","            Observation: the result of the action\n","            ... (this Thought/Action/Action Input/Observation can repeat N times)\n","            Thought: I now know the final answer\n","            Final Answer: the final answer to the original input question\"\"\"\n","\n","    def create_structured_chat_agent(self, tools: List[Tool], system_message: str = None) -> AgentExecutor:\n","        \"\"\"Create a structured chat agent for complex conversations\"\"\"\n","\n","    def create_structured_chat_agent(self, tools: List[Tool], system_message: str = None) -> AgentExecutor:\n","        \"\"\"Create a simple ReAct agent (same as create_react_agent but with different name)\"\"\"\n","\n","        # Just create another ReAct agent since structured chat is problematic\n","        return self.create_react_agent(tools, memory=True)\n","\n","    def create_pandas_agent(self, dataframe: pd.DataFrame, description: str = \"\") -> AgentExecutor:\n","        \"\"\"Create a pandas dataframe agent for data analysis\"\"\"\n","\n","        # Create the pandas agent with minimal parameters to avoid issues\n","        agent_executor = create_pandas_dataframe_agent(\n","            llm=self.llm,\n","            df=dataframe,\n","            verbose=True,\n","            handle_parsing_errors=True\n","        )\n","\n","        return agent_executor\n","\n","class ProductionCallbackHandler(BaseCallbackHandler):\n","    \"\"\"Production callback handler with comprehensive monitoring\"\"\"\n","\n","    def __init__(self, metrics: AgentMetrics, agent_type: str):\n","        self.metrics = metrics\n","        self.agent_type = agent_type\n","        self.start_time = None\n","        self.tools_used = []\n","        self.steps = []\n","        self.errors = []\n","\n","    def on_agent_action(self, action: AgentAction, **kwargs) -> None:\n","        \"\"\"Record agent actions and tool usage\"\"\"\n","        if self.start_time is None:\n","            self.start_time = time.time()\n","\n","        self.tools_used.append(action.tool)\n","        self.steps.append({\n","            \"step_type\": \"action\",\n","            \"tool\": action.tool,\n","            \"input\": action.tool_input,\n","            \"timestamp\": datetime.now()\n","        })\n","\n","        print(f\"ðŸ”§ Using tool: {action.tool}\")\n","\n","    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n","        \"\"\"Record successful completion\"\"\"\n","        duration = time.time() - self.start_time if self.start_time else 0\n","        self.metrics.record_execution(\n","            agent_type=self.agent_type,\n","            duration=duration,\n","            success=True,\n","            tools_used=self.tools_used\n","        )\n","\n","        print(f\"âœ… Agent completed successfully in {duration:.2f}s\")\n","\n","    def on_chain_error(self, error: Exception, **kwargs) -> None:\n","        \"\"\"Record errors and failures\"\"\"\n","        duration = time.time() - self.start_time if self.start_time else 0\n","        self.errors.append(str(error))\n","\n","        self.metrics.record_execution(\n","            agent_type=self.agent_type,\n","            duration=duration,\n","            success=False,\n","            tools_used=self.tools_used\n","        )\n","\n","        print(f\"âŒ Agent failed after {duration:.2f}s: {error}\")\n","\n","    def get_execution_trace(self) -> Dict[str, Any]:\n","        \"\"\"Get detailed execution trace\"\"\"\n","        return {\n","            \"agent_type\": self.agent_type,\n","            \"steps\": self.steps,\n","            \"tools_used\": self.tools_used,\n","            \"errors\": self.errors,\n","            \"total_duration\": time.time() - self.start_time if self.start_time else 0\n","        }\n","\n","# Production-ready tools with monitoring\n","@tool\n","def enhanced_web_search_tool(query: str) -> str:\n","    \"\"\"Enhanced web search with caching and error handling\"\"\"\n","    try:\n","        # Simulate web search with error handling\n","        import requests\n","        from urllib.parse import quote\n","\n","        # Add delay to simulate real search\n","        time.sleep(0.5)\n","\n","        # Simulate search results\n","        search_results = {\n","            \"artificial intelligence\": \"AI continues to advance rapidly with new developments in LLMs, computer vision, and robotics.\",\n","            \"climate change\": \"Latest climate research shows accelerating impacts and the urgent need for action.\",\n","            \"stock market\": \"Markets show mixed signals with technology stocks leading gains while energy sector faces headwinds.\",\n","            \"quantum computing\": \"Quantum computing advances with new error correction breakthroughs and commercial applications.\"\n","        }\n","\n","        # Find relevant results\n","        query_lower = query.lower()\n","        for topic, result in search_results.items():\n","            if topic in query_lower:\n","                return f\"Search results for '{query}': {result}\"\n","\n","        return f\"Search results for '{query}': General information found, but no specific match in simulation.\"\n","\n","    except Exception as e:\n","        return f\"Search failed: {str(e)}\"\n","\n","@tool\n","def enhanced_calculator_tool(expression: str) -> str:\n","    \"\"\"Enhanced calculator with validation and error handling\"\"\"\n","    try:\n","        # Validate expression\n","        allowed_chars = set('0123456789+-*/()., ')\n","        if not all(c in allowed_chars for c in expression):\n","            return \"Error: Expression contains invalid characters\"\n","\n","        # Evaluate safely\n","        result = eval(expression, {\"__builtins__\": {}}, {})\n","        return f\"Calculation result: {result}\"\n","\n","    except ZeroDivisionError:\n","        return \"Error: Division by zero\"\n","    except Exception as e:\n","        return f\"Calculation error: {str(e)}\"\n","\n","@tool\n","def data_analysis_tool(data_description: str) -> str:\n","    \"\"\"Analyze data patterns and provide insights\"\"\"\n","    try:\n","        # Simulate data analysis\n","        analysis_types = {\n","            \"trend\": \"Analysis shows upward trend with 15% growth over the analyzed period\",\n","            \"correlation\": \"Strong positive correlation (r=0.78) detected between variables\",\n","            \"distribution\": \"Data follows normal distribution with mean=50.2, std=12.8\",\n","            \"anomaly\": \"3 anomalies detected outside 2-sigma range\",\n","            \"forecast\": \"Predictive model suggests 12% growth in next quarter\"\n","        }\n","\n","        data_lower = data_description.lower()\n","        for analysis_type, result in analysis_types.items():\n","            if analysis_type in data_lower:\n","                return f\"Data analysis result: {result}\"\n","\n","        return f\"Data analysis completed for: {data_description}. Multiple factors analyzed.\"\n","\n","    except Exception as e:\n","        return f\"Analysis failed: {str(e)}\"\n","\n","print(\"âœ… LangChain Agent Factory and tools initialized successfully\")"],"metadata":{"id":"0XKsOgx-JY33"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 2: ADVANCED PATTERNS (13.7.3)**"],"metadata":{"id":"npVWQ75KKOQK"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 2: ADVANCED PATTERNS\")\n","print(\"=\"*60)\n","\n","class SelfImprovementAgent:\n","    \"\"\"Agent that learns and improves from its own performance\"\"\"\n","\n","    def __init__(self, base_agent: AgentExecutor, metrics: AgentMetrics):\n","        self.base_agent = base_agent\n","        self.metrics = metrics\n","        self.improvement_history = []\n","        self.performance_threshold = 0.8  # Minimum acceptable success rate\n","        self.learning_rate = 0.1\n","\n","    def execute_with_learning(self, query: str, feedback: Optional[str] = None) -> Dict[str, Any]:\n","        \"\"\"Execute query and learn from results\"\"\"\n","        start_time = time.time()\n","\n","        try:\n","            # Execute the query\n","            result = self.base_agent.invoke({\"input\": query})\n","            execution_time = time.time() - start_time\n","            success = True\n","\n","            # Analyze performance\n","            performance_analysis = self._analyze_performance(query, result, execution_time, feedback)\n","\n","            # Store improvement insights\n","            self.improvement_history.append({\n","                \"timestamp\": datetime.now(),\n","                \"query\": query,\n","                \"execution_time\": execution_time,\n","                \"success\": success,\n","                \"performance_analysis\": performance_analysis,\n","                \"feedback\": feedback\n","            })\n","\n","            # Apply improvements if needed\n","            self._apply_improvements(performance_analysis)\n","\n","            return {\n","                \"result\": result,\n","                \"execution_time\": execution_time,\n","                \"success\": success,\n","                \"performance_analysis\": performance_analysis,\n","                \"improvements_applied\": len(performance_analysis.get(\"improvements\", []))\n","            }\n","\n","        except Exception as e:\n","            execution_time = time.time() - start_time\n","            error_analysis = self._analyze_error(query, str(e), execution_time)\n","\n","            self.improvement_history.append({\n","                \"timestamp\": datetime.now(),\n","                \"query\": query,\n","                \"execution_time\": execution_time,\n","                \"success\": False,\n","                \"error\": str(e),\n","                \"error_analysis\": error_analysis\n","            })\n","\n","            return {\n","                \"result\": None,\n","                \"execution_time\": execution_time,\n","                \"success\": False,\n","                \"error\": str(e),\n","                \"error_analysis\": error_analysis\n","            }\n","\n","    def _analyze_performance(self, query: str, result: Dict, execution_time: float, feedback: str = None) -> Dict:\n","        \"\"\"Analyze agent performance and identify improvement opportunities\"\"\"\n","        analysis = {\n","            \"execution_time_rating\": \"good\" if execution_time < 30 else \"slow\",\n","            \"result_completeness\": self._assess_completeness(result),\n","            \"tool_usage_efficiency\": self._assess_tool_efficiency(result),\n","            \"improvements\": []\n","        }\n","\n","        # Check execution time\n","        if execution_time > 60:\n","            analysis[\"improvements\"].append({\n","                \"type\": \"performance\",\n","                \"description\": \"Execution time too long, consider parallel tool usage\",\n","                \"priority\": \"high\"\n","            })\n","\n","        # Check tool usage patterns\n","        if \"intermediate_steps\" in result:\n","            steps = result[\"intermediate_steps\"]\n","            if len(steps) > 10:\n","                analysis[\"improvements\"].append({\n","                    \"type\": \"efficiency\",\n","                    \"description\": \"Too many steps, improve planning strategy\",\n","                    \"priority\": \"medium\"\n","                })\n","\n","        # Incorporate user feedback\n","        if feedback:\n","            if \"incomplete\" in feedback.lower():\n","                analysis[\"improvements\"].append({\n","                    \"type\": \"completeness\",\n","                    \"description\": \"User indicates incomplete response, improve thoroughness\",\n","                    \"priority\": \"high\"\n","                })\n","            elif \"slow\" in feedback.lower():\n","                analysis[\"improvements\"].append({\n","                    \"type\": \"performance\",\n","                    \"description\": \"User indicates slow response, optimize execution\",\n","                    \"priority\": \"medium\"\n","                })\n","\n","        return analysis\n","\n","    def _assess_completeness(self, result: Dict) -> str:\n","        \"\"\"Assess the completeness of the agent's response\"\"\"\n","        output = result.get(\"output\", \"\")\n","        if len(output) < 50:\n","            return \"incomplete\"\n","        elif \"I don't know\" in output or \"not found\" in output:\n","            return \"partial\"\n","        else:\n","            return \"complete\"\n","\n","    def _assess_tool_efficiency(self, result: Dict) -> str:\n","        \"\"\"Assess how efficiently tools were used\"\"\"\n","        if \"intermediate_steps\" not in result:\n","            return \"unknown\"\n","\n","        steps = result[\"intermediate_steps\"]\n","        unique_tools = set()\n","        repeated_tools = 0\n","\n","        for step in steps:\n","            if hasattr(step, 'tool'):\n","                tool_name = step.tool\n","                if tool_name in unique_tools:\n","                    repeated_tools += 1\n","                unique_tools.add(tool_name)\n","\n","        if repeated_tools > len(unique_tools):\n","            return \"inefficient\"\n","        elif repeated_tools == 0:\n","            return \"optimal\"\n","        else:\n","            return \"acceptable\"\n","\n","    def _analyze_error(self, query: str, error: str, execution_time: float) -> Dict:\n","        \"\"\"Analyze errors to identify improvement opportunities\"\"\"\n","        analysis = {\n","            \"error_type\": self._classify_error(error),\n","            \"suggestions\": []\n","        }\n","\n","        if \"timeout\" in error.lower():\n","            analysis[\"suggestions\"].append(\"Reduce max_iterations or improve tool selection\")\n","        elif \"parsing\" in error.lower():\n","            analysis[\"suggestions\"].append(\"Improve prompt clarity or add parsing error handling\")\n","        elif \"rate limit\" in error.lower():\n","            analysis[\"suggestions\"].append(\"Implement rate limiting and retry logic\")\n","\n","        return analysis\n","\n","    def _classify_error(self, error: str) -> str:\n","        \"\"\"Classify error types for targeted improvements\"\"\"\n","        error_lower = error.lower()\n","        if \"timeout\" in error_lower:\n","            return \"timeout\"\n","        elif \"parsing\" in error_lower:\n","            return \"parsing\"\n","        elif \"rate limit\" in error_lower:\n","            return \"rate_limit\"\n","        elif \"authentication\" in error_lower:\n","            return \"auth\"\n","        else:\n","            return \"unknown\"\n","\n","    def _apply_improvements(self, analysis: Dict):\n","        \"\"\"Apply identified improvements to the agent\"\"\"\n","        improvements = analysis.get(\"improvements\", [])\n","\n","        for improvement in improvements:\n","            if improvement[\"priority\"] == \"high\":\n","                if improvement[\"type\"] == \"performance\":\n","                    # Adjust execution parameters\n","                    if hasattr(self.base_agent, 'max_execution_time'):\n","                        self.base_agent.max_execution_time = min(\n","                            self.base_agent.max_execution_time * 1.2, 600\n","                        )\n","                elif improvement[\"type\"] == \"efficiency\":\n","                    # Adjust iteration limits\n","                    if hasattr(self.base_agent, 'max_iterations'):\n","                        self.base_agent.max_iterations = max(\n","                            self.base_agent.max_iterations - 2, 5\n","                        )\n","\n","    def get_learning_insights(self) -> Dict[str, Any]:\n","        \"\"\"Get insights from learning history\"\"\"\n","        if not self.improvement_history:\n","            return {\"total_interactions\": 0}\n","\n","        total_interactions = len(self.improvement_history)\n","        successful_interactions = sum(1 for h in self.improvement_history if h[\"success\"])\n","        success_rate = successful_interactions / total_interactions\n","\n","        avg_execution_time = sum(h[\"execution_time\"] for h in self.improvement_history) / total_interactions\n","\n","        # Common improvement patterns\n","        improvement_types = {}\n","        for history in self.improvement_history:\n","            if \"performance_analysis\" in history:\n","                for improvement in history[\"performance_analysis\"].get(\"improvements\", []):\n","                    imp_type = improvement[\"type\"]\n","                    improvement_types[imp_type] = improvement_types.get(imp_type, 0) + 1\n","\n","        return {\n","            \"total_interactions\": total_interactions,\n","            \"success_rate\": success_rate,\n","            \"average_execution_time\": avg_execution_time,\n","            \"common_improvements\": improvement_types,\n","            \"recent_performance_trend\": self._calculate_performance_trend()\n","        }\n","\n","    def _calculate_performance_trend(self) -> str:\n","        \"\"\"Calculate performance trend over recent interactions\"\"\"\n","        if len(self.improvement_history) < 10:\n","            return \"insufficient_data\"\n","\n","        recent_10 = self.improvement_history[-10:]\n","        earlier_10 = self.improvement_history[-20:-10] if len(self.improvement_history) >= 20 else []\n","\n","        if not earlier_10:\n","            return \"insufficient_data\"\n","\n","        recent_success_rate = sum(1 for h in recent_10 if h[\"success\"]) / len(recent_10)\n","        earlier_success_rate = sum(1 for h in earlier_10 if h[\"success\"]) / len(earlier_10)\n","\n","        if recent_success_rate > earlier_success_rate + 0.1:\n","            return \"improving\"\n","        elif recent_success_rate < earlier_success_rate - 0.1:\n","            return \"declining\"\n","        else:\n","            return \"stable\"\n","\n","class GoalOrientedAgent:\n","    \"\"\"Agent that maintains and pursues long-term goals\"\"\"\n","\n","    def __init__(self, base_agent: AgentExecutor):\n","        self.base_agent = base_agent\n","        self.active_goals = []\n","        self.completed_goals = []\n","        self.goal_strategies = {}\n","\n","    def set_goal(self, goal_description: str, priority: int = 1, deadline: datetime = None) -> str:\n","        \"\"\"Set a new goal for the agent\"\"\"\n","        goal_id = hashlib.md5(f\"{goal_description}{datetime.now()}\".encode()).hexdigest()[:8]\n","\n","        goal = {\n","            \"id\": goal_id,\n","            \"description\": goal_description,\n","            \"priority\": priority,\n","            \"created_at\": datetime.now(),\n","            \"deadline\": deadline,\n","            \"status\": \"active\",\n","            \"progress\": 0.0,\n","            \"subtasks\": [],\n","            \"strategy\": None\n","        }\n","\n","        # Generate strategy for achieving the goal\n","        strategy = self._generate_goal_strategy(goal_description)\n","        goal[\"strategy\"] = strategy\n","        goal[\"subtasks\"] = strategy.get(\"subtasks\", [])\n","\n","        self.active_goals.append(goal)\n","        self.active_goals.sort(key=lambda x: x[\"priority\"], reverse=True)\n","\n","        print(f\"ðŸŽ¯ New goal set: {goal_description} (ID: {goal_id})\")\n","        return goal_id\n","\n","    def _generate_goal_strategy(self, goal_description: str) -> Dict:\n","        \"\"\"Generate a strategy for achieving a goal\"\"\"\n","        # Simple strategy generation based on goal type\n","        if \"research\" in goal_description.lower():\n","            return {\n","                \"type\": \"research\",\n","                \"subtasks\": [\n","                    \"Identify key information sources\",\n","                    \"Gather relevant data and evidence\",\n","                    \"Analyze and synthesize findings\",\n","                    \"Prepare comprehensive summary\"\n","                ],\n","                \"estimated_duration\": \"2-3 hours\",\n","                \"required_tools\": [\"web_search\", \"document_analysis\"]\n","            }\n","        elif \"analysis\" in goal_description.lower():\n","            return {\n","                \"type\": \"analysis\",\n","                \"subtasks\": [\n","                    \"Define analysis framework\",\n","                    \"Collect relevant data\",\n","                    \"Perform quantitative analysis\",\n","                    \"Generate insights and recommendations\"\n","                ],\n","                \"estimated_duration\": \"1-2 hours\",\n","                \"required_tools\": [\"calculator\", \"data_analysis\"]\n","            }\n","        elif \"monitoring\" in goal_description.lower():\n","            return {\n","                \"type\": \"monitoring\",\n","                \"subtasks\": [\n","                    \"Set up monitoring parameters\",\n","                    \"Establish baseline metrics\",\n","                    \"Schedule regular checks\",\n","                    \"Create alert mechanisms\"\n","                ],\n","                \"estimated_duration\": \"ongoing\",\n","                \"required_tools\": [\"web_search\", \"data_collection\"]\n","            }\n","        else:\n","            return {\n","                \"type\": \"general\",\n","                \"subtasks\": [\n","                    \"Break down goal into actionable steps\",\n","                    \"Execute steps systematically\",\n","                    \"Monitor progress and adjust\",\n","                    \"Complete and verify results\"\n","                ],\n","                \"estimated_duration\": \"variable\",\n","                \"required_tools\": [\"general_purpose\"]\n","            }\n","\n","    def work_towards_goals(self, time_limit: int = 300) -> Dict[str, Any]:\n","        \"\"\"Work towards active goals within time limit\"\"\"\n","        start_time = time.time()\n","        work_results = {\n","            \"goals_worked_on\": [],\n","            \"progress_made\": {},\n","            \"completed_goals\": [],\n","            \"total_time_spent\": 0\n","        }\n","\n","        for goal in sorted(self.active_goals, key=lambda x: x[\"priority\"], reverse=True):\n","            if time.time() - start_time >= time_limit:\n","                break\n","\n","            print(f\"\\nðŸŽ¯ Working on goal: {goal['description']}\")\n","            goal_work_result = self._work_on_goal(goal, time_limit - (time.time() - start_time))\n","\n","            work_results[\"goals_worked_on\"].append(goal[\"id\"])\n","            work_results[\"progress_made\"][goal[\"id\"]] = goal_work_result\n","\n","            # Update goal progress\n","            goal[\"progress\"] = goal_work_result.get(\"progress\", goal[\"progress\"])\n","\n","            # Check if goal is completed\n","            if goal[\"progress\"] >= 1.0:\n","                goal[\"status\"] = \"completed\"\n","                goal[\"completed_at\"] = datetime.now()\n","                self.completed_goals.append(goal)\n","                self.active_goals.remove(goal)\n","                work_results[\"completed_goals\"].append(goal[\"id\"])\n","                print(f\"âœ… Goal completed: {goal['description']}\")\n","\n","        work_results[\"total_time_spent\"] = time.time() - start_time\n","        return work_results\n","\n","    def _work_on_goal(self, goal: Dict, time_limit: float) -> Dict:\n","        \"\"\"Work on a specific goal\"\"\"\n","        start_time = time.time()\n","\n","        # Find next subtask to work on\n","        subtasks = goal[\"subtasks\"]\n","        current_subtask_index = int(goal[\"progress\"] * len(subtasks))\n","\n","        if current_subtask_index >= len(subtasks):\n","            return {\"progress\": 1.0, \"status\": \"completed\"}\n","\n","        current_subtask = subtasks[current_subtask_index]\n","        print(f\"ðŸ“‹ Working on subtask: {current_subtask}\")\n","\n","        try:\n","            # Execute subtask using base agent\n","            query = f\"Complete this subtask for the goal '{goal['description']}': {current_subtask}\"\n","            result = self.base_agent.invoke({\"input\": query})\n","\n","            # Calculate progress\n","            new_progress = (current_subtask_index + 1) / len(subtasks)\n","\n","            return {\n","                \"progress\": new_progress,\n","                \"subtask_completed\": current_subtask,\n","                \"result\": result.get(\"output\", \"\"),\n","                \"time_spent\": time.time() - start_time,\n","                \"status\": \"in_progress\"\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"progress\": goal[\"progress\"],  # No progress made\n","                \"error\": str(e),\n","                \"time_spent\": time.time() - start_time,\n","                \"status\": \"error\"\n","            }\n","\n","    def get_goal_status(self) -> Dict[str, Any]:\n","        \"\"\"Get status of all goals\"\"\"\n","        return {\n","            \"active_goals\": len(self.active_goals),\n","            \"completed_goals\": len(self.completed_goals),\n","            \"goals_by_priority\": {\n","                f\"priority_{p}\": len([g for g in self.active_goals if g[\"priority\"] == p])\n","                for p in range(1, 6)\n","            },\n","            \"average_progress\": sum(g[\"progress\"] for g in self.active_goals) / max(len(self.active_goals), 1),\n","            \"goals_near_deadline\": len([\n","                g for g in self.active_goals\n","                if g.get(\"deadline\") and g[\"deadline\"] < datetime.now() + timedelta(days=1)\n","            ])\n","        }"],"metadata":{"id":"f2IvsqroKOaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 3: EVALUATION AND PRODUCTION DEPLOYMENT (13.8)**"],"metadata":{"id":"9n65hJ_jKYGl"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 3: EVALUATION AND PRODUCTION DEPLOYMENT\")\n","print(\"=\"*60)\n","\n","class AgentEvaluationFramework:\n","    \"\"\"Comprehensive evaluation framework for agentic RAG systems\"\"\"\n","\n","    def __init__(self):\n","        self.evaluation_metrics = {\n","            \"accuracy\": [],\n","            \"completeness\": [],\n","            \"efficiency\": [],\n","            \"user_satisfaction\": [],\n","            \"tool_usage_effectiveness\": [],\n","            \"response_time\": [],\n","            \"error_rate\": [],\n","            \"cost_efficiency\": []\n","        }\n","        self.benchmark_datasets = {}\n","        self.evaluation_history = []\n","\n","    def add_benchmark_dataset(self, name: str, queries: List[Dict], expected_outputs: List[Dict]):\n","        \"\"\"Add a benchmark dataset for evaluation\"\"\"\n","        self.benchmark_datasets[name] = {\n","            \"queries\": queries,\n","            \"expected_outputs\": expected_outputs,\n","            \"created_at\": datetime.now()\n","        }\n","        print(f\"ðŸ“Š Added benchmark dataset: {name} ({len(queries)} queries)\")\n","\n","    def evaluate_agent_performance(self, agent: AgentExecutor, dataset_name: str,\n","                                 sample_size: int = None) -> Dict[str, Any]:\n","        \"\"\"Evaluate agent performance against benchmark dataset\"\"\"\n","        if dataset_name not in self.benchmark_datasets:\n","            raise ValueError(f\"Dataset {dataset_name} not found\")\n","\n","        dataset = self.benchmark_datasets[dataset_name]\n","        queries = dataset[\"queries\"]\n","        expected_outputs = dataset[\"expected_outputs\"]\n","\n","        # Sample queries if requested\n","        if sample_size and sample_size < len(queries):\n","            import random\n","            indices = random.sample(range(len(queries)), sample_size)\n","            queries = [queries[i] for i in indices]\n","            expected_outputs = [expected_outputs[i] for i in indices]\n","\n","        evaluation_results = {\n","            \"dataset_name\": dataset_name,\n","            \"total_queries\": len(queries),\n","            \"successful_queries\": 0,\n","            \"failed_queries\": 0,\n","            \"individual_results\": [],\n","            \"aggregate_metrics\": {},\n","            \"timestamp\": datetime.now()\n","        }\n","\n","        print(f\"ðŸ§ª Evaluating agent on {len(queries)} queries from {dataset_name}\")\n","\n","        for i, (query, expected) in enumerate(zip(queries, expected_outputs)):\n","            print(f\"Query {i+1}/{len(queries)}: {query.get('text', '')[:50]}...\")\n","\n","            result = self._evaluate_single_query(agent, query, expected)\n","            evaluation_results[\"individual_results\"].append(result)\n","\n","            if result[\"success\"]:\n","                evaluation_results[\"successful_queries\"] += 1\n","            else:\n","                evaluation_results[\"failed_queries\"] += 1\n","\n","        # Calculate aggregate metrics\n","        evaluation_results[\"aggregate_metrics\"] = self._calculate_aggregate_metrics(\n","            evaluation_results[\"individual_results\"]\n","        )\n","\n","        # Store evaluation history\n","        self.evaluation_history.append(evaluation_results)\n","\n","        return evaluation_results\n","\n","    def _evaluate_single_query(self, agent: AgentExecutor, query: Dict, expected: Dict) -> Dict:\n","        \"\"\"Evaluate a single query-response pair\"\"\"\n","        start_time = time.time()\n","\n","        try:\n","            # Execute query\n","            response = agent.invoke({\"input\": query.get(\"text\", \"\")})\n","            execution_time = time.time() - start_time\n","\n","            # Extract actual output\n","            actual_output = response.get(\"output\", \"\")\n","\n","            # Calculate metrics\n","            accuracy_score = self._calculate_accuracy(actual_output, expected.get(\"text\", \"\"))\n","            completeness_score = self._calculate_completeness(actual_output, expected.get(\"key_points\", []))\n","            efficiency_score = self._calculate_efficiency(response.get(\"intermediate_steps\", []))\n","\n","            return {\n","                \"query_id\": query.get(\"id\", \"\"),\n","                \"success\": True,\n","                \"execution_time\": execution_time,\n","                \"actual_output\": actual_output,\n","                \"expected_output\": expected.get(\"text\", \"\"),\n","                \"metrics\": {\n","                    \"accuracy\": accuracy_score,\n","                    \"completeness\": completeness_score,\n","                    \"efficiency\": efficiency_score,\n","                    \"response_time\": execution_time\n","                },\n","                \"tools_used\": self._extract_tools_used(response.get(\"intermediate_steps\", []))\n","            }\n","\n","        except Exception as e:\n","            execution_time = time.time() - start_time\n","            return {\n","                \"query_id\": query.get(\"id\", \"\"),\n","                \"success\": False,\n","                \"execution_time\": execution_time,\n","                \"error\": str(e),\n","                \"metrics\": {\n","                    \"accuracy\": 0.0,\n","                    \"completeness\": 0.0,\n","                    \"efficiency\": 0.0,\n","                    \"response_time\": execution_time\n","                }\n","            }\n","\n","    def _calculate_accuracy(self, actual: str, expected: str) -> float:\n","        \"\"\"Calculate accuracy score using simple text similarity\"\"\"\n","        if not expected:\n","            return 1.0 if actual else 0.0\n","\n","        # Simple word overlap metric\n","        actual_words = set(actual.lower().split())\n","        expected_words = set(expected.lower().split())\n","\n","        if not expected_words:\n","            return 1.0\n","\n","        overlap = len(actual_words.intersection(expected_words))\n","        return overlap / len(expected_words)\n","\n","    def _calculate_completeness(self, actual: str, key_points: List[str]) -> float:\n","        \"\"\"Calculate completeness based on key points coverage\"\"\"\n","        if not key_points:\n","            return 1.0\n","\n","        actual_lower = actual.lower()\n","        covered_points = 0\n","\n","        for point in key_points:\n","            if point.lower() in actual_lower:\n","                covered_points += 1\n","\n","        return covered_points / len(key_points)\n","\n","    def _calculate_efficiency(self, intermediate_steps: List) -> float:\n","        \"\"\"Calculate efficiency score based on execution steps\"\"\"\n","        if not intermediate_steps:\n","            return 1.0\n","\n","        # Penalize excessive steps\n","        optimal_steps = 3  # Assume 3 steps is optimal\n","        actual_steps = len(intermediate_steps)\n","\n","        if actual_steps <= optimal_steps:\n","            return 1.0\n","        else:\n","            return max(0.0, 1.0 - (actual_steps - optimal_steps) * 0.1)\n","\n","    def _extract_tools_used(self, intermediate_steps: List) -> List[str]:\n","        \"\"\"Extract tools used from intermediate steps\"\"\"\n","        tools = []\n","        for step in intermediate_steps:\n","            if hasattr(step, 'tool'):\n","                tools.append(step.tool)\n","            elif isinstance(step, tuple) and len(step) > 0:\n","                if hasattr(step[0], 'tool'):\n","                    tools.append(step[0].tool)\n","        return tools\n","\n","    def _calculate_aggregate_metrics(self, individual_results: List[Dict]) -> Dict[str, float]:\n","        \"\"\"Calculate aggregate metrics from individual results\"\"\"\n","        successful_results = [r for r in individual_results if r[\"success\"]]\n","\n","        if not successful_results:\n","            return {\n","                \"average_accuracy\": 0.0,\n","                \"average_completeness\": 0.0,\n","                \"average_efficiency\": 0.0,\n","                \"average_response_time\": 0.0,\n","                \"success_rate\": 0.0,\n","                \"error_rate\": 1.0\n","            }\n","\n","        metrics = {\n","            \"average_accuracy\": sum(r[\"metrics\"][\"accuracy\"] for r in successful_results) / len(successful_results),\n","            \"average_completeness\": sum(r[\"metrics\"][\"completeness\"] for r in successful_results) / len(successful_results),\n","            \"average_efficiency\": sum(r[\"metrics\"][\"efficiency\"] for r in successful_results) / len(successful_results),\n","            \"average_response_time\": sum(r[\"metrics\"][\"response_time\"] for r in successful_results) / len(successful_results),\n","            \"success_rate\": len(successful_results) / len(individual_results),\n","            \"error_rate\": (len(individual_results) - len(successful_results)) / len(individual_results)\n","        }\n","\n","        return metrics\n","\n","    def generate_evaluation_report(self, evaluation_results: Dict) -> str:\n","        \"\"\"Generate human-readable evaluation report\"\"\"\n","        metrics = evaluation_results[\"aggregate_metrics\"]\n","\n","        report = f\"\"\"\n","=== AGENT EVALUATION REPORT ===\n","Dataset: {evaluation_results['dataset_name']}\n","Evaluation Date: {evaluation_results['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n","Total Queries: {evaluation_results['total_queries']}\n","\n","PERFORMANCE METRICS:\n","âœ“ Success Rate: {metrics['success_rate']:.2%}\n","âœ“ Average Accuracy: {metrics['average_accuracy']:.2%}\n","âœ“ Average Completeness: {metrics['average_completeness']:.2%}\n","âœ“ Average Efficiency: {metrics['average_efficiency']:.2%}\n","âœ“ Average Response Time: {metrics['average_response_time']:.2f}s\n","âœ— Error Rate: {metrics['error_rate']:.2%}\n","\n","PERFORMANCE RATING:\n","\"\"\"\n","\n","        # Calculate overall rating\n","        overall_score = sum([\n","            metrics['success_rate'],\n","            metrics['average_accuracy'],\n","            metrics['average_completeness'],\n","            metrics['average_efficiency']\n","        ]) / 4\n","\n","        if overall_score >= 0.9:\n","            report += \"ðŸŒŸ EXCELLENT - Ready for production deployment\"\n","        elif overall_score >= 0.8:\n","            report += \"âœ… GOOD - Minor optimizations recommended\"\n","        elif overall_score >= 0.7:\n","            report += \"âš ï¸ ACCEPTABLE - Significant improvements needed\"\n","        else:\n","            report += \"âŒ POOR - Major issues must be addressed\"\n","\n","        return report\n","\n","print(\"âœ… Advanced Patterns and Evaluation Framework initialized successfully\")"],"metadata":{"id":"D88A7R9SKYNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 4: PRODUCTION MONITORING AND OBSERVABILITY**"],"metadata":{"id":"UDatcMQpK-xy"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 4: PRODUCTION MONITORING AND OBSERVABILITY\")\n","print(\"=\"*60)\n","\n","class ProductionMonitor:\n","    \"\"\"Comprehensive monitoring system for production agentic RAG deployments\"\"\"\n","\n","    def __init__(self, alert_thresholds: Dict[str, float] = None):\n","        self.metrics_store = {}\n","        self.alert_thresholds = alert_thresholds or {\n","            \"error_rate\": 0.05,  # 5% error rate threshold\n","            \"avg_response_time\": 30.0,  # 30 seconds\n","            \"success_rate\": 0.95,  # 95% success rate\n","            \"memory_usage\": 0.85,  # 85% memory usage\n","            \"cpu_usage\": 0.80  # 80% CPU usage\n","        }\n","        self.alerts = []\n","        self.health_checks = []\n","        self.system_stats = {}\n","\n","    def record_system_metrics(self):\n","        \"\"\"Record system-level metrics\"\"\"\n","        import psutil\n","\n","        self.system_stats = {\n","            \"timestamp\": datetime.now(),\n","            \"cpu_percent\": psutil.cpu_percent(interval=1),\n","            \"memory_percent\": psutil.virtual_memory().percent,\n","            \"disk_usage\": psutil.disk_usage('/').percent,\n","            \"network_io\": psutil.net_io_counters()._asdict() if psutil.net_io_counters() else {},\n","            \"process_count\": len(psutil.pids()),\n","            \"load_average\": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0]\n","        }\n","\n","        # Check for alerts\n","        self._check_system_alerts()\n","\n","        return self.system_stats\n","\n","    def record_agent_metrics(self, agent_id: str, execution_data: Dict):\n","        \"\"\"Record agent-specific metrics\"\"\"\n","        if agent_id not in self.metrics_store:\n","            self.metrics_store[agent_id] = {\n","                \"executions\": [],\n","                \"total_requests\": 0,\n","                \"successful_requests\": 0,\n","                \"total_execution_time\": 0,\n","                \"tool_usage\": {},\n","                \"error_types\": {}\n","            }\n","\n","        agent_metrics = self.metrics_store[agent_id]\n","        agent_metrics[\"executions\"].append({\n","            \"timestamp\": datetime.now(),\n","            \"execution_time\": execution_data.get(\"execution_time\", 0),\n","            \"success\": execution_data.get(\"success\", False),\n","            \"tools_used\": execution_data.get(\"tools_used\", []),\n","            \"error\": execution_data.get(\"error\"),\n","            \"query_complexity\": execution_data.get(\"query_complexity\", \"medium\")\n","        })\n","\n","        # Update aggregate metrics\n","        agent_metrics[\"total_requests\"] += 1\n","        if execution_data.get(\"success\", False):\n","            agent_metrics[\"successful_requests\"] += 1\n","\n","        agent_metrics[\"total_execution_time\"] += execution_data.get(\"execution_time\", 0)\n","\n","        # Update tool usage stats\n","        for tool in execution_data.get(\"tools_used\", []):\n","            agent_metrics[\"tool_usage\"][tool] = agent_metrics[\"tool_usage\"].get(tool, 0) + 1\n","\n","        # Update error stats\n","        if execution_data.get(\"error\"):\n","            error_type = execution_data.get(\"error_type\", \"unknown\")\n","            agent_metrics[\"error_types\"][error_type] = agent_metrics[\"error_types\"].get(error_type, 0) + 1\n","\n","        # Check for agent-specific alerts\n","        self._check_agent_alerts(agent_id)\n","\n","    def _check_system_alerts(self):\n","        \"\"\"Check system metrics against alert thresholds\"\"\"\n","        current_time = datetime.now()\n","\n","        # CPU usage alert\n","        if self.system_stats[\"cpu_percent\"] > self.alert_thresholds[\"cpu_usage\"] * 100:\n","            self.alerts.append({\n","                \"timestamp\": current_time,\n","                \"type\": \"system\",\n","                \"severity\": \"warning\",\n","                \"message\": f\"High CPU usage: {self.system_stats['cpu_percent']:.1f}%\",\n","                \"metric\": \"cpu_usage\",\n","                \"value\": self.system_stats[\"cpu_percent\"]\n","            })\n","\n","        # Memory usage alert\n","        if self.system_stats[\"memory_percent\"] > self.alert_thresholds[\"memory_usage\"] * 100:\n","            self.alerts.append({\n","                \"timestamp\": current_time,\n","                \"type\": \"system\",\n","                \"severity\": \"warning\",\n","                \"message\": f\"High memory usage: {self.system_stats['memory_percent']:.1f}%\",\n","                \"metric\": \"memory_usage\",\n","                \"value\": self.system_stats[\"memory_percent\"]\n","            })\n","\n","    def _check_agent_alerts(self, agent_id: str):\n","        \"\"\"Check agent metrics against alert thresholds\"\"\"\n","        agent_metrics = self.metrics_store[agent_id]\n","        current_time = datetime.now()\n","\n","        # Calculate recent performance (last 10 executions)\n","        recent_executions = agent_metrics[\"executions\"][-10:]\n","        if len(recent_executions) >= 5:  # Need minimum sample size\n","\n","            # Success rate alert\n","            recent_success_rate = sum(1 for ex in recent_executions if ex[\"success\"]) / len(recent_executions)\n","            if recent_success_rate < self.alert_thresholds[\"success_rate\"]:\n","                self.alerts.append({\n","                    \"timestamp\": current_time,\n","                    \"type\": \"agent\",\n","                    \"agent_id\": agent_id,\n","                    \"severity\": \"critical\",\n","                    \"message\": f\"Low success rate for agent {agent_id}: {recent_success_rate:.2%}\",\n","                    \"metric\": \"success_rate\",\n","                    \"value\": recent_success_rate\n","                })\n","\n","            # Response time alert\n","            recent_avg_time = sum(ex[\"execution_time\"] for ex in recent_executions) / len(recent_executions)\n","            if recent_avg_time > self.alert_thresholds[\"avg_response_time\"]:\n","                self.alerts.append({\n","                    \"timestamp\": current_time,\n","                    \"type\": \"agent\",\n","                    \"agent_id\": agent_id,\n","                    \"severity\": \"warning\",\n","                    \"message\": f\"Slow response time for agent {agent_id}: {recent_avg_time:.1f}s\",\n","                    \"metric\": \"avg_response_time\",\n","                    \"value\": recent_avg_time\n","                })\n","\n","    def get_dashboard_data(self) -> Dict[str, Any]:\n","        \"\"\"Get comprehensive dashboard data\"\"\"\n","        dashboard_data = {\n","            \"timestamp\": datetime.now(),\n","            \"system_health\": self._calculate_system_health(),\n","            \"agent_summary\": self._get_agent_summary(),\n","            \"recent_alerts\": self.alerts[-10:],  # Last 10 alerts\n","            \"performance_trends\": self._calculate_performance_trends(),\n","            \"resource_utilization\": self.system_stats\n","        }\n","\n","        return dashboard_data\n","\n","    def _calculate_system_health(self) -> str:\n","        \"\"\"Calculate overall system health status\"\"\"\n","        critical_alerts = [a for a in self.alerts[-20:] if a.get(\"severity\") == \"critical\"]\n","        warning_alerts = [a for a in self.alerts[-20:] if a.get(\"severity\") == \"warning\"]\n","\n","        if len(critical_alerts) > 0:\n","            return \"critical\"\n","        elif len(warning_alerts) > 3:\n","            return \"degraded\"\n","        elif self.system_stats.get(\"cpu_percent\", 0) > 90 or self.system_stats.get(\"memory_percent\", 0) > 90:\n","            return \"stressed\"\n","        else:\n","            return \"healthy\"\n","\n","    def _get_agent_summary(self) -> Dict[str, Any]:\n","        \"\"\"Get summary of all agent performance\"\"\"\n","        total_agents = len(self.metrics_store)\n","        total_requests = sum(agent[\"total_requests\"] for agent in self.metrics_store.values())\n","        total_successful = sum(agent[\"successful_requests\"] for agent in self.metrics_store.values())\n","\n","        overall_success_rate = total_successful / total_requests if total_requests > 0 else 0\n","\n","        return {\n","            \"total_agents\": total_agents,\n","            \"total_requests\": total_requests,\n","            \"overall_success_rate\": overall_success_rate,\n","            \"active_agents\": len([a for a in self.metrics_store.values() if a[\"executions\"] and\n","                                (datetime.now() - a[\"executions\"][-1][\"timestamp\"]).seconds < 300])\n","        }\n","\n","    def _calculate_performance_trends(self) -> Dict[str, Any]:\n","        \"\"\"Calculate performance trends over time\"\"\"\n","        # Simple trend calculation based on recent vs historical performance\n","        all_executions = []\n","        for agent_metrics in self.metrics_store.values():\n","            all_executions.extend(agent_metrics[\"executions\"])\n","\n","        if len(all_executions) < 20:\n","            return {\"trend\": \"insufficient_data\"}\n","\n","        # Sort by timestamp\n","        all_executions.sort(key=lambda x: x[\"timestamp\"])\n","\n","        # Compare recent vs historical\n","        split_point = len(all_executions) // 2\n","        historical = all_executions[:split_point]\n","        recent = all_executions[split_point:]\n","\n","        historical_success_rate = sum(1 for ex in historical if ex[\"success\"]) / len(historical)\n","        recent_success_rate = sum(1 for ex in recent if ex[\"success\"]) / len(recent)\n","\n","        historical_avg_time = sum(ex[\"execution_time\"] for ex in historical) / len(historical)\n","        recent_avg_time = sum(ex[\"execution_time\"] for ex in recent) / len(recent)\n","\n","        return {\n","            \"success_rate_trend\": \"improving\" if recent_success_rate > historical_success_rate else \"declining\",\n","            \"response_time_trend\": \"improving\" if recent_avg_time < historical_avg_time else \"declining\",\n","            \"historical_success_rate\": historical_success_rate,\n","            \"recent_success_rate\": recent_success_rate,\n","            \"historical_avg_time\": historical_avg_time,\n","            \"recent_avg_time\": recent_avg_time\n","        }\n","\n","    def generate_health_report(self) -> str:\n","        \"\"\"Generate comprehensive health report\"\"\"\n","        dashboard_data = self.get_dashboard_data()\n","\n","        report = f\"\"\"\n","=== PRODUCTION HEALTH REPORT ===\n","Generated: {dashboard_data['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n","\n","SYSTEM HEALTH: {dashboard_data['system_health'].upper()}\n","{'ðŸŸ¢' if dashboard_data['system_health'] == 'healthy' else 'ðŸŸ¡' if dashboard_data['system_health'] in ['degraded', 'stressed'] else 'ðŸ”´'}\n","\n","AGENT SUMMARY:\n","â€¢ Total Agents: {dashboard_data['agent_summary']['total_agents']}\n","â€¢ Active Agents: {dashboard_data['agent_summary']['active_agents']}\n","â€¢ Total Requests: {dashboard_data['agent_summary']['total_requests']}\n","â€¢ Overall Success Rate: {dashboard_data['agent_summary']['overall_success_rate']:.2%}\n","\n","RESOURCE UTILIZATION:\n","â€¢ CPU Usage: {dashboard_data['resource_utilization'].get('cpu_percent', 0):.1f}%\n","â€¢ Memory Usage: {dashboard_data['resource_utilization'].get('memory_percent', 0):.1f}%\n","â€¢ Disk Usage: {dashboard_data['resource_utilization'].get('disk_usage', 0):.1f}%\n","\n","PERFORMANCE TRENDS:\n","â€¢ Success Rate: {dashboard_data['performance_trends'].get('success_rate_trend', 'unknown')}\n","â€¢ Response Time: {dashboard_data['performance_trends'].get('response_time_trend', 'unknown')}\n","\n","RECENT ALERTS: {len(dashboard_data['recent_alerts'])}\n","\"\"\"\n","\n","        for alert in dashboard_data['recent_alerts'][-5:]:  # Show last 5 alerts\n","            severity_icon = \"ðŸ”´\" if alert['severity'] == 'critical' else \"ðŸŸ¡\"\n","            report += f\"\\n{severity_icon} [{alert['severity'].upper()}] {alert['message']}\"\n","\n","        return report\n","\n","class ScalabilityManager:\n","    \"\"\"Manage scaling of agentic systems based on load and performance\"\"\"\n","\n","    def __init__(self, monitor: ProductionMonitor):\n","        self.monitor = monitor\n","        self.scaling_policies = {\n","            \"cpu_threshold\": 70.0,  # Scale up if CPU > 70%\n","            \"memory_threshold\": 80.0,  # Scale up if memory > 80%\n","            \"response_time_threshold\": 20.0,  # Scale up if avg response time > 20s\n","            \"queue_length_threshold\": 10,  # Scale up if queue length > 10\n","            \"min_instances\": 1,\n","            \"max_instances\": 10,\n","            \"scale_up_cooldown\": 300,  # 5 minutes\n","            \"scale_down_cooldown\": 600  # 10 minutes\n","        }\n","        self.current_instances = 1\n","        self.last_scaling_action = None\n","        self.scaling_history = []\n","\n","    def check_scaling_needs(self) -> Dict[str, Any]:\n","        \"\"\"Check if scaling action is needed\"\"\"\n","        current_time = datetime.now()\n","\n","        # Get current metrics\n","        system_stats = self.monitor.system_stats\n","        dashboard_data = self.monitor.get_dashboard_data()\n","\n","        scaling_decision = {\n","            \"timestamp\": current_time,\n","            \"action\": \"none\",\n","            \"reason\": \"\",\n","            \"current_instances\": self.current_instances,\n","            \"recommended_instances\": self.current_instances,\n","            \"metrics\": {\n","                \"cpu_usage\": system_stats.get(\"cpu_percent\", 0),\n","                \"memory_usage\": system_stats.get(\"memory_percent\", 0),\n","                \"avg_response_time\": dashboard_data[\"performance_trends\"].get(\"recent_avg_time\", 0)\n","            }\n","        }\n","\n","        # Check scale-up conditions\n","        should_scale_up = False\n","        scale_up_reasons = []\n","\n","        if system_stats.get(\"cpu_percent\", 0) > self.scaling_policies[\"cpu_threshold\"]:\n","            should_scale_up = True\n","            scale_up_reasons.append(f\"High CPU usage: {system_stats['cpu_percent']:.1f}%\")\n","\n","        if system_stats.get(\"memory_percent\", 0) > self.scaling_policies[\"memory_threshold\"]:\n","            should_scale_up = True\n","            scale_up_reasons.append(f\"High memory usage: {system_stats['memory_percent']:.1f}%\")\n","\n","        recent_avg_time = dashboard_data[\"performance_trends\"].get(\"recent_avg_time\", 0)\n","        if recent_avg_time > self.scaling_policies[\"response_time_threshold\"]:\n","            should_scale_up = True\n","            scale_up_reasons.append(f\"Slow response time: {recent_avg_time:.1f}s\")\n","\n","        # Check scale-down conditions\n","        should_scale_down = (\n","            system_stats.get(\"cpu_percent\", 100) < self.scaling_policies[\"cpu_threshold\"] * 0.3 and\n","            system_stats.get(\"memory_percent\", 100) < self.scaling_policies[\"memory_threshold\"] * 0.3 and\n","            recent_avg_time < self.scaling_policies[\"response_time_threshold\"] * 0.5 and\n","            self.current_instances > self.scaling_policies[\"min_instances\"]\n","        )\n","\n","        # Apply cooldown periods\n","        if self.last_scaling_action:\n","            time_since_last_action = (current_time - self.last_scaling_action[\"timestamp\"]).seconds\n","\n","            if should_scale_up and time_since_last_action < self.scaling_policies[\"scale_up_cooldown\"]:\n","                should_scale_up = False\n","                scaling_decision[\"reason\"] = \"Scale-up blocked by cooldown period\"\n","\n","            if should_scale_down and time_since_last_action < self.scaling_policies[\"scale_down_cooldown\"]:\n","                should_scale_down = False\n","                scaling_decision[\"reason\"] = \"Scale-down blocked by cooldown period\"\n","\n","        # Determine scaling action\n","        if should_scale_up and self.current_instances < self.scaling_policies[\"max_instances\"]:\n","            scaling_decision[\"action\"] = \"scale_up\"\n","            scaling_decision[\"recommended_instances\"] = min(\n","                self.current_instances + 1,\n","                self.scaling_policies[\"max_instances\"]\n","            )\n","            scaling_decision[\"reason\"] = \"; \".join(scale_up_reasons)\n","\n","        elif should_scale_down:\n","            scaling_decision[\"action\"] = \"scale_down\"\n","            scaling_decision[\"recommended_instances\"] = max(\n","                self.current_instances - 1,\n","                self.scaling_policies[\"min_instances\"]\n","            )\n","            scaling_decision[\"reason\"] = \"Low resource utilization detected\"\n","\n","        return scaling_decision\n","\n","    def execute_scaling_action(self, scaling_decision: Dict) -> bool:\n","        \"\"\"Execute the recommended scaling action\"\"\"\n","        if scaling_decision[\"action\"] == \"none\":\n","            return True\n","\n","        try:\n","            # In a real implementation, this would interact with container orchestration\n","            # systems like Kubernetes, Docker Swarm, or cloud auto-scaling groups\n","\n","            old_instances = self.current_instances\n","            self.current_instances = scaling_decision[\"recommended_instances\"]\n","            self.last_scaling_action = scaling_decision\n","\n","            # Record scaling history\n","            self.scaling_history.append({\n","                \"timestamp\": scaling_decision[\"timestamp\"],\n","                \"action\": scaling_decision[\"action\"],\n","                \"from_instances\": old_instances,\n","                \"to_instances\": self.current_instances,\n","                \"reason\": scaling_decision[\"reason\"],\n","                \"success\": True\n","            })\n","\n","            print(f\"ðŸ”§ Scaled {scaling_decision['action']}: {old_instances} â†’ {self.current_instances} instances\")\n","            print(f\"   Reason: {scaling_decision['reason']}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"âŒ Scaling action failed: {e}\")\n","            return False\n","\n","    def get_scaling_status(self) -> Dict[str, Any]:\n","        \"\"\"Get current scaling status and history\"\"\"\n","        return {\n","            \"current_instances\": self.current_instances,\n","            \"scaling_policies\": self.scaling_policies,\n","            \"last_scaling_action\": self.last_scaling_action,\n","            \"scaling_history\": self.scaling_history[-10:],  # Last 10 scaling actions\n","            \"total_scaling_events\": len(self.scaling_history)\n","        }\n","\n","class SafetyManager:\n","    \"\"\"Manage safety boundaries and constraints for agentic systems\"\"\"\n","\n","    def __init__(self):\n","        self.safety_policies = {\n","            \"max_api_calls_per_minute\": 100,\n","            \"max_execution_time\": 300,  # 5 minutes\n","            \"allowed_domains\": [\"*.openai.com\", \"*.google.com\", \"*.wikipedia.org\"],\n","            \"blocked_keywords\": [\"delete\", \"remove\", \"drop\", \"truncate\"],\n","            \"max_cost_per_query\": 1.0,  # $1 per query\n","            \"max_daily_cost\": 100.0,  # $100 per day\n","            \"require_approval_for\": [\"financial_transactions\", \"data_modification\", \"external_communications\"]\n","        }\n","        self.safety_violations = []\n","        self.current_costs = {\"daily\": 0.0, \"query_costs\": []}\n","        self.api_call_counts = {}\n","\n","    def validate_query(self, query: str, agent_id: str) -> Dict[str, Any]:\n","        \"\"\"Validate query against safety policies\"\"\"\n","        validation_result = {\n","            \"approved\": True,\n","            \"violations\": [],\n","            \"warnings\": [],\n","            \"estimated_cost\": 0.0,\n","            \"requires_approval\": False\n","        }\n","\n","        # Check for blocked keywords\n","        query_lower = query.lower()\n","        for keyword in self.safety_policies[\"blocked_keywords\"]:\n","            if keyword in query_lower:\n","                validation_result[\"violations\"].append(f\"Contains blocked keyword: {keyword}\")\n","                validation_result[\"approved\"] = False\n","\n","        # Check for approval requirements\n","        for approval_category in self.safety_policies[\"require_approval_for\"]:\n","            if approval_category.replace(\"_\", \" \") in query_lower:\n","                validation_result[\"requires_approval\"] = True\n","                validation_result[\"warnings\"].append(f\"Requires approval: {approval_category}\")\n","\n","        # Estimate cost (simplified)\n","        estimated_tokens = len(query.split()) * 1.3  # Rough estimate\n","        validation_result[\"estimated_cost\"] = estimated_tokens * 0.0001  # $0.0001 per token estimate\n","\n","        # Check cost limits\n","        if validation_result[\"estimated_cost\"] > self.safety_policies[\"max_cost_per_query\"]:\n","            validation_result[\"violations\"].append(f\"Estimated cost exceeds limit: ${validation_result['estimated_cost']:.4f}\")\n","            validation_result[\"approved\"] = False\n","\n","        if self.current_costs[\"daily\"] + validation_result[\"estimated_cost\"] > self.safety_policies[\"max_daily_cost\"]:\n","            validation_result[\"violations\"].append(\"Daily cost limit would be exceeded\")\n","            validation_result[\"approved\"] = False\n","\n","        # Check API rate limits\n","        current_minute = datetime.now().replace(second=0, microsecond=0)\n","        minute_key = f\"{agent_id}_{current_minute}\"\n","        current_calls = self.api_call_counts.get(minute_key, 0)\n","\n","        if current_calls >= self.safety_policies[\"max_api_calls_per_minute\"]:\n","            validation_result[\"violations\"].append(\"API rate limit exceeded\")\n","            validation_result[\"approved\"] = False\n","\n","        return validation_result\n","\n","    def record_execution(self, agent_id: str, execution_data: Dict):\n","        \"\"\"Record execution for safety monitoring\"\"\"\n","        # Update API call counts\n","        current_minute = datetime.now().replace(second=0, microsecond=0)\n","        minute_key = f\"{agent_id}_{current_minute}\"\n","        self.api_call_counts[minute_key] = self.api_call_counts.get(minute_key, 0) + 1\n","\n","        # Update cost tracking\n","        actual_cost = execution_data.get(\"cost\", 0.0)\n","        self.current_costs[\"daily\"] += actual_cost\n","        self.current_costs[\"query_costs\"].append({\n","            \"timestamp\": datetime.now(),\n","            \"agent_id\": agent_id,\n","            \"cost\": actual_cost\n","        })\n","\n","        # Clean up old data (keep only last 24 hours)\n","        cutoff_time = datetime.now() - timedelta(hours=24)\n","        self.current_costs[\"query_costs\"] = [\n","            cost for cost in self.current_costs[\"query_costs\"]\n","            if cost[\"timestamp\"] > cutoff_time\n","        ]\n","\n","        # Recalculate daily cost\n","        self.current_costs[\"daily\"] = sum(cost[\"cost\"] for cost in self.current_costs[\"query_costs\"])\n","\n","        # Clean up old API call counts (keep only last hour)\n","        cutoff_minute = datetime.now().replace(second=0, microsecond=0) - timedelta(hours=1)\n","        keys_to_remove = [\n","            key for key in self.api_call_counts.keys()\n","            if datetime.fromisoformat(key.split('_', 1)[1]) < cutoff_minute\n","        ]\n","        for key in keys_to_remove:\n","            del self.api_call_counts[key]\n","\n","    def get_safety_status(self) -> Dict[str, Any]:\n","        \"\"\"Get current safety status\"\"\"\n","        return {\n","            \"daily_cost\": self.current_costs[\"daily\"],\n","            \"cost_limit\": self.safety_policies[\"max_daily_cost\"],\n","            \"cost_utilization\": self.current_costs[\"daily\"] / self.safety_policies[\"max_daily_cost\"],\n","            \"recent_violations\": len(self.safety_violations),\n","            \"api_calls_current_minute\": max(self.api_call_counts.values()) if self.api_call_counts else 0,\n","            \"safety_policies_active\": len(self.safety_policies)\n","        }"],"metadata":{"id":"Gj0cdUFGK-64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 5: COMPREHENSIVE DEMONSTRATIONS**"],"metadata":{"id":"hW3vyaE6MD_W"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 5: COMPREHENSIVE DEMONSTRATIONS\")\n","print(\"=\"*60)\n","\n","# Initialize production systems\n","print(\"ðŸš€ Initializing production systems...\")\n","production_metrics = AgentMetrics(use_prometheus=False)  # Use fallback metrics to avoid Prometheus issues\n","agent_factory = ProductionAgentFactory(metrics=production_metrics)\n","monitor = ProductionMonitor()\n","scalability_manager = ScalabilityManager(monitor)\n","safety_manager = SafetyManager()\n","\n","# Register production tools\n","agent_factory.register_tool(Tool(\n","    name=\"enhanced_web_search_tool\",\n","    func=enhanced_web_search_tool,\n","    description=\"Search the web for current information\"\n","))\n","\n","agent_factory.register_tool(Tool(\n","    name=\"enhanced_calculator_tool\",\n","    func=enhanced_calculator_tool,\n","    description=\"Perform mathematical calculations\"\n","))\n","\n","agent_factory.register_tool(Tool(\n","    name=\"data_analysis_tool\",\n","    func=data_analysis_tool,\n","    description=\"Analyze data patterns and trends\"\n","))\n","\n","# Create production agents\n","print(\"\\nðŸ¤– Creating production agents...\")\n","tools = list(agent_factory.tools_registry.values())\n","\n","# Create different types of agents\n","print(\"Creating ReAct agent...\")\n","react_agent = agent_factory.create_react_agent(tools, memory=True)\n","\n","print(\"Creating structured agent (simplified)...\")\n","structured_agent = agent_factory.create_structured_chat_agent(\n","    tools,\n","    system_message=\"You are a production-ready AI assistant with comprehensive monitoring and safety controls.\"\n",")\n","\n","# Create sample DataFrame for pandas agent\n","print(\"Creating pandas agent...\")\n","sample_data = pd.DataFrame({\n","    'product': ['A', 'B', 'C', 'D', 'E'],\n","    'sales': [100, 150, 200, 120, 180],\n","    'profit': [20, 30, 50, 25, 40],\n","    'region': ['North', 'South', 'East', 'West', 'Central']\n","})\n","\n","try:\n","    pandas_agent = agent_factory.create_pandas_agent(\n","        sample_data,\n","        \"Sales and profit data for different products and regions\"\n","    )\n","    print(\"âœ… Pandas agent created successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ Pandas agent creation failed: {e}\")\n","    pandas_agent = None\n","\n","print(\"âœ… All production agents created successfully\")\n","\n","# Create self-improvement and goal-oriented agents\n","print(\"\\nðŸ§  Creating advanced pattern agents...\")\n","self_improving_agent = SelfImprovementAgent(react_agent, production_metrics)\n","goal_oriented_agent = GoalOrientedAgent(structured_agent)\n","\n","# Set up evaluation framework\n","print(\"\\nðŸ“Š Setting up evaluation framework...\")\n","evaluator = AgentEvaluationFramework()\n","\n","# Create benchmark dataset\n","benchmark_queries = [\n","    {\"id\": \"q1\", \"text\": \"What are the latest developments in artificial intelligence?\"},\n","    {\"id\": \"q2\", \"text\": \"Calculate the compound interest on $10000 at 5% for 3 years\"},\n","    {\"id\": \"q3\", \"text\": \"Analyze the trend in technology stock prices\"},\n","    {\"id\": \"q4\", \"text\": \"What are the implications of quantum computing for cybersecurity?\"},\n","    {\"id\": \"q5\", \"text\": \"Compare renewable energy adoption rates globally\"}\n","]\n","\n","expected_outputs = [\n","    {\"text\": \"AI developments include advances in LLMs, computer vision, and robotics\", \"key_points\": [\"LLMs\", \"computer vision\", \"robotics\"]},\n","    {\"text\": \"Compound interest calculation results in $11576.25\", \"key_points\": [\"compound interest\", \"calculation\", \"result\"]},\n","    {\"text\": \"Technology stocks show mixed performance with growth in AI sectors\", \"key_points\": [\"technology stocks\", \"performance\", \"AI\"]},\n","    {\"text\": \"Quantum computing poses challenges to current encryption methods\", \"key_points\": [\"quantum computing\", \"encryption\", \"security\"]},\n","    {\"text\": \"Renewable energy adoption varies by region with solar and wind leading\", \"key_points\": [\"renewable energy\", \"adoption\", \"solar\", \"wind\"]}\n","]\n","\n","evaluator.add_benchmark_dataset(\"production_test\", benchmark_queries, expected_outputs)"],"metadata":{"id":"Itm7lXpAMEHz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 1: Basic Production Agent Operation**"],"metadata":{"id":"DRqGnAYOQ_qc"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 1: BASIC PRODUCTION AGENT OPERATION\")\n","print(\"=\"*60)\n","\n","def demonstrate_production_agent():\n","    \"\"\"Demonstrate basic production agent with monitoring\"\"\"\n","\n","    test_queries = [\n","        \"What are recent developments in quantum computing?\",\n","        \"Calculate 15% of 2500 and explain the calculation\",\n","        \"Analyze trends in renewable energy adoption\"\n","    ]\n","\n","    for i, query in enumerate(test_queries, 1):\n","        print(f\"\\nðŸ“ Test Query {i}: {query}\")\n","\n","        # Safety validation\n","        safety_check = safety_manager.validate_query(query, \"react_agent\")\n","        print(f\"ðŸ›¡ï¸ Safety Check: {'âœ… Approved' if safety_check['approved'] else 'âŒ Blocked'}\")\n","\n","        if not safety_check[\"approved\"]:\n","            print(f\"   Violations: {safety_check['violations']}\")\n","            continue\n","\n","        # Record system metrics before execution\n","        monitor.record_system_metrics()\n","\n","        # Execute query\n","        start_time = time.time()\n","        try:\n","            result = react_agent.invoke({\"input\": query})\n","            execution_time = time.time() - start_time\n","            success = True\n","            print(f\"âœ… Query completed in {execution_time:.2f}s\")\n","            print(f\"ðŸ“„ Response: {result.get('output', '')[:200]}...\")\n","\n","        except Exception as e:\n","            execution_time = time.time() - start_time\n","            success = False\n","            print(f\"âŒ Query failed after {execution_time:.2f}s: {e}\")\n","\n","        # Record metrics\n","        execution_data = {\n","            \"execution_time\": execution_time,\n","            \"success\": success,\n","            \"tools_used\": [\"web_search\", \"calculator\"] if \"calculate\" in query.lower() else [\"web_search\"],\n","            \"error\": None if success else str(e) if 'e' in locals() else None,\n","            \"cost\": safety_check[\"estimated_cost\"]\n","        }\n","\n","        try:\n","            monitor.record_agent_metrics(\"react_agent\", execution_data)\n","            safety_manager.record_execution(\"react_agent\", execution_data)\n","        except Exception as metrics_error:\n","            print(f\"âš ï¸ Metrics recording failed: {metrics_error}\")\n","            # Continue anyway\n","\n","    # Show monitoring results\n","    print(f\"\\nðŸ“Š Performance Summary:\")\n","    perf_summary = production_metrics.get_performance_summary()\n","    print(f\"   Total Executions: {perf_summary['total_executions']}\")\n","    print(f\"   Success Rate: {perf_summary['success_rate']:.2%}\")\n","    print(f\"   Average Duration: {perf_summary['average_duration']:.2f}s\")"],"metadata":{"id":"x0L3f54kQ_x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 2: Self-Improvement Agent**"],"metadata":{"id":"eoAqXZXwRLDF"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 2: SELF-IMPROVEMENT AGENT\")\n","print(\"=\"*60)\n","\n","def demonstrate_self_improvement():\n","    \"\"\"Demonstrate self-improvement capabilities\"\"\"\n","\n","    test_scenarios = [\n","        {\n","            \"query\": \"Research the impact of artificial intelligence on job markets\",\n","            \"feedback\": \"Good analysis but could be more comprehensive\"\n","        },\n","        {\n","            \"query\": \"Calculate the ROI for a $50000 investment with 8% annual return over 5 years\",\n","            \"feedback\": \"Calculation correct but explanation was too brief\"\n","        },\n","        {\n","            \"query\": \"Analyze current trends in sustainable energy technologies\",\n","            \"feedback\": \"Response was slow and missed some key developments\"\n","        }\n","    ]\n","\n","    print(\"ðŸ§  Testing self-improvement capabilities...\")\n","\n","    for i, scenario in enumerate(test_scenarios, 1):\n","        print(f\"\\nðŸŽ¯ Scenario {i}: {scenario['query'][:50]}...\")\n","\n","        # Execute with learning\n","        result = self_improving_agent.execute_with_learning(\n","            scenario[\"query\"],\n","            feedback=scenario[\"feedback\"]\n","        )\n","\n","        print(f\"âœ… Execution: {'Success' if result['success'] else 'Failed'}\")\n","        print(f\"â±ï¸ Time: {result['execution_time']:.2f}s\")\n","        print(f\"ðŸ”§ Improvements Applied: {result.get('improvements_applied', 0)}\")\n","\n","        if 'performance_analysis' in result:\n","            analysis = result['performance_analysis']\n","            print(f\"ðŸ“Š Performance Rating: {analysis.get('execution_time_rating', 'unknown')}\")\n","            print(f\"ðŸ“‹ Completeness: {analysis.get('result_completeness', 'unknown')}\")\n","\n","    # Show learning insights\n","    print(f\"\\nðŸŽ“ Learning Insights:\")\n","    insights = self_improving_agent.get_learning_insights()\n","    print(f\"   Total Interactions: {insights['total_interactions']}\")\n","    print(f\"   Success Rate: {insights['success_rate']:.2%}\")\n","    print(f\"   Performance Trend: {insights.get('recent_performance_trend', 'unknown')}\")\n","    print(f\"   Common Improvements: {insights.get('common_improvements', {})}\")"],"metadata":{"id":"JtEj23i5RLKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 3: Goal-Oriented Agent**"],"metadata":{"id":"TufX7hgaRTiu"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 3: GOAL-ORIENTED AGENT\")\n","print(\"=\"*60)\n","\n","def demonstrate_goal_oriented_behavior():\n","    \"\"\"Demonstrate goal-oriented agent capabilities\"\"\"\n","\n","    print(\"ðŸŽ¯ Setting up long-term goals...\")\n","\n","    # Set multiple goals with different priorities\n","    goals = [\n","        {\n","            \"description\": \"Research renewable energy market trends for next quarter\",\n","            \"priority\": 3,\n","            \"deadline\": datetime.now() + timedelta(days=7)\n","        },\n","        {\n","            \"description\": \"Analyze competitive landscape in AI industry\",\n","            \"priority\": 2,\n","            \"deadline\": datetime.now() + timedelta(days=14)\n","        },\n","        {\n","            \"description\": \"Monitor financial market indicators for portfolio optimization\",\n","            \"priority\": 1,\n","            \"deadline\": datetime.now() + timedelta(days=30)\n","        }\n","    ]\n","\n","    goal_ids = []\n","    for goal in goals:\n","        goal_id = goal_oriented_agent.set_goal(\n","            goal[\"description\"],\n","            priority=goal[\"priority\"],\n","            deadline=goal[\"deadline\"]\n","        )\n","        goal_ids.append(goal_id)\n","\n","    # Work towards goals\n","    print(f\"\\nðŸš€ Working towards goals...\")\n","    work_results = goal_oriented_agent.work_towards_goals(time_limit=60)  # 1 minute limit\n","\n","    print(f\"ðŸ“Š Work Session Results:\")\n","    print(f\"   Goals Worked On: {len(work_results['goals_worked_on'])}\")\n","    print(f\"   Goals Completed: {len(work_results['completed_goals'])}\")\n","    print(f\"   Total Time Spent: {work_results['total_time_spent']:.1f}s\")\n","\n","    # Show goal status\n","    goal_status = goal_oriented_agent.get_goal_status()\n","    print(f\"\\nðŸŽ¯ Goal Status:\")\n","    print(f\"   Active Goals: {goal_status['active_goals']}\")\n","    print(f\"   Completed Goals: {goal_status['completed_goals']}\")\n","    print(f\"   Average Progress: {goal_status['average_progress']:.1%}\")\n"],"metadata":{"id":"_-EQjqNNRTqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 4: Comprehensive Evaluation**"],"metadata":{"id":"A02-PInxRbHf"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 4: COMPREHENSIVE EVALUATION\")\n","print(\"=\"*60)\n","\n","def demonstrate_comprehensive_evaluation():\n","    \"\"\"Demonstrate comprehensive evaluation framework\"\"\"\n","\n","    print(\"ðŸ“Š Running comprehensive evaluation...\")\n","\n","    # Evaluate different agent types\n","    agents_to_test = [\n","        (\"ReAct Agent\", react_agent),\n","        (\"Structured Chat Agent\", structured_agent)\n","    ]\n","\n","    evaluation_results = {}\n","\n","    for agent_name, agent in agents_to_test:\n","        print(f\"\\nðŸ§ª Evaluating {agent_name}...\")\n","\n","        try:\n","            # Run evaluation on sample dataset\n","            results = evaluator.evaluate_agent_performance(\n","                agent,\n","                \"production_test\",\n","                sample_size=3  # Test with 3 queries for demo\n","            )\n","\n","            evaluation_results[agent_name] = results\n","\n","            # Generate and print report\n","            report = evaluator.generate_evaluation_report(results)\n","            print(report)\n","\n","        except Exception as e:\n","            print(f\"âŒ Evaluation failed for {agent_name}: {e}\")\n","\n","    return evaluation_results\n"],"metadata":{"id":"fxkcouC6RbPk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 5: Production Monitoring Dashboard**"],"metadata":{"id":"ZJqtw-KHRjS9"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 5: PRODUCTION MONITORING DASHBOARD\")\n","print(\"=\"*60)\n","\n","def demonstrate_production_monitoring():\n","    \"\"\"Demonstrate production monitoring and alerting\"\"\"\n","\n","    print(\"ðŸ“Š Generating production monitoring dashboard...\")\n","\n","    # Record some system metrics\n","    monitor.record_system_metrics()\n","\n","    # Get dashboard data\n","    dashboard_data = monitor.get_dashboard_data()\n","\n","    # Generate health report\n","    health_report = monitor.generate_health_report()\n","    print(health_report)\n","\n","    # Check scaling needs\n","    scaling_decision = scalability_manager.check_scaling_needs()\n","    print(f\"\\nðŸ”§ Scaling Analysis:\")\n","    print(f\"   Current Instances: {scaling_decision['current_instances']}\")\n","    print(f\"   Recommended Action: {scaling_decision['action']}\")\n","    print(f\"   Reason: {scaling_decision['reason']}\")\n","\n","    if scaling_decision['action'] != 'none':\n","        print(f\"   Recommended Instances: {scaling_decision['recommended_instances']}\")\n","\n","        # Execute scaling (simulation)\n","        success = scalability_manager.execute_scaling_action(scaling_decision)\n","        print(f\"   Scaling {'âœ… Successful' if success else 'âŒ Failed'}\")\n","\n","    # Show safety status\n","    safety_status = safety_manager.get_safety_status()\n","    print(f\"\\nðŸ›¡ï¸ Safety Status:\")\n","    print(f\"   Daily Cost: ${safety_status['daily_cost']:.2f} / ${safety_status['cost_limit']:.2f}\")\n","    print(f\"   Cost Utilization: {safety_status['cost_utilization']:.1%}\")\n","    print(f\"   Recent Violations: {safety_status['recent_violations']}\")\n","    print(f\"   API Calls/Min: {safety_status['api_calls_current_minute']}\")"],"metadata":{"id":"kIEBFoVBRjaz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# DEMONSTRATION 6: Advanced Visualization**"],"metadata":{"id":"nnIwzIASRrNa"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"DEMONSTRATION 6: ADVANCED VISUALIZATION\")\n","print(\"=\"*60)\n","\n","def create_production_dashboard_visualization():\n","    \"\"\"Create comprehensive production dashboard visualization\"\"\"\n","\n","    # Generate sample data for visualization\n","    sample_metrics = production_metrics.get_performance_summary()\n","\n","    # Create subplots\n","    fig = make_subplots(\n","        rows=3, cols=2,\n","        subplot_titles=('Agent Performance Overview', 'Response Time Distribution',\n","                       'Tool Usage Patterns', 'Success Rate Trends',\n","                       'Resource Utilization', 'Cost Analysis'),\n","        specs=[[{\"type\": \"indicator\"}, {\"type\": \"histogram\"}],\n","               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n","               [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n","    )\n","\n","    # Agent Performance Overview (Gauge)\n","    success_rate = sample_metrics.get('success_rate', 0.85) * 100\n","    fig.add_trace(\n","        go.Indicator(\n","            mode=\"gauge+number\",\n","            value=success_rate,\n","            title={\"text\": \"Success Rate (%)\"},\n","            gauge={\n","                \"axis\": {\"range\": [0, 100]},\n","                \"bar\": {\"color\": \"darkgreen\"},\n","                \"steps\": [\n","                    {\"range\": [0, 60], \"color\": \"lightgray\"},\n","                    {\"range\": [60, 80], \"color\": \"yellow\"},\n","                    {\"range\": [80, 100], \"color\": \"lightgreen\"}\n","                ],\n","                \"threshold\": {\n","                    \"line\": {\"color\": \"red\", \"width\": 4},\n","                    \"thickness\": 0.75,\n","                    \"value\": 90\n","                }\n","            }\n","        ),\n","        row=1, col=1\n","    )\n","\n","    # Response Time Distribution\n","    response_times = np.random.lognormal(2.5, 0.5, 100)  # Sample data\n","    fig.add_trace(\n","        go.Histogram(x=response_times, name=\"Response Times\", nbinsx=20),\n","        row=1, col=2\n","    )\n","\n","    # Tool Usage Patterns\n","    tools = ['web_search', 'calculator', 'data_analysis', 'document_search']\n","    usage_counts = [45, 30, 25, 15]  # Sample data\n","    fig.add_trace(\n","        go.Bar(x=tools, y=usage_counts, name=\"Tool Usage\"),\n","        row=2, col=1\n","    )\n","\n","    # Success Rate Trends\n","    time_points = pd.date_range(start='2024-01-01', periods=30, freq='D')\n","    success_rates = 0.8 + 0.15 * np.random.random(30)  # Sample trend data\n","    fig.add_trace(\n","        go.Scatter(x=time_points, y=success_rates, mode='lines+markers', name=\"Success Rate\"),\n","        row=2, col=2\n","    )\n","\n","    # Resource Utilization\n","    resources = ['CPU', 'Memory', 'Disk', 'Network']\n","    utilization = [65, 78, 45, 32]  # Sample data\n","    fig.add_trace(\n","        go.Pie(labels=resources, values=utilization, name=\"Resource Usage\"),\n","        row=3, col=1\n","    )\n","\n","    # Cost Analysis\n","    cost_categories = ['API Calls', 'Compute', 'Storage', 'Network']\n","    costs = [120, 80, 30, 15]  # Sample cost data\n","    fig.add_trace(\n","        go.Bar(x=cost_categories, y=costs, name=\"Costs ($)\"),\n","        row=3, col=2\n","    )\n","\n","    # Update layout\n","    fig.update_layout(\n","        height=1000,\n","        showlegend=False,\n","        title_text=\"Production Agentic RAG System Dashboard\",\n","        title_x=0.5\n","    )\n","\n","    # Show the plot\n","    fig.show()\n","\n","    print(\"ðŸ“Š Production dashboard visualization created successfully!\")\n","\n","    return fig"],"metadata":{"id":"Oa9eJSHiRrVu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# MAIN EXECUTION AND TESTING**"],"metadata":{"id":"6oc-es5DRzmn"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"MAIN EXECUTION AND TESTING\")\n","print(\"=\"*60)\n","\n","def run_comprehensive_demonstration():\n","    \"\"\"Run all demonstrations in sequence\"\"\"\n","\n","    print(\"ðŸš€ Starting comprehensive production demonstration...\")\n","\n","    try:\n","        # Run all demonstrations\n","        print(\"\\n\" + \"=\"*50)\n","        demonstrate_production_agent()\n","\n","        print(\"\\n\" + \"=\"*50)\n","        demonstrate_self_improvement()\n","\n","        print(\"\\n\" + \"=\"*50)\n","        demonstrate_goal_oriented_behavior()\n","\n","        print(\"\\n\" + \"=\"*50)\n","        evaluation_results = demonstrate_comprehensive_evaluation()\n","\n","        print(\"\\n\" + \"=\"*50)\n","        demonstrate_production_monitoring()\n","\n","        print(\"\\n\" + \"=\"*50)\n","        dashboard_fig = create_production_dashboard_visualization()\n","\n","        # Final summary\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"COMPREHENSIVE DEMONSTRATION COMPLETE\")\n","        print(\"=\"*60)\n","\n","        final_metrics = production_metrics.get_performance_summary()\n","        print(f\"ðŸ“Š Final Performance Summary:\")\n","        print(f\"   Total Executions: {final_metrics['total_executions']}\")\n","        print(f\"   Overall Success Rate: {final_metrics['success_rate']:.2%}\")\n","        print(f\"   Average Execution Time: {final_metrics['average_duration']:.2f}s\")\n","\n","        # System health check\n","        health_report = monitor.generate_health_report()\n","        print(f\"\\nðŸ¥ Final Health Check:\")\n","        print(health_report)\n","\n","        print(f\"\\nâœ… All production systems demonstrated successfully!\")\n","        print(f\"ðŸŽ¯ Key Takeaways:\")\n","        print(f\"   - Production monitoring provides comprehensive observability\")\n","        print(f\"   - Self-improvement agents learn from experience and feedback\")\n","        print(f\"   - Goal-oriented agents can pursue long-term objectives\")\n","        print(f\"   - Comprehensive evaluation ensures quality and reliability\")\n","        print(f\"   - Safety mechanisms protect against misuse and errors\")\n","        print(f\"   - Scalability systems handle varying loads automatically\")\n","\n","        return {\n","            \"success\": True,\n","            \"final_metrics\": final_metrics,\n","            \"evaluation_results\": evaluation_results,\n","            \"dashboard_created\": True\n","        }\n","\n","    except Exception as e:\n","        print(f\"âŒ Demonstration failed: {e}\")\n","        return {\n","            \"success\": False,\n","            \"error\": str(e)\n","        }\n","\n","# Execute the comprehensive demonstration\n","if __name__ == \"__main__\":\n","    # Run comprehensive demonstration\n","    demo_results = run_comprehensive_demonstration()\n","\n","    if demo_results[\"success\"]:\n","        print(f\"\\nðŸŽ‰ Production deployment demonstration completed successfully!\")\n","        print(f\"ðŸ’¡ Ready for real-world deployment with:\")\n","        print(f\"   âœ“ Comprehensive monitoring and alerting\")\n","        print(f\"   âœ“ Automatic scaling capabilities\")\n","        print(f\"   âœ“ Safety boundaries and cost controls\")\n","        print(f\"   âœ“ Self-improvement and learning systems\")\n","        print(f\"   âœ“ Goal-oriented autonomous behavior\")\n","        print(f\"   âœ“ Production-ready evaluation frameworks\")\n","    else:\n","        print(f\"âŒ Demonstration encountered issues: {demo_results.get('error', 'Unknown error')}\")\n","\n","print(f\"\\nâœ¨ Notebook Complete! Advanced patterns and production deployment demonstrated.\")"],"metadata":{"id":"uAHJjwTmRztx"},"execution_count":null,"outputs":[]}]}