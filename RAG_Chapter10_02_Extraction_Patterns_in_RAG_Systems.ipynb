{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnk9THQNaQjAQYFrBb+Xpl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"KknZVCzX7uBx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QDaRmvO7c33"},"outputs":[],"source":["# Install necessary packages\n","!pip install langchain langchain-openai langchain-community chromadb pydantic\n","\n","import os\n","import time\n","import json\n","import re\n","from typing import List, Optional, Dict, Any\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set your OpenAI API key (in Colab, you should use secrets or environment variables)\n","from getpass import getpass\n","OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n","\n","# Import necessary components\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_community.vectorstores import Chroma\n","from langchain.chains import LLMChain, MapReduceDocumentsChain, ReduceDocumentsChain\n","from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.output_parsers import PydanticOutputParser, StructuredOutputParser, ResponseSchema\n","from langchain.prompts import ChatPromptTemplate, PromptTemplate\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from pydantic import BaseModel, Field\n","\n","# Initialize the LLM\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","print(\"Setup complete!\")"]},{"cell_type":"markdown","source":["**10.3.1 Reference-Based Extraction**\n","\n","---\n","\n","**Create Sample Research Papers Data**"],"metadata":{"id":"IOYJZRBM8NHj"}},{"cell_type":"code","source":["research_documents = [\n","    Document(\n","        page_content=\"\"\"\n","        Title: Advanced Techniques in Retrieval Augmented Generation\n","\n","        Authors: Jane Smith, Robert Johnson, Wei Zhang\n","\n","        Publication: Journal of Artificial Intelligence Research, 2023\n","\n","        Abstract: Retrieval Augmented Generation (RAG) combines neural generation with information retrieval to enhance the factuality and reliability of generated text. This paper presents several novel techniques for improving the retrieval component, including adaptive retrievers, multi-step retrieval, and hybrid search methods. Our approaches demonstrate significant improvements in accuracy and relevance across multiple benchmark datasets.\n","\n","        Methodology: We evaluated our techniques on the MS MARCO, Natural Questions, and HotpotQA datasets. Experiments were conducted using a combination of dense and sparse retrievers, with a T5-large model serving as the generator. All models were implemented in PyTorch and trained on 8 NVIDIA A100 GPUs.\n","\n","        Results: Our adaptive retrieval approach achieved a 7.2% improvement in recall@10 compared to the strongest baseline. The multi-step retrieval technique showed particular strength in complex queries, improving answer accuracy by 12.3% on multi-hop questions. Hybrid search methods balanced efficiency and effectiveness, with only a 15% increase in computational cost yielding a 9.8% improvement in overall accuracy.\n","        \"\"\",\n","        metadata={\"source\": \"advanced_rag_techniques.pdf\", \"year\": 2023}\n","    ),\n","    Document(\n","        page_content=\"\"\"\n","        Title: Enhancing RAG Systems with Knowledge Graph Integration\n","\n","        Authors: Michael Lee, Sarah Garcia, John Patel\n","\n","        Publication: Conference on Neural Information Processing Systems, 2022\n","\n","        Abstract: This paper investigates the integration of knowledge graphs into Retrieval Augmented Generation systems to improve factual consistency and reasoning capabilities. We propose a novel architecture that jointly reasons over retrieved documents and knowledge graph subgraphs. Our approach enables more structured reasoning while maintaining the flexibility of text generation.\n","\n","        Methodology: We constructed a benchmark dataset combining text retrieval with knowledge graph queries. Our system uses a dual-encoder architecture to process both textual and graph-structured inputs. Evaluation was performed on a modified version of the CommonsenseQA and WebQuestions datasets, focusing on questions requiring both factual retrieval and reasoning.\n","\n","        Results: Knowledge graph integration improved factual accuracy by 14.7% compared to text-only RAG systems. Our approach was particularly effective for queries involving relationships between entities, showing a 23.2% improvement in such cases. Manual evaluation showed a 28% reduction in hallucinated facts when compared to standard RAG approaches.\n","        \"\"\",\n","        metadata={\"source\": \"knowledge_graph_rag.pdf\", \"year\": 2022}\n","    ),\n","    Document(\n","        page_content=\"\"\"\n","        Title: Long-Context Retrieval Strategies for RAG Applications\n","\n","        Authors: Carlos Diaz, Emily Wilson, Aisha Kumar\n","\n","        Publication: ACL Workshop on Document Intelligence, 2023\n","\n","        Abstract: As language models support increasingly large context windows, retrieval strategies must adapt to leverage this capability effectively. This paper explores techniques for retrieving and organizing information for long-context RAG applications. We introduce a hierarchical retrieval framework that combines local and global context to optimize information selection for extended documents.\n","\n","        Methodology: We developed a benchmark suite of tasks requiring comprehension of long documents, including financial reports, legal contracts, and academic papers. Our hierarchical retriever combines BM25 for initial document selection, dense retrieval for passage ranking, and a novel context-aware reranker to optimize the final selection. Experiments used GPT-4 as the base language model with various context management strategies.\n","\n","        Results: Our hierarchical approach improved answer accuracy by 18.3% compared to flat retrieval methods when dealing with documents exceeding 50 pages. The context-aware reranker demonstrated a 12.5% improvement in relevant information selection. Importantly, the system maintained consistent performance even as document length increased, showing only a 3% degradation when moving from 20-page to 100-page documents.\n","        \"\"\",\n","        metadata={\"source\": \"long_context_retrieval.pdf\", \"year\": 2023}\n","    )\n","]\n","\n","print(f\"Created {len(research_documents)} sample research paper documents\")"],"metadata":{"id":"JPKqI_HD8NRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Basic Reference-Based Extraction with Pydantic**"],"metadata":{"id":"psWmTsIq8k2c"}},{"cell_type":"code","source":["from langchain.output_parsers import PydanticOutputParser\n","from pydantic import BaseModel, Field\n","from typing import List, Optional\n","\n","# Define the data structure we want to extract\n","class ResearchPaper(BaseModel):\n","    title: str = Field(description=\"The title of the research paper\")\n","    authors: List[str] = Field(description=\"List of authors' names\")\n","    publication_year: int = Field(description=\"Year the paper was published\")\n","    abstract: str = Field(description=\"The paper's abstract\")\n","    methods: Optional[List[str]] = Field(description=\"Research methods used\", default=None)\n","    findings: Optional[List[str]] = Field(description=\"Key findings of the paper\", default=None)\n","\n","# Create a parser for this structure\n","parser = PydanticOutputParser(pydantic_object=ResearchPaper)\n","\n","# Create an extraction template with reference examples\n","extraction_template = \"\"\"\n","Extract structured information from the research paper below, following the format of the example.\n","\n","EXAMPLE INPUT:\n","Title: Advances in Neural Information Processing Systems\n","Authors: John Smith, Jane Doe\n","Publication: Conference on Neural Information Processing Systems, 2022\n","Abstract: This paper presents a novel approach to neural network optimization that improves training efficiency by 30% while maintaining accuracy. We introduce a dynamic learning rate adjustment method that adapts based on gradient consistency across batches.\n","Methodology: We evaluated our approach on CIFAR-10 and ImageNet using ResNet architectures. Experiments were conducted using 4 NVIDIA A100 GPUs with batch sizes ranging from 32 to 256.\n","Results: Our method achieved 94.2% accuracy on CIFAR-10 and 76.8% top-1 accuracy on ImageNet, while reducing training time from 24 hours to 16.8 hours compared to baseline methods.\n","\n","EXAMPLE OUTPUT:\n","{\n","  \"title\": \"Advances in Neural Information Processing Systems\",\n","  \"authors\": [\"John Smith\", \"Jane Doe\"],\n","  \"publication_year\": 2022,\n","  \"abstract\": \"This paper presents a novel approach to neural network optimization that improves training efficiency by 30% while maintaining accuracy. We introduce a dynamic learning rate adjustment method that adapts based on gradient consistency across batches.\",\n","  \"methods\": [\"Evaluated on CIFAR-10 and ImageNet\", \"Used ResNet architectures\", \"Trained on 4 NVIDIA A100 GPUs\", \"Batch sizes from 32 to 256\"],\n","  \"findings\": [\"94.2% accuracy on CIFAR-10\", \"76.8% top-1 accuracy on ImageNet\", \"Reduced training time from 24 hours to 16.8 hours\"]\n","}\n","\n","INPUT PAPER:\n","{paper_text}\n","\n","OUTPUT:\n","{format_instructions}\n","\"\"\"\n","\n","# Create a prompt from the template\n","prompt = ChatPromptTemplate.from_template(\n","    template=extraction_template\n",").partial(format_instructions=parser.get_format_instructions())\n","\n","# Create the extraction chain\n","extraction_chain = prompt | llm | parser\n","\n","# Example execution\n","paper_text = research_documents[0].page_content\n","\n","# Extract structured information\n","try:\n","    extracted_data = extraction_chain.invoke({\"paper_text\": paper_text})\n","    print(json.dumps(extracted_data.dict(), indent=2))\n","except Exception as e:\n","    print(f\"Extraction failed: {e}\")\n","    print(\"Attempting to handle the error and extract available information...\")\n","    # You would implement fallback mechanisms here"],"metadata":{"id":"ynhLr7TC8lAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Batch Extraction from Multiple Documents**"],"metadata":{"id":"rhomKo1284Uz"}},{"cell_type":"code","source":["from langchain.output_parsers import PydanticOutputParser\n","from pydantic import BaseModel, Field\n","from typing import List, Optional\n","\n","# Define the data structure we want to extract\n","class ResearchPaper(BaseModel):\n","    title: str = Field(description=\"The title of the research paper\")\n","    authors: List[str] = Field(description=\"List of authors' names\")\n","    publication_year: int = Field(description=\"Year the paper was published\")\n","    abstract: str = Field(description=\"The paper's abstract\")\n","    methods: Optional[List[str]] = Field(description=\"Research methods used\", default=None)\n","    findings: Optional[List[str]] = Field(description=\"Key findings of the paper\", default=None)\n","\n","# Create a parser for this structure\n","parser = PydanticOutputParser(pydantic_object=ResearchPaper)\n","\n","# Create an extraction template with reference examples\n","extraction_template = \"\"\"\n","Extract structured information from the research paper below, following the format of the example.\n","\n","EXAMPLE INPUT:\n","Title: Advances in Neural Information Processing Systems\n","Authors: John Smith, Jane Doe\n","Publication: Conference on Neural Information Processing Systems, 2022\n","Abstract: This paper presents a novel approach to neural network optimization that improves training efficiency by 30% while maintaining accuracy. We introduce a dynamic learning rate adjustment method that adapts based on gradient consistency across batches.\n","Methodology: We evaluated our approach on CIFAR-10 and ImageNet using ResNet architectures. Experiments were conducted using 4 NVIDIA A100 GPUs with batch sizes ranging from 32 to 256.\n","Results: Our method achieved 94.2% accuracy on CIFAR-10 and 76.8% top-1 accuracy on ImageNet, while reducing training time from 24 hours to 16.8 hours compared to baseline methods.\n","\n","EXAMPLE OUTPUT:\n","{\n","  \"title\": \"Advances in Neural Information Processing Systems\",\n","  \"authors\": [\"John Smith\", \"Jane Doe\"],\n","  \"publication_year\": 2022,\n","  \"abstract\": \"This paper presents a novel approach to neural network optimization that improves training efficiency by 30% while maintaining accuracy. We introduce a dynamic learning rate adjustment method that adapts based on gradient consistency across batches.\",\n","  \"methods\": [\"Evaluated on CIFAR-10 and ImageNet\", \"Used ResNet architectures\", \"Trained on 4 NVIDIA A100 GPUs\", \"Batch sizes from 32 to 256\"],\n","  \"findings\": [\"94.2% accuracy on CIFAR-10\", \"76.8% top-1 accuracy on ImageNet\", \"Reduced training time from 24 hours to 16.8 hours\"]\n","}\n","\n","INPUT PAPER:\n","{paper_text}\n","\n","OUTPUT:\n","{format_instructions}\n","\"\"\"\n","\n","# Create a prompt from the template\n","prompt = ChatPromptTemplate.from_template(\n","    template=extraction_template\n",").partial(format_instructions=parser.get_format_instructions())\n","\n","# Create the extraction chain\n","extraction_chain = prompt | llm | parser\n","\n","# Example execution\n","paper_text = research_documents[0].page_content\n","\n","# Extract structured information\n","try:\n","    extracted_data = extraction_chain.invoke({\"paper_text\": paper_text})\n","    print(json.dumps(extracted_data.dict(), indent=2))\n","except Exception as e:\n","    print(f\"Extraction failed: {e}\")\n","    print(\"Attempting to handle the error and extract available information...\")\n","    # You would implement fallback mechanisms here"],"metadata":{"id":"UT-yf3d884gI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Extraction with Format Validation and Error Handling**"],"metadata":{"id":"r2E32eyj8_ki"}},{"cell_type":"code","source":["def validate_and_repair_extraction(extracted_text, parser, llm):\n","    \"\"\"Validate extracted text and attempt to repair if invalid.\"\"\"\n","    try:\n","        # Try to parse the extraction directly\n","        parsed_data = parser.parse(extracted_text)\n","        return parsed_data, True\n","    except Exception as initial_error:\n","        print(f\"Initial parsing failed: {initial_error}\")\n","\n","        # Attempt to repair the output\n","        repair_template = \"\"\"\n","        The following JSON is invalid or doesn't match the expected schema:\n","\n","        {invalid_json}\n","\n","        The expected schema is:\n","        {format_instructions}\n","\n","        Please fix the JSON to match the schema exactly. Return ONLY the fixed JSON, nothing else.\n","        \"\"\"\n","\n","        repair_prompt = ChatPromptTemplate.from_template(repair_template)\n","        repair_chain = repair_prompt | llm\n","\n","        try:\n","            repair_response = repair_chain.invoke({\n","                \"invalid_json\": extracted_text,\n","                \"format_instructions\": parser.get_format_instructions()\n","            })\n","\n","            # Try to parse the repaired JSON\n","            repaired_text = repair_response.content\n","            parsed_data = parser.parse(repaired_text)\n","            print(\"Successfully repaired and parsed the extraction\")\n","            return parsed_data, True\n","        except Exception as repair_error:\n","            print(f\"Repair attempt failed: {repair_error}\")\n","            return None, False\n","\n","# Create a more robust extraction chain\n","def robust_extract(document, llm, parser):\n","    \"\"\"Perform extraction with validation and repair attempts.\"\"\"\n","    # First, get the raw extraction\n","    raw_extraction_template = \"\"\"\n","    Extract structured information from the research paper below.\n","    Return the information in JSON format according to this specification:\n","    {format_instructions}\n","\n","    PAPER:\n","    {paper_text}\n","\n","    EXTRACTED JSON:\n","    \"\"\"\n","\n","    raw_prompt = ChatPromptTemplate.from_template(raw_extraction_template).partial(\n","        format_instructions=parser.get_format_instructions()\n","    )\n","\n","    raw_extraction_chain = raw_prompt | llm\n","\n","    # Get raw extraction\n","    raw_result = raw_extraction_chain.invoke({\"paper_text\": document.page_content})\n","\n","    # Validate and potentially repair\n","    parsed_data, success = validate_and_repair_extraction(\n","        raw_result.content,\n","        parser,\n","        llm\n","    )\n","\n","    if success:\n","        return parsed_data\n","    else:\n","        # Final fallback: try a simplified extraction\n","        print(\"Attempting simplified extraction...\")\n","        simplified_model = create_simplified_model()\n","        simplified_parser = PydanticOutputParser(pydantic_object=simplified_model)\n","\n","        simplified_prompt = ChatPromptTemplate.from_template(raw_extraction_template).partial(\n","            format_instructions=simplified_parser.get_format_instructions()\n","        )\n","\n","        simplified_chain = simplified_prompt | llm\n","        simplified_result = simplified_chain.invoke({\"paper_text\": document.page_content})\n","\n","        try:\n","            return simplified_parser.parse(simplified_result.content)\n","        except Exception as e:\n","            print(f\"Even simplified extraction failed: {e}\")\n","            return {\"title\": \"Extraction Failed\", \"error\": str(e)}\n","\n","def create_simplified_model():\n","    \"\"\"Create a simplified model with fewer required fields.\"\"\"\n","    class SimplifiedPaper(BaseModel):\n","        title: str = Field(description=\"The title of the research paper\")\n","        authors: Optional[List[str]] = Field(description=\"List of authors' names\", default=[])\n","        publication_year: Optional[int] = Field(description=\"Year the paper was published\", default=None)\n","        abstract: Optional[str] = Field(description=\"The paper's abstract\", default=\"\")\n","\n","    return SimplifiedPaper\n","\n","# Test the robust extraction on a potentially challenging document\n","test_document = Document(\n","    page_content=\"\"\"\n","    Title: Challenges in Multi-modal Learning for RAG\n","\n","    Authors: Alex Thompson, Maria Rodriguez\n","\n","    Abstract: This work explores the challenges in incorporating multiple modalities (text, images, audio) in RAG systems. We identify key bottlenecks and propose architectural modifications.\n","    \"\"\"\n",")\n","\n","robust_result = robust_extract(test_document, llm, parser)\n","print(\"\\nRobust extraction result:\")\n","print(json.dumps(robust_result.dict() if hasattr(robust_result, 'dict') else robust_result, indent=2))"],"metadata":{"id":"oaF3ZKdO8_sx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.3.2 Handling Long-Form Content Extraction**\n","\n","---\n","\n","**Create Sample Long Document**"],"metadata":{"id":"Vr3OC9NV9K-B"}},{"cell_type":"code","source":["# Create a sample long financial report document\n","long_financial_report = \"\"\"\n","ANNUAL FINANCIAL REPORT - ACME CORPORATION\n","Fiscal Year 2023\n","\n","EXECUTIVE SUMMARY\n","-----------------\n","Acme Corporation achieved record revenue of $1.2 billion in fiscal year 2023, representing a 15% increase over the previous year. Net income rose to $180 million, up 22% year-over-year. The Board of Directors has approved a dividend of $2.50 per share, payable to shareholders of record as of March 15, 2024.\n","\n","FINANCIAL HIGHLIGHTS\n","-------------------\n","- Revenue: $1.2 billion (+15% YoY)\n","- Gross Profit: $450 million (+18% YoY)\n","- Operating Income: $225 million (+20% YoY)\n","- Net Income: $180 million (+22% YoY)\n","- Earnings Per Share (EPS): $4.25 (+24% YoY)\n","- Return on Equity (ROE): 18.5% (up from 16.2%)\n","- Debt-to-Equity Ratio: 0.65 (improved from 0.72)\n","\n","REVENUE BY SEGMENT\n","-----------------\n","1. Consumer Products: $480 million (+10% YoY)\n","   - North America: $320 million\n","   - Europe: $90 million\n","   - Asia-Pacific: $70 million\n","\n","2. Enterprise Solutions: $350 million (+22% YoY)\n","   - Software Services: $210 million\n","   - Hardware Solutions: $140 million\n","\n","3. Digital Services: $370 million (+18% YoY)\n","   - Cloud Services: $220 million\n","   - Consulting: $150 million\n","\n","BALANCE SHEET SUMMARY\n","--------------------\n","Assets:\n","- Cash and Equivalents: $250 million\n","- Accounts Receivable: $180 million\n","- Inventory: $120 million\n","- Property and Equipment: $450 million\n","- Intangible Assets: $350 million\n","- Other Assets: $150 million\n","Total Assets: $1.5 billion (+12% YoY)\n","\n","Liabilities:\n","- Short-term Debt: $80 million\n","- Accounts Payable: $110 million\n","- Accrued Expenses: $95 million\n","- Long-term Debt: $350 million\n","- Other Liabilities: $140 million\n","Total Liabilities: $775 million (+5% YoY)\n","\n","Stockholders' Equity: $725 million (+20% YoY)\n","\n","CASH FLOW SUMMARY\n","----------------\n","- Operating Cash Flow: $320 million (+25% YoY)\n","- Capital Expenditures: $150 million\n","- Free Cash Flow: $170 million (+32% YoY)\n","- Cash Dividends Paid: $85 million\n","- Share Repurchases: $50 million\n","\n","OUTLOOK FOR 2024\n","---------------\n","The company projects revenue growth of 12-15% for fiscal year 2024, with expected revenue between $1.34-1.38 billion. EPS is projected to be in the range of $4.75-$5.00. The company plans to increase R&D spending by 20% to accelerate product development in artificial intelligence and sustainable technologies.\n","\n","RISK FACTORS\n","-----------\n","1. Market Competition: Increasing competition in the digital services segment may pressure margins.\n","2. Supply Chain: Ongoing global supply chain challenges may impact inventory management.\n","3. Regulatory Environment: New data privacy regulations may require additional compliance investments.\n","4. Currency Fluctuations: Significant operations in international markets expose the company to currency risks.\n","5. Technological Disruption: Rapid technological changes require continued innovation to maintain market position.\n","\n","MANAGEMENT DISCUSSION\n","-------------------\n","\"Our strong performance in 2023 reflects the successful execution of our strategic initiatives,\" said John Smith, CEO. \"We've made significant progress in expanding our digital services offerings while maintaining solid growth in our core consumer products. The investments we've made in automation and operational efficiency have yielded substantial margin improvements.\"\n","\n","\"Our balance sheet remains strong, giving us the flexibility to pursue strategic acquisitions while returning capital to shareholders,\" added Mary Johnson, CFO. \"We've reduced our debt-to-equity ratio while increasing our dividend and share repurchase program.\"\n","\n","AUDITOR'S STATEMENT\n","-----------------\n","The financial statements of Acme Corporation have been audited by Independent Accounting LLP, who expressed an unqualified opinion on these statements. The audit was conducted in accordance with generally accepted auditing standards.\n","\n","For the complete audited financial statements, including notes and detailed disclosures, please refer to the attached appendix.\n","\"\"\"\n","\n","long_legal_contract = \"\"\"\n","COMMERCIAL LEASE AGREEMENT\n","\n","THIS LEASE AGREEMENT (the \"Agreement\") is made and entered into on January 15, 2023, by and between PROPERTY HOLDINGS LLC, a Delaware limited liability company (\"Landlord\"), and TENANT CORPORATION, a Nevada corporation (\"Tenant\").\n","\n","WITNESSETH:\n","\n","WHEREAS, Landlord is the owner of certain real property located at 123 Business Avenue, Metropolis, USA, including a commercial building with approximately 25,000 square feet of leasable space (the \"Building\"); and\n","\n","WHEREAS, Tenant desires to lease approximately 10,000 square feet of space within the Building, as more particularly described in Exhibit A attached hereto (the \"Premises\"), and Landlord desires to lease the Premises to Tenant upon the terms and conditions set forth herein;\n","\n","NOW, THEREFORE, in consideration of the mutual covenants and agreements herein contained, Landlord and Tenant hereby agree as follows:\n","\n","1. PREMISES AND TERM\n","\n","1.1 Premises. Landlord hereby leases to Tenant, and Tenant hereby leases from Landlord, the Premises described in Exhibit A, together with the non-exclusive right to use common areas of the Building and the property on which the Building is located (the \"Property\").\n","\n","1.2 Term. The term of this Lease shall be for a period of five (5) years (the \"Initial Term\"), commencing on March 1, 2023 (the \"Commencement Date\") and ending on February 28, 2028, unless sooner terminated or extended as provided herein.\n","\n","1.3 Option to Extend. Provided Tenant is not in default under this Lease, Tenant shall have one (1) option to extend the term of this Lease for an additional period of five (5) years (the \"Extension Term\") upon the same terms and conditions as set forth in this Lease, except that the Base Rent during the Extension Term shall be as set forth in Section 2.2 below. Tenant shall exercise such option by giving Landlord written notice at least one hundred eighty (180) days prior to the expiration of the Initial Term.\n","\n","2. RENT AND OTHER CHARGES\n","\n","2.1 Base Rent. Tenant shall pay to Landlord as base rent for the Premises the sum of Twenty-Five Dollars ($25.00) per square foot per year, for a total annual rent of Two Hundred Fifty Thousand Dollars ($250,000.00), payable in equal monthly installments of Twenty Thousand Eight Hundred Thirty-Three Dollars and Thirty-Three Cents ($20,833.33) in advance on the first day of each calendar month during the term of this Lease (the \"Base Rent\").\n","\n","2.2 Base Rent During Extension Term. If Tenant exercises its option to extend as provided in Section 1.3, the Base Rent during the Extension Term shall be adjusted to the then-current fair market rental value of the Premises, but in no event less than 103% of the Base Rent in effect during the final year of the Initial Term. The parties shall negotiate in good faith to determine the fair market rental value of the Premises.\n","\n","2.3 Additional Rent. In addition to the Base Rent, Tenant shall pay as additional rent Tenant's Proportionate Share (as defined below) of Operating Expenses (as defined below) to the extent that such Operating Expenses exceed the Operating Expenses for calendar year 2023 (the \"Base Year\"). Tenant's \"Proportionate Share\" shall be forty percent (40%), which is the ratio that the rentable area of the Premises bears to the rentable area of the Building.\n","\n","2.4 Operating Expenses. \"Operating Expenses\" shall mean all costs and expenses incurred by Landlord in the ownership, operation, management, and maintenance of the Building and the Property, including, but not limited to:\n","\n","   (a) Real property taxes and assessments;\n","   (b) Premiums for property, liability, and other insurance carried by Landlord;\n","   (c) Utilities not separately metered to tenants;\n","   (d) Maintenance, repair, and replacement of Building systems, including HVAC, electrical, plumbing, and mechanical systems;\n","   (e) Maintenance and repair of the roof, foundation, exterior walls, and structural elements of the Building;\n","   (f) Maintenance of common areas, including lobbies, elevators, corridors, restrooms, parking areas, landscaping, and sidewalks;\n","   (g) Janitorial services for common areas;\n","   (h) Security services;\n","   (i) Property management fees, not to exceed four percent (4%) of the gross rental receipts for the Building;\n","   (j) Amortization of capital improvements made to reduce Operating Expenses or to comply with applicable laws enacted after the Commencement Date.\n","\"\"\"\n","\n","# Split the documents into manageable chunks\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200\n",")\n","\n","financial_chunks = text_splitter.create_documents([long_financial_report])\n","legal_chunks = text_splitter.create_documents([long_legal_contract])\n","\n","print(f\"Split financial report into {len(financial_chunks)} chunks\")\n","print(f\"Split legal contract into {len(legal_chunks)} chunks\")"],"metadata":{"id":"C1SFKEfF9LGv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Chunk-and-Summarize Approach**"],"metadata":{"id":"NNxRWvZi9ZBj"}},{"cell_type":"code","source":["# Define the extraction template for each chunk\n","chunk_extract_template = \"\"\"\n","Extract all financial metrics mentioned in the following text.\n","Include metric name, value, time period, and any comparison to previous periods.\n","\n","TEXT:\n","{text}\n","\n","EXTRACTED FINANCIAL METRICS:\n","\"\"\"\n","\n","# Define a template for combining extracted metrics\n","combine_template = \"\"\"\n","Below are financial metrics extracted from different sections of a document.\n","Combine them into a single comprehensive list, removing any duplicates.\n","If the same metric appears multiple times with different values, include all instances with their context.\n","\n","EXTRACTED METRICS FROM ALL SECTIONS:\n","{text}\n","\n","CONSOLIDATED FINANCIAL METRICS:\n","\"\"\"\n","\n","# Create the prompts\n","chunk_extract_prompt = PromptTemplate.from_template(chunk_extract_template)\n","combine_prompt = PromptTemplate.from_template(combine_template)\n","\n","# Create chains for processing chunks and combining results\n","chunk_extract_chain = LLMChain(llm=llm, prompt=chunk_extract_prompt)\n","combine_chain = LLMChain(llm=llm, prompt=combine_prompt)\n","\n","# Create the document chains\n","reduce_chain = StuffDocumentsChain(\n","    llm_chain=combine_chain,\n","    document_variable_name=\"text\"\n",")\n","\n","map_reduce_chain = MapReduceDocumentsChain(\n","    llm_chain=chunk_extract_chain,\n","    reduce_documents_chain=reduce_chain,\n","    document_variable_name=\"text\",\n","    return_intermediate_steps=True\n",")\n","\n","# Process the chunked financial report\n","result = map_reduce_chain.invoke(financial_chunks)\n","\n","# Access the results\n","final_summary = result['output_text']\n","chunk_extractions = result['intermediate_steps']\n","\n","print(f\"Found {len(chunk_extractions)} extractions from individual chunks\")\n","print(\"\\nFinal consolidated metrics:\")\n","print(final_summary)"],"metadata":{"id":"w4KjyxsI9ZKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Sequential Extraction with State Tracking**"],"metadata":{"id":"1tKsXm4P9n9U"}},{"cell_type":"code","source":["# Create and execute a simplified sequential extraction\n","def simplified_sequential_extraction(chunks, initial_data=None):\n","    \"\"\"Process document chunks sequentially, updating state as we go.\"\"\"\n","    # Initialize with empty data if none provided\n","    current_data = initial_data or {\n","        \"entities\": {},\n","        \"events\": []\n","    }\n","\n","    # Create a template for extracting from each chunk\n","    extraction_template = \"\"\"\n","    Continue extracting information from this document chunk, based on what we've already extracted.\n","\n","    Current extracted information:\n","    {current_data}\n","\n","    New document chunk ({chunk_num} of {total_chunks}):\n","    {chunk_text}\n","\n","    Please update the extraction with any new information found in this chunk.\n","    Format your response as:\n","\n","    UPDATED ENTITIES:\n","    [key1]: [value1]\n","    [key2]: [value2]\n","    ...\n","\n","    UPDATED EVENTS:\n","    - [event1]\n","    - [event2]\n","    ...\n","    \"\"\"\n","\n","    extraction_prompt = PromptTemplate.from_template(extraction_template)\n","    extraction_chain = LLMChain(llm=llm, prompt=extraction_prompt)\n","\n","    # Process each chunk sequentially\n","    for i, chunk in enumerate(chunks, 1):\n","        print(f\"Processing chunk {i}/{len(chunks)}\")\n","\n","        # Format current data for the prompt\n","        entities_str = \"\\n\".join([f\"{k}: {v}\" for k, v in current_data[\"entities\"].items()])\n","        events_str = \"\\n- \" + \"\\n- \".join(current_data[\"events\"]) if current_data[\"events\"] else \"None yet\"\n","\n","        formatted_data = f\"ENTITIES:\\n{entities_str}\\n\\nEVENTS:\\n{events_str}\"\n","\n","        # Extract from this chunk\n","        response = extraction_chain.invoke({\n","            \"current_data\": formatted_data,\n","            \"chunk_num\": i,\n","            \"total_chunks\": len(chunks),\n","            \"chunk_text\": chunk.page_content\n","        })\n","\n","        # Parse the response\n","        response_text = response['text']\n","\n","        # Extract updated entities\n","        entities_match = re.search(r'UPDATED ENTITIES:(.*?)UPDATED EVENTS:', response_text, re.DOTALL)\n","        if entities_match:\n","            entities_text = entities_match.group(1).strip()\n","            # Parse line by line\n","            for line in entities_text.split('\\n'):\n","                line = line.strip()\n","                if line and ':' in line:\n","                    parts = line.split(':', 1)\n","                    key = parts[0].strip()\n","                    value = parts[1].strip()\n","                    if key and value and value != \"N/A\":\n","                        current_data[\"entities\"][key] = value\n","\n","        # Extract updated events\n","        events_match = re.search(r'UPDATED EVENTS:(.*?)$', response_text, re.DOTALL)\n","        if events_match:\n","            events_text = events_match.group(1).strip()\n","            # Parse line by line\n","            for line in events_text.split('\\n'):\n","                line = line.strip()\n","                if line.startswith('- '):\n","                    event = line[2:].strip()\n","                    if event and event not in current_data[\"events\"] and event != \"N/A\":\n","                        current_data[\"events\"].append(event)\n","\n","    return current_data\n","\n","# Initialize with some schema knowledge to help extraction\n","initial_legal_state = {\n","    \"entities\": {\n","        \"landlord\": \"PROPERTY HOLDINGS LLC (Delaware limited liability company)\",\n","        \"tenant\": \"TENANT CORPORATION (Nevada corporation)\",\n","        \"premises\": \"unknown\"\n","    },\n","    \"events\": []\n","}\n","\n","# Run the sequential extraction on the legal contract chunks\n","print(\"Running sequential extraction with state tracking on legal document...\")\n","legal_state = simplified_sequential_extraction(legal_chunks[:3], initial_legal_state)\n","\n","print(\"\\nFinal extracted state after processing legal document chunks:\")\n","print(\"\\nEntities:\")\n","for key, value in legal_state[\"entities\"].items():\n","    print(f\"  {key}: {value}\")\n","\n","print(\"\\nEvents:\")\n","for event in legal_state[\"events\"]:\n","    print(f\"  - {event}\")"],"metadata":{"id":"v-oSFNx59oNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Targeted Extraction with Section Routing**"],"metadata":{"id":"xBKOWFBmCwbg"}},{"cell_type":"code","source":["def section_based_extraction(document, target_sections):\n","    \"\"\"First identify document sections, then extract from relevant ones.\"\"\"\n","    # Step 1: Identify section boundaries\n","    section_identification_template = \"\"\"\n","    Identify the main sections in the following document.\n","    For each section, provide the section title and where it begins in the document.\n","\n","    DOCUMENT PREVIEW (first 1000 chars):\n","    {document_preview}\n","\n","    SECTIONS (title, start marker):\n","    \"\"\"\n","\n","    section_prompt = PromptTemplate.from_template(section_identification_template)\n","    section_chain = LLMChain(llm=llm, prompt=section_prompt)\n","\n","    # Get section boundaries\n","    response = section_chain.invoke({\n","        \"document_preview\": document[:1000]\n","    })\n","\n","    # Parse the identified sections\n","    sections = {}\n","    for line in response['text'].strip().split('\\n'):\n","        line = line.strip()\n","        if ':' in line:\n","            try:\n","                title, marker = line.split(':', 1)\n","                sections[title.strip().upper()] = marker.strip()\n","            except:\n","                pass\n","\n","    # Add common section names if not found\n","    standard_sections = [\n","        \"EXECUTIVE SUMMARY\", \"FINANCIAL HIGHLIGHTS\", \"REVENUE BY SEGMENT\",\n","        \"BALANCE SHEET\", \"CASH FLOW\", \"OUTLOOK\", \"RISK FACTORS\"\n","    ]\n","\n","    for section in standard_sections:\n","        if section not in sections:\n","            # Try to find it in the document\n","            if section in document:\n","                sections[section] = section\n","\n","    # Step 2: Extract content from targeted sections\n","    extracted_data = {}\n","\n","    for section_name, extraction_instructions in target_sections.items():\n","        section_start = None\n","\n","        # Try to find the section\n","        for known_section, marker in sections.items():\n","            if section_name.upper() in known_section:\n","                section_start = document.find(marker)\n","                if section_start == -1:  # If marker not found, try section name\n","                    section_start = document.find(known_section)\n","                break\n","\n","        if section_start is None:\n","            # Try direct search with the section name\n","            section_start = document.find(section_name.upper())\n","\n","        if section_start >= 0:\n","            # Find the next section or end of document\n","            next_section_start = len(document)\n","            for marker in sections.values():\n","                pos = document.find(marker, section_start + len(marker))\n","                if pos > section_start and pos < next_section_start:\n","                    next_section_start = pos\n","\n","            # Extract this section content\n","            section_content = document[section_start:next_section_start].strip()\n","\n","            # Create extraction prompt\n","            extraction_template = f\"\"\"\n","            Extract the following information from this section:\n","            {extraction_instructions}\n","\n","            SECTION TEXT:\n","            {{section_text}}\n","\n","            EXTRACTED INFORMATION:\n","            \"\"\"\n","\n","            extraction_prompt = PromptTemplate.from_template(extraction_template)\n","            extraction_chain = LLMChain(llm=llm, prompt=extraction_prompt)\n","\n","            # Extract from this section\n","            result = extraction_chain.invoke({\"section_text\": section_content})\n","            extracted_data[section_name] = result['text']\n","\n","    return extracted_data\n","\n","# Define targeted sections for financial report\n","financial_targets = {\n","    \"FINANCIAL HIGHLIGHTS\": \"Extract all financial metrics including revenue, profit, EPS, and growth percentages.\",\n","    \"REVENUE BY SEGMENT\": \"Extract revenue for each business segment and their growth rates.\",\n","    \"OUTLOOK\": \"Extract revenue projections, growth targets, and key strategic initiatives for next year.\"\n","}\n","\n","# Try the section-based extraction on the financial report\n","print(\"\\nRunning section-based extraction on financial report...\")\n","financial_sections = section_based_extraction(long_financial_report, financial_targets)\n","\n","print(\"\\nExtracted information by section:\")\n","for section, data in financial_sections.items():\n","    print(f\"\\n--- {section} ---\")"],"metadata":{"id":"hWo4_NxhCwqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.3.3 Function-Free Extraction Methods**\n","\n","---\n","\n","**Template-Based Extraction with Output Parsing**"],"metadata":{"id":"USyKk28bC_F2"}},{"cell_type":"code","source":["from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n","from langchain.prompts import PromptTemplate\n","\n","# Define the response schemas\n","response_schemas = [\n","    ResponseSchema(name=\"person_name\", description=\"The full name of the person\"),\n","    ResponseSchema(name=\"age\", description=\"The age of the person as an integer\"),\n","    ResponseSchema(name=\"occupation\", description=\"The person's job or primary occupation\"),\n","    ResponseSchema(name=\"location\", description=\"Where the person currently lives\"),\n","    ResponseSchema(name=\"interests\", description=\"A list of the person's main interests or hobbies\")\n","]\n","\n","# Create a parser that can handle this structure\n","parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","# Get the format instructions\n","format_instructions = parser.get_format_instructions()\n","\n","# Create the extraction template\n","template = \"\"\"\n","Extract the following information about the person described in the text.\n","Be precise and only extract information explicitly stated in the text.\n","If information is not present, output \"unknown\" for that field.\n","\n","{format_instructions}\n","\n","TEXT:\n","{text}\n","\n","EXTRACTED INFORMATION:\n","\"\"\"\n","\n","# Create the prompt\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"text\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")\n","\n","# Create the extraction chain\n","extraction_chain = prompt | llm | parser\n","\n","# Example text for extraction\n","text = \"\"\"\n","Jane Smith is a 42-year-old software engineer living in Portland, Oregon.\n","She has been in the tech industry for over 15 years, specializing in backend systems.\n","In her free time, Jane enjoys hiking the trails of the Pacific Northwest, reading science fiction novels,\n","and experimenting with new recipes in her kitchen. She's also recently taken up pottery classes.\n","\"\"\"\n","\n","# Perform the extraction\n","try:\n","    result = extraction_chain.invoke({\"text\": text})\n","    print(\"Structured output using template-based extraction:\")\n","    print(json.dumps(result, indent=2))\n","except Exception as e:\n","    print(f\"Extraction failed: {e}\")"],"metadata":{"id":"3vURLvIUC_Sc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Delimiter-Based Extraction**"],"metadata":{"id":"X2bwP0huDNOo"}},{"cell_type":"code","source":["# Define a delimiter-based extraction template\n","delimiter = \"####\"\n","\n","extraction_template = f\"\"\"\n","Extract key information from the text below.\n","Format your response as follows:\n","\n","{delimiter} PERSON NAME\n","[The full name of the person]\n","\n","{delimiter} AGE\n","[The age as a number]\n","\n","{delimiter} OCCUPATION\n","[The person's job]\n","\n","{delimiter} LOCATION\n","[Where the person lives]\n","\n","{delimiter} INTERESTS\n","[A comma-separated list of the person's interests or hobbies]\n","\n","TEXT:\n","{{text}}\n","\n","EXTRACTED INFORMATION:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    template=extraction_template,\n","    input_variables=[\"text\"]\n",")\n","\n","# Create the extraction chain\n","raw_extraction_chain = prompt | llm\n","\n","# Parse the delimited output\n","def parse_delimited_output(output):\n","    \"\"\"Parse a response with delimiter-separated sections.\"\"\"\n","    extracted_data = {}\n","\n","    # Get the text content from the output (which could be an AIMessage)\n","    if hasattr(output, 'content'):  # Handle AIMessage object\n","        output_text = output.content\n","    elif isinstance(output, dict) and 'text' in output:  # Handle dictionary\n","        output_text = output['text']\n","    else:  # Try using the output directly\n","        output_text = str(output)\n","\n","    # Split on the delimiter\n","    sections = output_text.split(delimiter)\n","\n","    # Process each section\n","    for section in sections:\n","        section = section.strip()\n","        if not section:\n","            continue\n","\n","        # The first line after delimiter is the field name\n","        lines = section.split('\\n', 1)\n","        if len(lines) < 2:\n","            continue\n","\n","        field_name = lines[0].strip().lower().replace(' ', '_')\n","        field_value = lines[1].strip()\n","\n","        extracted_data[field_name] = field_value\n","\n","    return extracted_data\n","\n","# Combine the chain with the parser\n","def extract_with_delimiters(text):\n","    \"\"\"Extract information using delimiter-based format.\"\"\"\n","    raw_output = raw_extraction_chain.invoke({\"text\": text})\n","    parsed_output = parse_delimited_output(raw_output)\n","    return parsed_output\n","\n","# Test extraction\n","result = extract_with_delimiters(text)\n","print(\"\\nDelimiter-based extraction results:\")\n","print(json.dumps(result, indent=2))"],"metadata":{"id":"KUQ32xceDNZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**JSON Schema-Guided Extraction**"],"metadata":{"id":"No8eceO2NXrr"}},{"cell_type":"code","source":["# Define a JSON schema to guide extraction\n","json_schema = {\n","    \"type\": \"object\",\n","    \"properties\": {\n","        \"person_name\": {\n","            \"type\": \"string\",\n","            \"description\": \"The full name of the person\"\n","        },\n","        \"age\": {\n","            \"type\": \"integer\",\n","            \"description\": \"The age of the person in years\"\n","        },\n","        \"occupation\": {\n","            \"type\": \"string\",\n","            \"description\": \"The person's job or primary occupation\"\n","        },\n","        \"location\": {\n","            \"type\": \"object\",\n","            \"properties\": {\n","                \"city\": {\"type\": \"string\"},\n","                \"state\": {\"type\": \"string\"},\n","                \"country\": {\"type\": \"string\"}\n","            },\n","            \"description\": \"Where the person currently lives\"\n","        },\n","        \"interests\": {\n","            \"type\": \"array\",\n","            \"items\": {\"type\": \"string\"},\n","            \"description\": \"The person's hobbies or interests\"\n","        },\n","        \"years_of_experience\": {\n","            \"type\": \"integer\",\n","            \"description\": \"Years of professional experience\"\n","        }\n","    },\n","    \"required\": [\"person_name\"]\n","}\n","\n","# Create a JSON schema extraction template\n","json_extraction_template = \"\"\"\n","Extract structured information about the person described in the text according to this JSON schema:\n","\n","{schema}\n","\n","If information for a field is not explicitly mentioned in the text, omit that field from the output.\n","All output should be valid JSON.\n","\n","TEXT:\n","{text}\n","\n","EXTRACTED JSON:\n","\"\"\"\n","\n","json_prompt = PromptTemplate(\n","    template=json_extraction_template,\n","    input_variables=[\"text\", \"schema\"]\n",")\n","\n","# Format the schema as a readable string\n","formatted_schema = json.dumps(json_schema, indent=2)\n","\n","# Create the extraction chain\n","json_extraction_chain = json_prompt | llm\n","\n","# Handle parsing\n","def extract_with_json_schema(text):\n","    \"\"\"Extract information using JSON schema guidance.\"\"\"\n","    raw_output = json_extraction_chain.invoke({\n","        \"text\": text,\n","        \"schema\": formatted_schema\n","    })\n","\n","    # Extract JSON from the response\n","    # Handle different output types (AIMessage or dictionary)\n","    if hasattr(raw_output, 'content'):  # Handle AIMessage object\n","        output_text = raw_output.content\n","    elif isinstance(raw_output, dict) and 'text' in raw_output:  # Handle dictionary\n","        output_text = raw_output['text']\n","    else:  # Try using the output directly\n","        output_text = str(raw_output)\n","\n","    # Clean up the output to extract valid JSON\n","    output_text = output_text.strip()\n","\n","    # Find JSON block if enclosed in triple backticks\n","    json_match = re.search(r'```(?:json)?\\s*(.*?)\\s*```', output_text, re.DOTALL)\n","    if json_match:\n","        json_str = json_match.group(1)\n","    else:\n","        # Try to extract anything that looks like JSON\n","        json_match = re.search(r'({.*})', output_text, re.DOTALL)\n","        if json_match:\n","            json_str = json_match.group(1)\n","        else:\n","            json_str = output_text\n","\n","    # Try to parse the JSON\n","    try:\n","        return json.loads(json_str)\n","    except json.JSONDecodeError as e:\n","        return {\"error\": f\"Failed to parse JSON: {str(e)}\", \"raw\": json_str}\n","\n","# Test extraction\n","result = extract_with_json_schema(text)\n","print(\"\\nJSON schema-guided extraction results:\")\n","print(json.dumps(result, indent=2))"],"metadata":{"id":"p0DOapffNX3C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Tabular Extraction with Markdown**"],"metadata":{"id":"7DWARHBAOFhv"}},{"cell_type":"code","source":["def extract_tabular_data(text, columns):\n","    \"\"\"Extract data in tabular format using markdown tables.\"\"\"\n","    # Create a column list for the prompt\n","    column_list = \", \".join(columns)\n","\n","    table_extraction_template = f\"\"\"\n","    Extract information from the text into a markdown table.\n","\n","    The table should have these columns: {column_list}\n","\n","    Format the table properly with markdown syntax, including the header row and separator row.\n","    Only include information explicitly stated in the text. If information for a column is not available, use \"N/A\".\n","\n","    TEXT:\n","    {{text}}\n","\n","    MARKDOWN TABLE:\n","    \"\"\"\n","\n","    table_prompt = PromptTemplate(\n","        template=table_extraction_template,\n","        input_variables=[\"text\"]\n","    )\n","\n","    # Create the extraction chain\n","    table_extraction_chain = table_prompt | llm\n","\n","    # Execute and get the table\n","    result = table_extraction_chain.invoke({\"text\": text})\n","\n","    # Handle different output types\n","    if hasattr(result, 'content'):  # Handle AIMessage object\n","        result_text = result.content\n","    elif isinstance(result, dict) and 'text' in result:  # Handle dictionary\n","        result_text = result['text']\n","    else:  # Try using the result directly\n","        result_text = str(result)\n","\n","    # Parse the markdown table into a list of dictionaries\n","    return parse_markdown_table(result_text)\n","\n","def parse_markdown_table(markdown_table):\n","    \"\"\"Parse a markdown table into a list of dictionaries.\"\"\"\n","    # Clean and extract table content\n","    table_text = markdown_table.strip()\n","\n","    # Split into lines\n","    lines = table_text.split('\\n')\n","\n","    # Need at least 3 lines for a valid table (header, separator, data)\n","    if len(lines) < 3:\n","        return []\n","\n","    # Process the header row\n","    header_line = lines[0].strip()\n","    if header_line.startswith('|'):\n","        header_line = header_line[1:]\n","    if header_line.endswith('|'):\n","        header_line = header_line[:-1]\n","\n","    headers = [h.strip() for h in header_line.split('|')]\n","\n","    # Skip the separator line\n","    data_rows = []\n","    for line in lines[2:]:\n","        line = line.strip()\n","        if not line or '|' not in line:\n","            continue\n","\n","        # Remove leading/trailing |\n","        if line.startswith('|'):\n","            line = line[1:]\n","        if line.endswith('|'):\n","            line = line[:-1]\n","\n","        values = [v.strip() for v in line.split('|')]\n","\n","        # Create a dictionary for this row\n","        if len(values) == len(headers):\n","            row_dict = {}\n","            for i, header in enumerate(headers):\n","                row_dict[header] = values[i]\n","            data_rows.append(row_dict)\n","\n","    return data_rows\n","\n","# Create a sample text with multiple people to extract\n","multi_person_text = \"\"\"\n","The research team consists of several key members:\n","\n","Dr. John Williams is a 45-year-old senior researcher specializing in artificial intelligence. Based in Boston, Massachusetts, he has published over 30 papers on machine learning. When not working, he enjoys playing chess and hiking.\n","\n","Sarah Chen, 38, is an associate professor of computer science at Stanford University. Living in Palo Alto, California, she leads the natural language processing group. Her interests include playing violin and competitive swimming.\n","\n","Marcus Johnson, a 29-year-old research assistant from Austin, Texas, recently joined the team. He has a background in statistics and data visualization. In his free time, he enjoys rock climbing and photography.\n","\n","Emily Rodriguez is the team's technical writer. At 34, she has 10 years of experience translating complex research into accessible content. Based in Seattle, Washington, she enjoys gardening and painting landscapes.\n","\"\"\"\n","\n","# Extract into a table format\n","table_columns = [\"Name\", \"Age\", \"Occupation\", \"Location\", \"Interests\"]\n","tabular_result = extract_tabular_data(multi_person_text, table_columns)\n","\n","print(\"\\nTabular extraction results:\")\n","for row in tabular_result:\n","    print(json.dumps(row, indent=2))\n","\n","# Display as a formatted table\n","print(\"\\nFormatted as table:\")\n","print(f\"| {'Name':<15} | {'Age':<5} | {'Occupation':<25} | {'Location':<20} | {'Interests':<30} |\")\n","print(f\"| {'-'*15} | {'-'*5} | {'-'*25} | {'-'*20} | {'-'*30} |\")\n","for row in tabular_result:\n","    name = row.get('Name', 'N/A')\n","    age = row.get('Age', 'N/A')\n","    occupation = row.get('Occupation', 'N/A')\n","    location = row.get('Location', 'N/A')\n","    interests = row.get('Interests', 'N/A')\n","    print(f\"| {name:<15} | {age:<5} | {occupation:<25} | {location:<20} | {interests:<30} |\")"],"metadata":{"id":"bi-2_wNHOFql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","This notebook has demonstrated three key extraction patterns for RAG systems:\n","\n","1. **Reference-Based Extraction**: Using examples to guide extraction with consistent formatting\n","2. **Long-Form Content Extraction**: Techniques for handling documents longer than the context window\n","3. **Function-Free Extraction Methods**: Reliable extraction approaches that don't require function calling\n"],"metadata":{"id":"m8nHUP-FOzI6"}}]}