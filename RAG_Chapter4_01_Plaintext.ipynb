{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9PebDm7TIBAgzgzBD3EaD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","!pip install chardet pandas numpy\n","\n","# Import necessary libraries\n","import os\n","import chardet\n","import json\n","import re\n","from typing import List, Optional\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","from google.colab import files\n","import io\n","\n","# Test imports\n","print(\"Setup complete! All required packages installed.\")\n"],"metadata":{"id":"QOk7LS2YLlEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_sample_files():\n","    \"\"\"Creates sample files demonstrating different document loading scenarios\"\"\"\n","\n","    # Create directory for our samples\n","    !mkdir -p rag_sample_data\n","\n","    # Create business memo\n","    business_content = \"\"\"\n","    QUARTERLY BUSINESS REVIEW\n","    Date: February 9, 2025\n","    Department: Engineering\n","\n","    PERFORMANCE METRICS:\n","    • Project completion rate: 95%\n","    • Code quality score: 9.2/10\n","    • Customer satisfaction: 4.8/5\n","\n","    ACTION ITEMS:\n","    1. Review Q2 objectives\n","    2. Update team KPIs\n","    3. Schedule stakeholder meeting\n","    \"\"\"\n","\n","    with open('rag_sample_data/business_memo.txt', 'w') as f:\n","        f.write(business_content)\n","\n","    # Create technical documentation\n","    technical_content = \"\"\"\n","    API DOCUMENTATION\n","    ================\n","\n","    Endpoint: /api/v1/documents\n","    Method: POST\n","\n","    Request Format:\n","    {\n","        \"document_id\": \"string\",\n","        \"content\": \"string\",\n","        \"metadata\": {\n","            \"author\": \"string\",\n","            \"date\": \"ISO-8601 timestamp\"\n","        }\n","    }\n","    \"\"\"\n","\n","    with open('rag_sample_data/technical_doc.txt', 'w') as f:\n","        f.write(technical_content)\n","\n","    # Create multilingual content\n","    multilingual_content = \"\"\"\n","    Global Documentation Guidelines\n","    ============================\n","    English: Please follow the style guide\n","    中文: 请遵守文体指南\n","    日本語: スタイルガイドに従ってください\n","    हिंदी: कृपया स्टाइल गाइड का पालन करें\n","    \"\"\"\n","\n","    with open('rag_sample_data/multilingual.txt', 'w', encoding='utf-8') as f:\n","        f.write(multilingual_content)\n","\n","# Create sample files\n","create_sample_files()\n","\n","# Verify creation\n","!ls -l rag_sample_data/"],"metadata":{"id":"foVSCPJqO3Yk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EnhancedTextLoader:\n","    \"\"\"\n","    A comprehensive text loader designed for RAG systems.\n","    Handles encoding detection, metadata extraction, and content cleaning.\n","    \"\"\"\n","    def __init__(self, file_path: str, encoding: Optional[str] = None):\n","        \"\"\"Initialize the loader with a file path and optional encoding.\"\"\"\n","        self.file_path = file_path\n","        self.encoding = encoding\n","        self.metadata = {}\n","\n","    def detect_encoding(self) -> str:\n","        \"\"\"\n","        Automatically detect file encoding.\n","        Returns the detected encoding string.\n","        \"\"\"\n","        with open(self.file_path, 'rb') as file:\n","            raw_data = file.read()\n","            result = chardet.detect(raw_data)\n","            return result['encoding']\n","\n","    def extract_metadata(self, content: str) -> dict:\n","        \"\"\"Extract useful metadata about the document.\"\"\"\n","        lines = content.split('\\n')\n","        metadata = {\n","            'filename': os.path.basename(self.file_path),\n","            'file_size': os.path.getsize(self.file_path),\n","            'line_count': len(lines),\n","            'word_count': len(content.split()),\n","            'char_count': len(content),\n","            'avg_line_length': sum(len(line) for line in lines) / len(lines) if lines else 0,\n","            'has_unicode': any(ord(c) > 127 for c in content)\n","        }\n","        return metadata\n","\n","    def clean_text(self, content: str) -> str:\n","        \"\"\"Clean text while preserving document structure.\"\"\"\n","        # Remove null bytes and other control characters\n","        content = content.replace('\\x00', '')\n","\n","        # Normalize line endings\n","        content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n","\n","        # Split into lines for processing\n","        lines = content.splitlines()\n","        cleaned_lines = []\n","\n","        for line in lines:\n","            # Preserve indentation\n","            indent = len(line) - len(line.lstrip())\n","            cleaned_line = line.strip()\n","\n","            # Skip empty lines\n","            if cleaned_line:\n","                # Restore indentation with spaces\n","                cleaned_lines.append(' ' * indent + cleaned_line)\n","            else:\n","                cleaned_lines.append('')\n","\n","        return '\\n'.join(cleaned_lines)\n","\n","    def load(self) -> tuple[str, dict]:\n","        \"\"\"\n","        Load and process the text file.\n","        Returns tuple of (cleaned_content, metadata).\n","        \"\"\"\n","        try:\n","            # Detect encoding if not specified\n","            if not self.encoding:\n","                self.encoding = self.detect_encoding()\n","\n","            # Read the file\n","            with open(self.file_path, 'r', encoding=self.encoding) as file:\n","                content = file.read()\n","\n","            # Clean the content\n","            cleaned_content = self.clean_text(content)\n","\n","            # Extract metadata\n","            self.metadata = self.extract_metadata(cleaned_content)\n","\n","            return cleaned_content, self.metadata\n","\n","        except Exception as e:\n","            print(f\"Error loading file {self.file_path}: {str(e)}\")\n","            return None, None"],"metadata":{"id":"XM81QrcrP7mz","executionInfo":{"status":"ok","timestamp":1739080059264,"user_tz":-330,"elapsed":35,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Test the complete implementation\n","def test_loader():\n","    \"\"\"Test the EnhancedTextLoader with our sample documents.\"\"\"\n","    sample_files = [\n","        \"rag_sample_data/business_memo.txt\",\n","        \"rag_sample_data/technical_doc.txt\",\n","        \"rag_sample_data/multilingual.txt\"\n","    ]\n","\n","    for file_path in sample_files:\n","        print(f\"\\nProcessing: {file_path}\")\n","        loader = EnhancedTextLoader(file_path)\n","        content, metadata = loader.load()\n","\n","        if content is not None:\n","            print(\"\\nMetadata:\")\n","            for key, value in metadata.items():\n","                print(f\"{key}: {value}\")\n","\n","            print(\"\\nFirst 100 characters of content:\")\n","            print(content[:100])\n","            print(\"-\" * 50)\n","        else:\n","            print(f\"Failed to load {file_path}\")\n","\n","# Run the test\n","test_loader()"],"metadata":{"id":"lBt3WZ2hNx0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def handle_different_encodings():\n","    \"\"\"Demonstrate handling of different text encodings\"\"\"\n","\n","    # Create test files with different encodings\n","    test_content = \"This is a test file with special characters: é, ñ, 漢\"\n","\n","    # UTF-8 encoded file\n","    with open('rag_sample_data/utf8_test.txt', 'w', encoding='utf-8') as f:\n","        f.write(test_content)\n","\n","    # Try loading without specifying encoding\n","    loader = EnhancedTextLoader('rag_sample_data/utf8_test.txt')\n","    content, metadata = loader.load()\n","    print(\"Auto-detected encoding:\", metadata.get('encoding', 'unknown'))\n","\n","    # Try loading with explicit encoding\n","    loader_explicit = EnhancedTextLoader('rag_sample_data/utf8_test.txt', encoding='utf-8')\n","    content_explicit, metadata_explicit = loader_explicit.load()\n","    print(\"Explicit encoding results match:\", content == content_explicit)\n","\n","# Test encoding handling\n","handle_different_encodings()"],"metadata":{"id":"p5XLlOooQjjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_structure_preservation():\n","    \"\"\"Demonstrate how document structure is preserved\"\"\"\n","\n","    # Create a test file with specific formatting\n","    structured_content = \"\"\"\n","    SECTION 1:\n","        • First bullet point\n","        • Second bullet point\n","            - Sub bullet\n","\n","    SECTION 2:\n","        1. Numbered item\n","        2. Another item\n","           More details here\n","    \"\"\"\n","\n","    with open('rag_sample_data/structured_test.txt', 'w') as f:\n","        f.write(structured_content)\n","\n","    # Load and verify structure preservation\n","    loader = EnhancedTextLoader('rag_sample_data/structured_test.txt')\n","    content, _ = loader.load()\n","    print(\"Original structure maintained:\")\n","    print(content)\n","\n","# Test structure preservation\n","test_structure_preservation()"],"metadata":{"id":"2_xqpqdyQ8fZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def demonstrate_metadata_extraction():\n","    \"\"\"Show comprehensive metadata extraction\"\"\"\n","\n","    test_files = [\n","        'rag_sample_data/business_memo.txt',\n","        'rag_sample_data/technical_doc.txt',\n","        'rag_sample_data/multilingual.txt'\n","    ]\n","\n","    for file_path in test_files:\n","        loader = EnhancedTextLoader(file_path)\n","        _, metadata = loader.load()\n","\n","        print(f\"\\nMetadata for {os.path.basename(file_path)}:\")\n","        for key, value in metadata.items():\n","            print(f\"{key}: {value}\")\n","\n","# Test metadata extraction\n","demonstrate_metadata_extraction()"],"metadata":{"id":"cdpTiEkYQ-6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_error_handling():\n","    \"\"\"Demonstrate robust error handling\"\"\"\n","\n","    # Test with non-existent file\n","    loader = EnhancedTextLoader('non_existent.txt')\n","    content, metadata = loader.load()\n","    print(\"Non-existent file handled gracefully:\", content is None)\n","\n","    # Test with corrupt file\n","    with open('rag_sample_data/corrupt.txt', 'wb') as f:\n","        f.write(b'\\x80\\x81\\x82\\x83')  # Invalid bytes\n","\n","    loader = EnhancedTextLoader('rag_sample_data/corrupt.txt')\n","    content, metadata = loader.load()\n","    print(\"Corrupt file handled gracefully:\", content is None)\n","\n","# Test error handling\n","test_error_handling()"],"metadata":{"id":"7mCktSxDRdpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def comprehensive_testing():\n","    \"\"\"Run comprehensive tests with various document types\"\"\"\n","\n","    # Test results container\n","    test_results = {\n","        'total_tests': 0,\n","        'passed': 0,\n","        'failed': 0,\n","        'issues': []\n","    }\n","\n","    # Test all sample files\n","    for file_path in os.listdir('rag_sample_data'):\n","        test_results['total_tests'] += 1\n","        try:\n","            loader = EnhancedTextLoader(f'rag_sample_data/{file_path}')\n","            content, metadata = loader.load()\n","            if content and metadata:\n","                test_results['passed'] += 1\n","            else:\n","                test_results['failed'] += 1\n","                test_results['issues'].append(f\"Failed to load {file_path}\")\n","        except Exception as e:\n","            test_results['failed'] += 1\n","            test_results['issues'].append(f\"Error processing {file_path}: {str(e)}\")\n","\n","    # Print test results\n","    print(\"\\nTest Results:\")\n","    print(f\"Total Tests: {test_results['total_tests']}\")\n","    print(f\"Passed: {test_results['passed']}\")\n","    print(f\"Failed: {test_results['failed']}\")\n","    if test_results['issues']:\n","        print(\"\\nIssues Found:\")\n","        for issue in test_results['issues']:\n","            print(f\"- {issue}\")\n","\n","# Run comprehensive tests\n","comprehensive_testing()"],"metadata":{"id":"un_g_2f6Rfu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Practical Exercices**"],"metadata":{"id":"3EdUsomQSoBr"}},{"cell_type":"code","source":["class HierarchicalTextLoader(EnhancedTextLoader):\n","    \"\"\"\n","    Enhanced loader that understands document hierarchies.\n","    Extends EnhancedTextLoader with section detection capabilities.\n","    \"\"\"\n","    def extract_hierarchy(self, content: str) -> dict:\n","        \"\"\"Extract document hierarchy from markdown-style headers\"\"\"\n","        sections = {}\n","        current_section = None\n","        current_level = 0\n","\n","        for line in content.split('\\n'):\n","            if line.strip().startswith('#'):\n","                # Count header level\n","                level = len(line.strip()) - len(line.strip().lstrip('#'))\n","                title = line.strip('#').strip()\n","\n","                if level == 1:\n","                    sections[title] = {'content': [], 'subsections': {}}\n","                    current_section = title\n","                    current_level = 1\n","                elif current_section and level > current_level:\n","                    sections[current_section]['subsections'][title] = []\n","            else:\n","                if current_section and line.strip():\n","                    if current_level == 1:\n","                        sections[current_section]['content'].append(line)\n","\n","        return sections\n","\n","def test_hierarchical_loader():\n","    \"\"\"Test the hierarchical document loader\"\"\"\n","\n","    # Create a test document with clear hierarchy\n","    hierarchical_content = \"\"\"\n","    # Main Title\n","\n","    ## Section 1\n","    This is the first section content.\n","\n","    ### Subsection 1.1\n","    Deeper level content here.\n","\n","    ## Section 2\n","    Another main section.\n","\n","    ### Subsection 2.1\n","    More content here.\n","    \"\"\"\n","\n","    with open('rag_sample_data/hierarchical_doc.txt', 'w') as f:\n","        f.write(hierarchical_content)\n","\n","    # Test the implementation\n","    loader = HierarchicalTextLoader('rag_sample_data/hierarchical_doc.txt')\n","    content, metadata = loader.load()\n","    hierarchy = loader.extract_hierarchy(content)\n","    print(\"Document Hierarchy:\")\n","    print(json.dumps(hierarchy, indent=2))\n","\n","# Run the test\n","test_hierarchical_loader()"],"metadata":{"id":"Q6VDYRKyTgAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MetadataEnhancedLoader(EnhancedTextLoader):\n","    \"\"\"\n","    Enhanced loader with advanced metadata analysis capabilities.\n","    \"\"\"\n","    def analyze_content_patterns(self, content: str) -> dict:\n","        \"\"\"Analyze content for various patterns and metrics\"\"\"\n","\n","        # Sentence analysis\n","        sentences = [s.strip() for s in content.split('.') if s.strip()]\n","        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n","\n","        # Paragraph analysis\n","        paragraphs = [p for p in content.split('\\n\\n') if p.strip()]\n","        avg_paragraph_length = sum(len(p.split()) for p in paragraphs) / len(paragraphs) if paragraphs else 0\n","\n","        # Common words analysis\n","        words = content.lower().split()\n","        word_freq = Counter(words).most_common(10)\n","\n","        return {\n","            'avg_sentence_length': round(avg_sentence_length, 2),\n","            'avg_paragraph_length': round(avg_paragraph_length, 2),\n","            'paragraph_count': len(paragraphs),\n","            'common_words': dict(word_freq)\n","        }\n","\n","    def load(self) -> tuple[str, dict]:\n","        \"\"\"Override load to include pattern analysis\"\"\"\n","        content, metadata = super().load()\n","        if content:\n","            metadata['content_patterns'] = self.analyze_content_patterns(content)\n","        return content, metadata\n","\n","def test_metadata_loader():\n","    \"\"\"Test the enhanced metadata analysis\"\"\"\n","    loader = MetadataEnhancedLoader('rag_sample_data/business_memo.txt')\n","    content, metadata = loader.load()\n","    print(\"Enhanced Content Analysis:\")\n","    print(json.dumps(metadata.get('content_patterns', {}), indent=2))\n","\n","# Run the test\n","test_metadata_loader()"],"metadata":{"id":"LIUpp3sJTlvS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","class MultilingualLoader(EnhancedTextLoader):\n","    \"\"\"\n","    Enhanced loader with multilingual text support.\n","    \"\"\"\n","    def detect_languages(self, content: str) -> dict:\n","        \"\"\"Detect languages present in the document\"\"\"\n","        patterns = {\n","            'latin': r'[a-zA-Z]',\n","            'chinese': r'[\\u4e00-\\u9fff]',\n","            'japanese': r'[\\u3040-\\u30ff]',\n","            'korean': r'[\\uac00-\\ud7af]',\n","            'devanagari': r'[\\u0900-\\u097f]'\n","        }\n","\n","        language_presence = {}\n","        for lang, pattern in patterns.items():\n","            matches = len(re.findall(pattern, content))\n","            if matches > 0:\n","                language_presence[lang] = matches\n","\n","        return language_presence\n","\n","    def load(self) -> tuple[str, dict]:\n","        \"\"\"Override load to include language detection\"\"\"\n","        content, metadata = super().load()\n","        if content:\n","            metadata['languages'] = self.detect_languages(content)\n","        return content, metadata\n","\n","# Test multilingual support\n","def test_multilingual_loader():\n","    \"\"\"Test the multilingual loader\"\"\"\n","    loader = MultilingualLoader('rag_sample_data/multilingual.txt')\n","    content, metadata = loader.load()\n","    print(\"Language Detection Results:\")\n","    print(json.dumps(metadata['languages'], indent=2))\n","\n","# Run the test\n","test_multilingual_loader()"],"metadata":{"id":"2F-0_qIjUK5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ComprehensiveDocumentLoader(HierarchicalTextLoader, MetadataEnhancedLoader, MultilingualLoader):\n","    \"\"\"\n","    A comprehensive document loader that combines hierarchical structure analysis,\n","    enhanced metadata extraction, and multilingual support.\n","    \"\"\"\n","    def __init__(self, file_path: str, encoding: Optional[str] = None):\n","        super().__init__(file_path, encoding)\n","        self.hierarchy = {}\n","        self.patterns = {}\n","        self.languages = {}\n","\n","    def load(self) -> tuple[str, dict]:\n","        \"\"\"\n","        Load and analyze document with all available features.\n","        Returns tuple of (content, comprehensive_metadata).\n","        \"\"\"\n","        # First, load content using parent class method\n","        content, base_metadata = super().load()\n","\n","        if content:\n","            # Extract hierarchical structure\n","            self.hierarchy = self.extract_hierarchy(content)\n","\n","            # Analyze content patterns\n","            self.patterns = self.analyze_content_patterns(content)\n","\n","            # Detect languages\n","            self.languages = self.detect_languages(content)\n","\n","            # Combine all metadata\n","            comprehensive_metadata = {\n","                **base_metadata,\n","                'document_structure': self.hierarchy,\n","                'content_analysis': self.patterns,\n","                'language_analysis': self.languages\n","            }\n","\n","            return content, comprehensive_metadata\n","\n","        return None, None\n","\n","def test_comprehensive_loader():\n","    \"\"\"Test the comprehensive document loader with a complex document\"\"\"\n","\n","    # Create a complex test document that exercises all features\n","    complex_content = \"\"\"\n","    # Technical Documentation\n","\n","    ## Introduction\n","    This is a multilingual technical guide.\n","    这是一个多语言技术指南。\n","\n","    ## System Architecture\n","    The system consists of three main components:\n","    • Frontend Interface\n","    • Backend API\n","    • Database Layer\n","\n","    ## Implementation Details\n","    Here are the key implementation points:\n","    1. Use RESTful principles\n","    2. Follow security guidelines\n","    3. Implement proper error handling\n","\n","    ### Code Examples\n","    Example implementation:\n","    ```python\n","    def process_data():\n","        return {\"status\": \"success\"}\n","    ```\n","    \"\"\"\n","\n","    # Write test document\n","    with open('rag_sample_data/complex_doc.txt', 'w', encoding='utf-8') as f:\n","        f.write(complex_content)\n","\n","    # Test the comprehensive loader\n","    loader = ComprehensiveDocumentLoader('rag_sample_data/complex_doc.txt')\n","    content, metadata = loader.load()\n","\n","    print(\"Comprehensive Document Analysis:\")\n","    print(\"\\n1. Basic Metadata:\")\n","    print(json.dumps({k:v for k,v in metadata.items()\n","                     if k not in ['document_structure', 'content_analysis', 'language_analysis']},\n","                     indent=2))\n","\n","    print(\"\\n2. Document Structure:\")\n","    print(json.dumps(metadata.get('document_structure', {}), indent=2))\n","\n","    print(\"\\n3. Content Analysis:\")\n","    print(json.dumps(metadata.get('content_analysis', {}), indent=2))\n","\n","    print(\"\\n4. Language Analysis:\")\n","    print(json.dumps(metadata.get('language_analysis', {}), indent=2))\n","\n","# Run the comprehensive test\n","test_comprehensive_loader()"],"metadata":{"id":"_Nf_RltDU3FD"},"execution_count":null,"outputs":[]}]}