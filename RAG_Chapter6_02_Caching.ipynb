{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGMqF6Yex2MMDZUMPtWb/1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"hSGQSAAFKdpd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFmN0rKHKVQu"},"outputs":[],"source":["!pip install -q langchain\n","!pip install -q langchain_community\n","!pip install -q langchain-openai\n","!pip install -q sentence-transformers\n","!pip install -q numpy\n","!pip install -q python-dotenv\n","\n","# Print versions for debugging\n","print(\"Installed package versions:\")\n","!pip freeze | grep langchain"]},{"cell_type":"markdown","source":["**Import required libraries:**"],"metadata":{"id":"4WxxSYE7K1Vf"}},{"cell_type":"code","source":["import os\n","import time\n","import hashlib\n","import pickle\n","import numpy as np\n","from typing import Dict, Optional, List\n","from getpass import getpass\n","from langchain_openai import OpenAIEmbeddings"],"metadata":{"id":"IkH54wUtK4mL","executionInfo":{"status":"ok","timestamp":1740241477395,"user_tz":-330,"elapsed":4,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["**Initialize Base Configuration**"],"metadata":{"id":"fEadQRF0LNvh"}},{"cell_type":"code","source":["# Securely input your OpenAI API key\n","openai_api_key = getpass('Enter your OpenAI API key: ')\n","os.environ['OPENAI_API_KEY'] = openai_api_key\n","\n","# Initialize base embeddings model\n","base_embedder = OpenAIEmbeddings(\n","    model=\"text-embedding-3-large\",\n","    openai_api_key=openai_api_key\n",")\n","\n","# Create test data that we'll use across all examples\n","test_texts = [\n","    \"Machine learning is fascinating\",\n","    \"AI is transforming industries\",\n","    \"Machine learning is fascinating\",  # Repeated text\n","    \"Python is great for AI\",\n","    \"AI is transforming industries\"     # Repeated text\n","]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgvOr0RYLSjY","executionInfo":{"status":"ok","timestamp":1740241501844,"user_tz":-330,"elapsed":7837,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"f8b51422-ea4c-42ed-a8f5-18a2c86898f0"},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}]},{"cell_type":"markdown","source":["**Simple In-Memory Cache**"],"metadata":{"id":"xzIEQAsjLdlN"}},{"cell_type":"code","source":["class SimpleEmbeddingCache:\n","    def __init__(self, embedding_model):\n","        self.cache = {}\n","        self.model = embedding_model\n","        self.hits = 0\n","        self.misses = 0\n","\n","    def get_embedding(self, text: str) -> List[float]:\n","        \"\"\"Get embedding from cache or compute it.\"\"\"\n","        if text in self.cache:\n","            self.hits += 1\n","            return self.cache[text]\n","\n","        self.misses += 1\n","        embedding = self.model.embed_query(text)\n","        self.cache[text] = embedding\n","        return embedding\n","\n","    def get_stats(self):\n","        \"\"\"Return cache statistics.\"\"\"\n","        total = self.hits + self.misses\n","        hit_rate = (self.hits / total * 100) if total > 0 else 0\n","        return {\n","            'hits': self.hits,\n","            'misses': self.misses,\n","            'hit_rate': f\"{hit_rate:.2f}%\"\n","        }\n","\n","\"\"\"Let's test the simple cache:\"\"\"\n","\n","# Initialize simple cache\n","simple_cache = SimpleEmbeddingCache(base_embedder)\n","\n","print(\"Testing Simple Cache Implementation...\\n\")\n","start_time = time.time()\n","\n","for text in test_texts:\n","    embedding = simple_cache.get_embedding(text)\n","    print(f\"Processed: '{text}'\")\n","\n","end_time = time.time()\n","\n","# Print statistics\n","stats = simple_cache.get_stats()\n","print(f\"\\nSimple Cache Statistics:\")\n","print(f\"Cache Hits: {stats['hits']}\")\n","print(f\"Cache Misses: {stats['misses']}\")\n","print(f\"Hit Rate: {stats['hit_rate']}\")\n","print(f\"Processing Time: {end_time - start_time:.2f} seconds\")"],"metadata":{"id":"qwQi8dLuLf4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**File-Based Cache Implementation**"],"metadata":{"id":"pp6YmW_aL92x"}},{"cell_type":"code","source":["class FileCacheEmbeddings:\n","    def __init__(self, embedding_model, cache_dir=\"./embedding_cache\"):\n","        self.model = embedding_model\n","        self.cache_dir = cache_dir\n","        self.stats = {'hits': 0, 'misses': 0}\n","        os.makedirs(cache_dir, exist_ok=True)\n","\n","    def _get_cache_path(self, text: str) -> str:\n","        \"\"\"Generate cache file path for the text.\"\"\"\n","        text_hash = hashlib.sha256(text.encode()).hexdigest()\n","        return os.path.join(self.cache_dir, f\"{text_hash}.pkl\")\n","\n","    def get_embedding(self, text: str) -> List[float]:\n","        \"\"\"Get embedding from cache or compute it.\"\"\"\n","        cache_path = self._get_cache_path(text)\n","\n","        # Try to load from cache\n","        if os.path.exists(cache_path):\n","            with open(cache_path, 'rb') as f:\n","                self.stats['hits'] += 1\n","                return pickle.load(f)\n","\n","        # Compute new embedding\n","        self.stats['misses'] += 1\n","        embedding = self.model.embed_query(text)\n","\n","        # Save to cache\n","        with open(cache_path, 'wb') as f:\n","            pickle.dump(embedding, f)\n","\n","        return embedding\n","\n","    def get_stats(self):\n","        \"\"\"Return cache statistics.\"\"\"\n","        total = self.stats['hits'] + self.stats['misses']\n","        hit_rate = (self.stats['hits'] / total * 100) if total > 0 else 0\n","        return {\n","            'hits': self.stats['hits'],\n","            'misses': self.stats['misses'],\n","            'hit_rate': f\"{hit_rate:.2f}%\"\n","        }\n","\n","\"\"\"Test the file-based cache:\"\"\"\n","\n","# Initialize file-based cache\n","file_cache = FileCacheEmbeddings(base_embedder)\n","\n","print(\"Testing File-Based Cache Implementation...\\n\")\n","start_time = time.time()\n","\n","# First round of processing\n","print(\"First round of processing:\")\n","for text in test_texts:\n","    embedding = file_cache.get_embedding(text)\n","    print(f\"Processed: '{text}'\")\n","\n","print(\"\\nStats after first round:\")\n","print(file_cache.get_stats())\n","\n","# Second round to test cache hits\n","print(\"\\nSecond round of processing:\")\n","for text in test_texts:\n","    embedding = file_cache.get_embedding(text)\n","    print(f\"Processed: '{text}'\")\n","\n","end_time = time.time()\n","\n","print(\"\\nFinal Statistics:\")\n","stats = file_cache.get_stats()\n","for key, value in stats.items():\n","    print(f\"{key.replace('_', ' ').title()}: {value}\")\n","print(f\"Total Processing Time: {end_time - start_time:.2f} seconds\")"],"metadata":{"id":"32ZEoUWnLwNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Production-Ready Cache Implementation**"],"metadata":{"id":"X--MreABMIN6"}},{"cell_type":"code","source":["class ProductionEmbeddingCache:\n","    def __init__(self,\n","                 embedding_model,\n","                 ttl_seconds: int = 86400,  # 24 hours\n","                 namespace: str = \"default\"):\n","        self.model = embedding_model\n","        self.ttl_seconds = ttl_seconds\n","        self.namespace = namespace\n","        self.cache: Dict[str, dict] = {}\n","        self.stats = {'hits': 0, 'misses': 0}\n","\n","    def _generate_key(self, text: str) -> str:\n","        \"\"\"Generate a consistent hash key for the text.\"\"\"\n","        normalized_text = text.lower().strip()\n","        return hashlib.sha256(\n","            f\"{self.namespace}:{normalized_text}\".encode()\n","        ).hexdigest()\n","\n","    def _is_cache_valid(self, cache_entry: dict) -> bool:\n","        \"\"\"Check if cache entry is still valid based on TTL.\"\"\"\n","        return time.time() - cache_entry['timestamp'] < self.ttl_seconds\n","\n","    def get_embedding(self, text: str) -> Optional[List[float]]:\n","        \"\"\"Get embedding from cache or compute and cache it.\"\"\"\n","        cache_key = self._generate_key(text)\n","\n","        # Check cache\n","        if cache_key in self.cache:\n","            entry = self.cache[cache_key]\n","            if self._is_cache_valid(entry):\n","                self.stats['hits'] += 1\n","                return entry['embedding']\n","            else:\n","                # Remove expired entry\n","                del self.cache[cache_key]\n","\n","        # Compute new embedding\n","        self.stats['misses'] += 1\n","        try:\n","            embedding = self.model.embed_query(text)\n","\n","            # Cache the result\n","            self.cache[cache_key] = {\n","                'embedding': embedding,\n","                'timestamp': time.time()\n","            }\n","\n","            return embedding\n","        except Exception as e:\n","            print(f\"Error generating embedding: {str(e)}\")\n","            return None\n","\n","    def get_stats(self):\n","        \"\"\"Return cache statistics.\"\"\"\n","        total = self.stats['hits'] + self.stats['misses']\n","        hit_rate = (self.stats['hits'] / total * 100) if total > 0 else 0\n","        return {\n","            'hits': self.stats['hits'],\n","            'misses': self.stats['misses'],\n","            'hit_rate': f\"{hit_rate:.2f}%\",\n","            'cache_size': len(self.cache)\n","        }\n","\n","\"\"\"Test the production cache:\"\"\"\n","\n","# Initialize production cache\n","prod_cache = ProductionEmbeddingCache(\n","    base_embedder,\n","    ttl_seconds=3600,  # 1 hour TTL\n","    namespace=\"production\"\n",")\n","\n","print(\"Testing Production Cache Implementation...\\n\")\n","start_time = time.time()\n","\n","# First round of processing\n","print(\"First round of processing:\")\n","for text in test_texts:\n","    embedding = prod_cache.get_embedding(text)\n","    print(f\"Processed: '{text}'\")\n","\n","print(\"\\nStats after first round:\")\n","print(prod_cache.get_stats())\n","\n","# Wait a bit to demonstrate TTL\n","time.sleep(2)  # Simulate time passing\n","\n","print(\"\\nSecond round of processing:\")\n","for text in test_texts:\n","    embedding = prod_cache.get_embedding(text)\n","    print(f\"Processed: '{text}'\")\n","\n","end_time = time.time()\n","\n","print(\"\\nFinal Statistics:\")\n","stats = prod_cache.get_stats()\n","for key, value in stats.items():\n","    print(f\"{key.replace('_', ' ').title()}: {value}\")\n","print(f\"Total Processing Time: {end_time - start_time:.2f} seconds\")"],"metadata":{"id":"eQyW389qLsRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Performance Comparison**"],"metadata":{"id":"gNeUktgyMTad"}},{"cell_type":"code","source":["def run_performance_test(cache_name, cache_impl, test_data):\n","    start_time = time.time()\n","\n","    # Process all texts\n","    for text in test_data:\n","        _ = cache_impl.get_embedding(text)\n","\n","    end_time = time.time()\n","    processing_time = end_time - start_time\n","\n","    # Get stats if available\n","    stats = cache_impl.get_stats()\n","\n","    return {\n","        'name': cache_name,\n","        'processing_time': processing_time,\n","        'stats': stats\n","    }\n","\n","# Generate larger test dataset\n","extended_test_data = []\n","base_texts = [\n","    \"Machine learning is fascinating\",\n","    \"AI is transforming industries\",\n","    \"Deep learning revolutionizes AI\",\n","    \"Natural language processing advances\"\n","]\n","\n","# Add some repetition to test caching\n","for _ in range(3):\n","    extended_test_data.extend(base_texts)\n","\n","# Initialize fresh instances for fair comparison\n","fresh_simple_cache = SimpleEmbeddingCache(base_embedder)\n","fresh_file_cache = FileCacheEmbeddings(base_embedder)\n","fresh_prod_cache = ProductionEmbeddingCache(base_embedder)\n","\n","# Run performance tests\n","print(\"Running Performance Comparison...\\n\")\n","\n","implementations = [\n","    ('Simple Cache', fresh_simple_cache),\n","    ('File Cache', fresh_file_cache),\n","    ('Production Cache', fresh_prod_cache)\n","]\n","\n","results = []\n","for name, impl in implementations:\n","    print(f\"\\nTesting {name}...\")\n","    result = run_performance_test(name, impl, extended_test_data)\n","    results.append(result)\n","    print(f\"Processing Time: {result['processing_time']:.2f} seconds\")\n","    print(\"Statistics:\", result['stats'])"],"metadata":{"id":"99FLqBlEMT1B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cleanup**"],"metadata":{"id":"RR2YKRWaMi5E"}},{"cell_type":"code","source":["# Clean up cache directory\n","import shutil\n","if os.path.exists(\"./embedding_cache\"):\n","    shutil.rmtree(\"./embedding_cache\")\n","print(\"Cleaned up cache directory\")"],"metadata":{"id":"LZ9OKUV3MlDU"},"execution_count":null,"outputs":[]}]}