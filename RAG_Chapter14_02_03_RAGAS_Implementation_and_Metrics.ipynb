{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6OB1Qi9xTGpFRH3Q2JL9+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**# SETUP AND INSTALLATION**"],"metadata":{"id":"QgI2CgB-vGzo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zEYUvIIBvF-m"},"outputs":[],"source":["# Install required packages\n","!pip install ragas langchain openai sentence-transformers datasets\n","!pip install nltk rouge-score sacrebleu\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","import json\n","from typing import List, Dict, Any\n","import os\n","\n","# RAGAS imports\n","from ragas import evaluate\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    context_precision,\n","    context_recall,\n","    answer_correctness,\n","    answer_similarity\n",")\n","\n","# LangChain imports\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.llms import OpenAI\n","from langchain.chat_models import ChatOpenAI\n","\n","# Traditional metrics\n","import nltk\n","from rouge_score import rouge_scorer\n","from sacrebleu import BLEU\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Download NLTK data\n","nltk.download('punkt', quiet=True)\n","\n","# Set your OpenAI API key\n","os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n","\n","# Initialize models for RAGAS\n","embeddings = OpenAIEmbeddings()\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","print(\"‚úÖ Setup complete! Ready for RAGAS evaluation.\")"]},{"cell_type":"markdown","source":["**# 14.2.2 ESSENTIAL RAGAS METRICS - SAMPLE DATA**"],"metadata":{"id":"KwiB2Ab0wXk8"}},{"cell_type":"code","source":["# Create sample RAG evaluation dataset\n","sample_data = [\n","    {\n","        \"question\": \"What is the capital of France?\",\n","        \"contexts\": [\n","            \"Paris is the capital and most populous city of France. It is located in northern France.\",\n","            \"France is a country in Western Europe with several major cities including Lyon and Marseille.\"\n","        ],\n","        \"answer\": \"The capital of France is Paris, which is also its most populous city.\",\n","        \"ground_truth\": \"Paris is the capital of France.\"\n","    },\n","    {\n","        \"question\": \"How does photosynthesis work?\",\n","        \"contexts\": [\n","            \"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen.\",\n","            \"Chlorophyll in plant leaves captures light energy to drive the photosynthetic process.\",\n","            \"The chemical equation for photosynthesis is: 6CO2 + 6H2O + light energy ‚Üí C6H12O6 + 6O2\"\n","        ],\n","        \"answer\": \"Photosynthesis is how plants make food using sunlight, CO2, and water to produce glucose and oxygen.\",\n","        \"ground_truth\": \"Photosynthesis converts light energy, carbon dioxide and water into glucose and oxygen.\"\n","    },\n","    {\n","        \"question\": \"What causes earthquakes?\",\n","        \"contexts\": [\n","            \"Earthquakes are caused by the sudden release of energy stored in rocks beneath Earth's surface.\",\n","            \"Tectonic plates moving against each other create stress that builds up over time.\",\n","            \"When the stress exceeds the strength of rocks, they break suddenly, releasing energy as seismic waves.\"\n","        ],\n","        \"answer\": \"Earthquakes happen when tectonic plates move and create stress underground. When rocks can't handle the stress anymore, they break and release energy as seismic waves.\",\n","        \"ground_truth\": \"Earthquakes are caused by tectonic plate movement and sudden rock fracture releasing seismic energy.\"\n","    },\n","    {\n","        \"question\": \"What is machine learning?\",\n","        \"contexts\": [\n","            \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n","            \"Popular machine learning algorithms include neural networks, decision trees, and support vector machines.\",\n","            \"Deep learning is a specialized form of machine learning using neural networks with multiple layers.\"\n","        ],\n","        \"answer\": \"Machine learning is part of AI where computers learn patterns from data automatically, using algorithms like neural networks to make predictions without explicit programming.\",\n","        \"ground_truth\": \"Machine learning is a branch of AI that allows computers to learn patterns from data.\"\n","    },\n","    {\n","        \"question\": \"How do vaccines work?\",\n","        \"contexts\": [\n","            \"Vaccines contain weakened or inactive parts of a pathogen that trigger an immune response.\",\n","            \"The immune system creates antibodies and memory cells when exposed to vaccine antigens.\",\n","            \"If the real pathogen is encountered later, memory cells quickly produce antibodies for protection.\"\n","        ],\n","        \"answer\": \"Vaccines work by training your immune system with safe versions of germs, so it can recognize and fight the real disease later.\",\n","        \"ground_truth\": \"Vaccines stimulate immune system to create antibodies and memory for future pathogen protection.\"\n","    }\n","]\n","\n","# Convert to RAGAS dataset format\n","ragas_dataset = Dataset.from_dict({\n","    \"question\": [item[\"question\"] for item in sample_data],\n","    \"contexts\": [item[\"contexts\"] for item in sample_data],\n","    \"answer\": [item[\"answer\"] for item in sample_data],\n","    \"ground_truth\": [item[\"ground_truth\"] for item in sample_data]\n","})\n","\n","print(\"üìä Sample RAG evaluation dataset created!\")\n","print(f\"Dataset size: {len(sample_data)} examples\")\n"],"metadata":{"id":"i5q15FtFwXw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# 14.2.2 ESSENTIAL RAGAS METRICS - IMPLEMENTATION**"],"metadata":{"id":"Pz903oFiwti0"}},{"cell_type":"code","source":["class RAGASEvaluator:\n","    \"\"\"Complete RAGAS evaluation implementation with detailed explanations.\"\"\"\n","\n","    def __init__(self, llm=None, embeddings=None):\n","        self.llm = llm or ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","        self.embeddings = embeddings or OpenAIEmbeddings()\n","\n","    def evaluate_faithfulness(self, dataset):\n","        \"\"\"\n","        Faithfulness measures how factually accurate the answer is based on the given context.\n","        Score range: 0.0 to 1.0 (higher is better)\n","\n","        How it works:\n","        1. LLM identifies claims in the generated answer\n","        2. LLM verifies each claim against the provided context\n","        3. Score = (verified claims) / (total claims)\n","        \"\"\"\n","        print(\"üîç Evaluating Faithfulness...\")\n","        print(\"This measures if the answer is factually grounded in the provided context.\")\n","\n","        result = evaluate(\n","            dataset,\n","            metrics=[faithfulness],\n","            llm=self.llm,\n","            embeddings=self.embeddings\n","        )\n","\n","        score = result['faithfulness']\n","        print(f\"‚úÖ Faithfulness Score: {score:.3f}\")\n","\n","        # Interpretation guide\n","        if score >= 0.9:\n","            print(\"üåü Excellent: Answers are highly faithful to context\")\n","        elif score >= 0.7:\n","            print(\"üëç Good: Most answers are grounded in context\")\n","        elif score >= 0.5:\n","            print(\"‚ö†Ô∏è  Moderate: Some hallucination issues detected\")\n","        else:\n","            print(\"üö® Poor: Significant hallucination problems\")\n","\n","        return score\n","\n","    def evaluate_answer_relevancy(self, dataset):\n","        \"\"\"\n","        Answer Relevancy measures how well the answer addresses the specific question.\n","        Score range: 0.0 to 1.0 (higher is better)\n","\n","        How it works:\n","        1. LLM generates potential questions that the answer could address\n","        2. Computes similarity between original question and generated questions\n","        3. Higher similarity indicates better relevancy\n","        \"\"\"\n","        print(\"\\nüéØ Evaluating Answer Relevancy...\")\n","        print(\"This measures how well answers address the specific questions asked.\")\n","\n","        result = evaluate(\n","            dataset,\n","            metrics=[answer_relevancy],\n","            llm=self.llm,\n","            embeddings=self.embeddings\n","        )\n","\n","        score = result['answer_relevancy']\n","        print(f\"‚úÖ Answer Relevancy Score: {score:.3f}\")\n","\n","        # Interpretation guide\n","        if score >= 0.9:\n","            print(\"üåü Excellent: Answers directly address questions\")\n","        elif score >= 0.7:\n","            print(\"üëç Good: Answers are mostly relevant\")\n","        elif score >= 0.5:\n","            print(\"‚ö†Ô∏è  Moderate: Some answers drift from the question\")\n","        else:\n","            print(\"üö® Poor: Answers frequently miss the point\")\n","\n","        return score\n","\n","    def evaluate_context_precision(self, dataset):\n","        \"\"\"\n","        Context Precision measures the signal-to-noise ratio in retrieved contexts.\n","        Score range: 0.0 to 1.0 (higher is better)\n","\n","        How it works:\n","        1. LLM determines which contexts are relevant to answering the question\n","        2. Evaluates if relevant contexts are ranked higher than irrelevant ones\n","        3. Higher precision means better retrieval ranking\n","        \"\"\"\n","        print(\"\\nüìã Evaluating Context Precision...\")\n","        print(\"This measures the quality of retrieved context ranking.\")\n","\n","        result = evaluate(\n","            dataset,\n","            metrics=[context_precision],\n","            llm=self.llm,\n","            embeddings=self.embeddings\n","        )\n","\n","        score = result['context_precision']\n","        print(f\"‚úÖ Context Precision Score: {score:.3f}\")\n","\n","        # Interpretation guide\n","        if score >= 0.9:\n","            print(\"üåü Excellent: Highly relevant contexts ranked first\")\n","        elif score >= 0.7:\n","            print(\"üëç Good: Mostly relevant contexts in top positions\")\n","        elif score >= 0.5:\n","            print(\"‚ö†Ô∏è  Moderate: Mixed relevant/irrelevant contexts\")\n","        else:\n","            print(\"üö® Poor: Retrieval ranking needs improvement\")\n","\n","        return score\n","\n","    def evaluate_context_recall(self, dataset):\n","        \"\"\"\n","        Context Recall measures completeness of retrieved context.\n","        Score range: 0.0 to 1.0 (higher is better)\n","\n","        How it works:\n","        1. LLM identifies information needed to answer the question (from ground truth)\n","        2. Checks if this information is present in retrieved contexts\n","        3. Score = (relevant info retrieved) / (total relevant info needed)\n","        \"\"\"\n","        print(\"\\nüîÑ Evaluating Context Recall...\")\n","        print(\"This measures how completely the retrieval captures relevant information.\")\n","\n","        result = evaluate(\n","            dataset,\n","            metrics=[context_recall],\n","            llm=self.llm,\n","            embeddings=self.embeddings\n","        )\n","\n","        score = result['context_recall']\n","        print(f\"‚úÖ Context Recall Score: {score:.3f}\")\n","\n","        # Interpretation guide\n","        if score >= 0.9:\n","            print(\"üåü Excellent: Retrieval captures nearly all relevant info\")\n","        elif score >= 0.7:\n","            print(\"üëç Good: Most relevant information is retrieved\")\n","        elif score >= 0.5:\n","            print(\"‚ö†Ô∏è  Moderate: Some relevant information is missing\")\n","        else:\n","            print(\"üö® Poor: Significant gaps in retrieved information\")\n","\n","        return score\n","\n","    def comprehensive_evaluation(self, dataset):\n","        \"\"\"Run all RAGAS metrics and provide comprehensive analysis.\"\"\"\n","        print(\"üöÄ Running Comprehensive RAGAS Evaluation\")\n","        print(\"=\" * 60)\n","\n","        # Run all metrics together (more efficient)\n","        result = evaluate(\n","            dataset,\n","            metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n","            llm=self.llm,\n","            embeddings=self.embeddings\n","        )\n","\n","        # Extract scores\n","        scores = {\n","            'faithfulness': result['faithfulness'],\n","            'answer_relevancy': result['answer_relevancy'],\n","            'context_precision': result['context_precision'],\n","            'context_recall': result['context_recall']\n","        }\n","\n","        # Extract scores (RAGAS returns lists, so we take the mean)\n","        faithfulness_score = np.mean(result['faithfulness']) if isinstance(result['faithfulness'], list) else result['faithfulness']\n","        relevancy_score = np.mean(result['answer_relevancy']) if isinstance(result['answer_relevancy'], list) else result['answer_relevancy']\n","        precision_score = np.mean(result['context_precision']) if isinstance(result['context_precision'], list) else result['context_precision']\n","        recall_score = np.mean(result['context_recall']) if isinstance(result['context_recall'], list) else result['context_recall']\n","\n","        # Calculate overall RAG score (weighted average)\n","        overall_score = (\n","            faithfulness_score * 0.3 +\n","            relevancy_score * 0.3 +\n","            precision_score * 0.2 +\n","            recall_score * 0.2\n","        )\n","\n","        # Update scores dictionary with numeric values\n","        scores = {\n","            'faithfulness': faithfulness_score,\n","            'answer_relevancy': relevancy_score,\n","            'context_precision': precision_score,\n","            'context_recall': recall_score\n","        }\n","\n","        # Display results\n","        print(\"\\nüìä RAGAS Evaluation Results:\")\n","        print(\"-\" * 40)\n","        for metric, score in scores.items():\n","            print(f\"{metric:20}: {score:.3f}\")\n","        print(\"-\" * 40)\n","        print(f\"{'Overall RAG Score':20}: {overall_score:.3f}\")\n","\n","        # Detailed analysis\n","        print(\"\\nüîç Detailed Analysis:\")\n","        self._analyze_scores(scores)\n","\n","        return scores, overall_score\n","\n","    def _analyze_scores(self, scores):\n","        \"\"\"Provide detailed analysis of RAGAS scores.\"\"\"\n","\n","        # Identify strengths and weaknesses\n","        strengths = []\n","        weaknesses = []\n","\n","        for metric, score in scores.items():\n","            if score >= 0.8:\n","                strengths.append(f\"{metric} ({score:.3f})\")\n","            elif score < 0.6:\n","                weaknesses.append(f\"{metric} ({score:.3f})\")\n","\n","        if strengths:\n","            print(f\"‚úÖ Strengths: {', '.join(strengths)}\")\n","        if weaknesses:\n","            print(f\"‚ö†Ô∏è  Areas for improvement: {', '.join(weaknesses)}\")\n","\n","        # Specific recommendations\n","        print(\"\\nüí° Recommendations:\")\n","\n","        if scores['faithfulness'] < 0.7:\n","            print(\"- Address hallucination: Improve prompt engineering or add fact-checking\")\n","\n","        if scores['answer_relevancy'] < 0.7:\n","            print(\"- Improve relevancy: Better query understanding or response filtering\")\n","\n","        if scores['context_precision'] < 0.7:\n","            print(\"- Enhance retrieval: Improve ranking algorithm or embedding quality\")\n","\n","        if scores['context_recall'] < 0.7:\n","            print(\"- Increase coverage: Expand knowledge base or improve search recall\")\n","\n","# Run RAGAS evaluation\n","evaluator = RAGASEvaluator()\n","scores, overall_score = evaluator.comprehensive_evaluation(ragas_dataset)\n","\n","# ================================================================\n","# 14.2.3 CUSTOM METRICS IMPLEMENTATION\n","# ================================================================\n","\n","class CustomRAGASMetrics:\n","    \"\"\"Implementation of custom domain-specific RAGAS metrics.\"\"\"\n","\n","    def __init__(self, llm=None):\n","        self.llm = llm or ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","    def create_domain_specific_metric(self, domain=\"medical\"):\n","        \"\"\"\n","        Create a custom metric for domain-specific evaluation.\n","        This example shows medical domain evaluation.\n","        \"\"\"\n","        print(f\"\\nüè• Creating Custom {domain.title()} Domain Metric\")\n","\n","        if domain == \"medical\":\n","            return self._create_medical_accuracy_metric()\n","        elif domain == \"legal\":\n","            return self._create_legal_precision_metric()\n","        elif domain == \"financial\":\n","            return self._create_financial_compliance_metric()\n","        else:\n","            return self._create_generic_domain_metric(domain)\n","\n","    def _create_medical_accuracy_metric(self):\n","        \"\"\"Custom metric for medical information accuracy.\"\"\"\n","\n","        medical_evaluation_prompt = \"\"\"\n","        You are a medical expert evaluating the accuracy and safety of health information.\n","\n","        Given:\n","        - Question: {question}\n","        - Context: {context}\n","        - Answer: {answer}\n","\n","        Evaluate the answer on these medical criteria:\n","        1. Medical accuracy: Are the facts correct?\n","        2. Safety: Could this information harm someone if followed?\n","        3. Appropriate scope: Does it avoid diagnosing or prescribing?\n","        4. Disclaimers: Does it appropriately direct to healthcare professionals?\n","\n","        Provide a score from 0.0 to 1.0 and brief explanation.\n","        Format: Score: X.X | Explanation: [brief explanation]\n","        \"\"\"\n","\n","        def medical_accuracy(row):\n","            prompt = medical_evaluation_prompt.format(\n","                question=row['question'],\n","                context='\\n'.join(row['contexts']),\n","                answer=row['answer']\n","            )\n","\n","            response = self.llm.predict(prompt)\n","\n","            # Parse score (simplified - in production, use more robust parsing)\n","            try:\n","                score_part = response.split('Score:')[1].split('|')[0].strip()\n","                score = float(score_part)\n","                return score\n","            except:\n","                return 0.5  # Default if parsing fails\n","\n","        return medical_accuracy\n","\n","    def _create_legal_precision_metric(self):\n","        \"\"\"Custom metric for legal information precision.\"\"\"\n","\n","        legal_evaluation_prompt = \"\"\"\n","        You are a legal expert evaluating legal information quality.\n","\n","        Given:\n","        - Question: {question}\n","        - Context: {context}\n","        - Answer: {answer}\n","\n","        Evaluate on:\n","        1. Legal accuracy: Are legal concepts correctly explained?\n","        2. Jurisdiction awareness: Does it acknowledge legal variations?\n","        3. Disclaimers: Does it appropriately advise consulting lawyers?\n","        4. Clarity: Is complex legal language made accessible?\n","\n","        Score from 0.0 to 1.0.\n","        Format: Score: X.X\n","        \"\"\"\n","\n","        def legal_precision(row):\n","            prompt = legal_evaluation_prompt.format(\n","                question=row['question'],\n","                context='\\n'.join(row['contexts']),\n","                answer=row['answer']\n","            )\n","\n","            response = self.llm.predict(prompt)\n","\n","            try:\n","                score = float(response.split('Score:')[1].strip())\n","                return score\n","            except:\n","                return 0.5\n","\n","        return legal_precision\n","\n","    def evaluate_with_custom_metrics(self, dataset, custom_metrics):\n","        \"\"\"Evaluate dataset using custom metrics.\"\"\"\n","\n","        results = {}\n","\n","        for metric_name, metric_func in custom_metrics.items():\n","            print(f\"\\nüîß Evaluating custom metric: {metric_name}\")\n","\n","            scores = []\n","            for i in range(len(dataset)):\n","                row = {\n","                    'question': dataset[i]['question'],\n","                    'contexts': dataset[i]['contexts'],\n","                    'answer': dataset[i]['answer'],\n","                    'ground_truth': dataset[i]['ground_truth']\n","                }\n","                score = metric_func(row)\n","                scores.append(score)\n","\n","            avg_score = np.mean(scores)\n","            results[metric_name] = {\n","                'scores': scores,\n","                'average': avg_score\n","            }\n","\n","            print(f\"‚úÖ {metric_name}: {avg_score:.3f}\")\n","\n","        return results\n","\n","# Demonstrate custom metrics\n","print(\"\\n\" + \"=\"*60)\n","print(\"üîß CUSTOM METRICS DEMONSTRATION\")\n","print(\"=\"*60)\n","\n","custom_evaluator = CustomRAGASMetrics()\n","\n","# Create domain-specific metrics\n","medical_metric = custom_evaluator.create_domain_specific_metric(\"medical\")\n","legal_metric = custom_evaluator.create_domain_specific_metric(\"legal\")\n","\n","# Example evaluation (commented out due to API costs)\n","# custom_metrics = {\n","#     'medical_accuracy': medical_metric,\n","#     'legal_precision': legal_metric\n","# }\n","# custom_results = custom_evaluator.evaluate_with_custom_metrics(ragas_dataset, custom_metrics)\n","\n","print(\"üí° Custom metrics created! Uncomment evaluation code to run with your API key.\")\n"],"metadata":{"id":"4FpYD5fuwtrr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# 14.3.2 TRADITIONAL GENERATION QUALITY METRICS**"],"metadata":{"id":"Km1XY30zyS2l"}},{"cell_type":"code","source":["class TraditionalMetrics:\n","    \"\"\"Implementation of traditional text generation quality metrics.\"\"\"\n","\n","    def __init__(self):\n","        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","    def calculate_bleu_score(self, predictions, references):\n","        \"\"\"Calculate BLEU scores for generated text.\"\"\"\n","        print(\"\\nüìä Calculating BLEU Scores...\")\n","        print(\"BLEU measures n-gram overlap between prediction and reference\")\n","\n","        bleu = BLEU()\n","        scores = []\n","\n","        for pred, ref in zip(predictions, references):\n","            # BLEU expects tokenized input\n","            pred_tokens = pred.split()\n","            ref_tokens = [ref.split()]  # List of reference tokenizations\n","\n","            score = bleu.sentence_score(pred, [ref]).score / 100.0  # Normalize to 0-1\n","            scores.append(score)\n","\n","        avg_bleu = np.mean(scores)\n","        print(f\"‚úÖ Average BLEU Score: {avg_bleu:.3f}\")\n","\n","        # Interpretation\n","        if avg_bleu >= 0.4:\n","            print(\"üåü Excellent lexical similarity\")\n","        elif avg_bleu >= 0.2:\n","            print(\"üëç Good lexical overlap\")\n","        elif avg_bleu >= 0.1:\n","            print(\"‚ö†Ô∏è  Moderate similarity\")\n","        else:\n","            print(\"üö® Low lexical similarity\")\n","\n","        return scores, avg_bleu\n","\n","    def calculate_rouge_scores(self, predictions, references):\n","        \"\"\"Calculate ROUGE scores for generated text.\"\"\"\n","        print(\"\\nüìä Calculating ROUGE Scores...\")\n","        print(\"ROUGE measures recall-oriented overlap with reference summaries\")\n","\n","        rouge1_scores = []\n","        rouge2_scores = []\n","        rougeL_scores = []\n","\n","        for pred, ref in zip(predictions, references):\n","            scores = self.rouge_scorer.score(ref, pred)\n","            rouge1_scores.append(scores['rouge1'].fmeasure)\n","            rouge2_scores.append(scores['rouge2'].fmeasure)\n","            rougeL_scores.append(scores['rougeL'].fmeasure)\n","\n","        avg_rouge1 = np.mean(rouge1_scores)\n","        avg_rouge2 = np.mean(rouge2_scores)\n","        avg_rougeL = np.mean(rougeL_scores)\n","\n","        print(f\"‚úÖ ROUGE-1 (unigram): {avg_rouge1:.3f}\")\n","        print(f\"‚úÖ ROUGE-2 (bigram):  {avg_rouge2:.3f}\")\n","        print(f\"‚úÖ ROUGE-L (longest): {avg_rougeL:.3f}\")\n","\n","        return {\n","            'rouge1': (rouge1_scores, avg_rouge1),\n","            'rouge2': (rouge2_scores, avg_rouge2),\n","            'rougeL': (rougeL_scores, avg_rougeL)\n","        }\n","\n","    def calculate_semantic_similarity(self, predictions, references):\n","        \"\"\"Calculate semantic similarity using sentence embeddings.\"\"\"\n","        print(\"\\nüìä Calculating Semantic Similarity...\")\n","        print(\"Measures semantic closeness using sentence embeddings\")\n","\n","        # Get embeddings\n","        pred_embeddings = self.sentence_model.encode(predictions)\n","        ref_embeddings = self.sentence_model.encode(references)\n","\n","        # Calculate cosine similarities\n","        similarities = []\n","        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):\n","            similarity = cosine_similarity([pred_emb], [ref_emb])[0][0]\n","            similarities.append(similarity)\n","\n","        avg_similarity = np.mean(similarities)\n","        print(f\"‚úÖ Average Semantic Similarity: {avg_similarity:.3f}\")\n","\n","        # Interpretation\n","        if avg_similarity >= 0.8:\n","            print(\"üåü Excellent semantic alignment\")\n","        elif avg_similarity >= 0.6:\n","            print(\"üëç Good semantic similarity\")\n","        elif avg_similarity >= 0.4:\n","            print(\"‚ö†Ô∏è  Moderate semantic overlap\")\n","        else:\n","            print(\"üö® Low semantic similarity\")\n","\n","        return similarities, avg_similarity\n","\n","    def comprehensive_traditional_evaluation(self, predictions, references):\n","        \"\"\"Run all traditional metrics and compare with RAGAS.\"\"\"\n","        print(\"\\nüîÑ COMPREHENSIVE TRADITIONAL METRICS EVALUATION\")\n","        print(\"=\"*60)\n","\n","        # Calculate all metrics\n","        bleu_scores, avg_bleu = self.calculate_bleu_score(predictions, references)\n","        rouge_results = self.calculate_rouge_scores(predictions, references)\n","        sem_scores, avg_sem = self.calculate_semantic_similarity(predictions, references)\n","\n","        # Create comparison summary\n","        print(\"\\nüìã Traditional Metrics Summary:\")\n","        print(\"-\" * 40)\n","        print(f\"{'BLEU':20}: {avg_bleu:.3f}\")\n","        print(f\"{'ROUGE-1':20}: {rouge_results['rouge1'][1]:.3f}\")\n","        print(f\"{'ROUGE-2':20}: {rouge_results['rouge2'][1]:.3f}\")\n","        print(f\"{'ROUGE-L':20}: {rouge_results['rougeL'][1]:.3f}\")\n","        print(f\"{'Semantic Similarity':20}: {avg_sem:.3f}\")\n","\n","        # Analysis\n","        print(\"\\nüîç Traditional vs RAGAS Analysis:\")\n","        print(\"üìå Traditional metrics focus on surface-level similarity\")\n","        print(\"üìå RAGAS metrics focus on semantic correctness and context usage\")\n","        print(\"üìå Use both for comprehensive evaluation:\")\n","        print(\"   - Traditional: Quick quality checks, benchmark comparisons\")\n","        print(\"   - RAGAS: Deep quality assessment, hallucination detection\")\n","\n","        return {\n","            'bleu': (bleu_scores, avg_bleu),\n","            'rouge': rouge_results,\n","            'semantic_similarity': (sem_scores, avg_sem)\n","        }\n","\n","# Run traditional metrics evaluation\n","print(\"\\n\" + \"=\"*60)\n","print(\"üìä TRADITIONAL METRICS EVALUATION\")\n","print(\"=\"*60)\n","\n","traditional_evaluator = TraditionalMetrics()\n","\n","# Extract predictions and references from sample data\n","predictions = [item[\"answer\"] for item in sample_data]\n","references = [item[\"ground_truth\"] for item in sample_data]\n","\n","traditional_results = traditional_evaluator.comprehensive_traditional_evaluation(predictions, references)\n"],"metadata":{"id":"x2uXE1ZsyTk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# COMPARATIVE ANALYSIS**"],"metadata":{"id":"iWNa_bO7ypf2"}},{"cell_type":"code","source":["def compare_evaluation_approaches():\n","    \"\"\"Compare RAGAS vs Traditional metrics with analysis.\"\"\"\n","    print(\"\\nüî¨ COMPARATIVE ANALYSIS: RAGAS vs TRADITIONAL\")\n","    print(\"=\"*60)\n","\n","    comparison_data = {\n","        'Aspect': [\n","            'Evaluation Focus',\n","            'Context Awareness',\n","            'Hallucination Detection',\n","            'Semantic Understanding',\n","            'Reference Requirement',\n","            'Computational Cost',\n","            'Human Alignment',\n","            'RAG-Specific Design'\n","        ],\n","        'RAGAS': [\n","            'End-to-end RAG quality',\n","            'Fully context-aware',\n","            'Excellent detection',\n","            'Deep semantic analysis',\n","            'Optional (reference-free)',\n","            'Higher (LLM calls)',\n","            'High correlation',\n","            'Purpose-built for RAG'\n","        ],\n","        'Traditional': [\n","            'Surface text similarity',\n","            'Context-unaware',\n","            'Cannot detect',\n","            'Limited to n-grams',\n","            'Required',\n","            'Lower (simple metrics)',\n","            'Moderate correlation',\n","            'General-purpose'\n","        ]\n","    }\n","\n","    comparison_df = pd.DataFrame(comparison_data)\n","    print(comparison_df.to_string(index=False))\n","\n","    print(\"\\nüí° Key Insights:\")\n","    print(\"1. RAGAS provides deeper, more meaningful evaluation for RAG systems\")\n","    print(\"2. Traditional metrics remain useful for quick checks and benchmarking\")\n","    print(\"3. Best practice: Use both approaches complementarily\")\n","    print(\"4. RAGAS better correlates with human judgment for RAG tasks\")\n","    print(\"5. Traditional metrics are faster but miss critical RAG-specific issues\")\n","\n","compare_evaluation_approaches()"],"metadata":{"id":"ijfcisiDypqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# PRACTICAL EVALUATION PIPELINE**"],"metadata":{"id":"LGiohNKwyxP7"}},{"cell_type":"code","source":["class ProductionEvaluationPipeline:\n","    \"\"\"Complete evaluation pipeline for production RAG systems.\"\"\"\n","\n","    def __init__(self):\n","        self.ragas_evaluator = RAGASEvaluator()\n","        self.traditional_evaluator = TraditionalMetrics()\n","        self.custom_evaluator = CustomRAGASMetrics()\n","\n","    def evaluate_rag_system(self, dataset, include_traditional=True, custom_metrics=None):\n","        \"\"\"Run complete evaluation pipeline.\"\"\"\n","\n","        print(\"üöÄ PRODUCTION RAG EVALUATION PIPELINE\")\n","        print(\"=\"*60)\n","\n","        results = {}\n","\n","        # 1. RAGAS Evaluation\n","        print(\"\\n1Ô∏è‚É£ Running RAGAS Evaluation...\")\n","        ragas_scores, overall_ragas = self.ragas_evaluator.comprehensive_evaluation(dataset)\n","        results['ragas'] = {'scores': ragas_scores, 'overall': overall_ragas}\n","\n","        # 2. Traditional Metrics (if requested)\n","        if include_traditional:\n","            print(\"\\n2Ô∏è‚É£ Running Traditional Metrics...\")\n","            predictions = [dataset[i]['answer'] for i in range(len(dataset))]\n","            references = [dataset[i]['ground_truth'] for i in range(len(dataset))]\n","            traditional_scores = self.traditional_evaluator.comprehensive_traditional_evaluation(\n","                predictions, references\n","            )\n","            results['traditional'] = traditional_scores\n","\n","        # 3. Custom Metrics (if provided)\n","        if custom_metrics:\n","            print(\"\\n3Ô∏è‚É£ Running Custom Metrics...\")\n","            custom_scores = self.custom_evaluator.evaluate_with_custom_metrics(\n","                dataset, custom_metrics\n","            )\n","            results['custom'] = custom_scores\n","\n","        # 4. Generate Report\n","        self._generate_evaluation_report(results)\n","\n","        return results\n","\n","    def _generate_evaluation_report(self, results):\n","        \"\"\"Generate comprehensive evaluation report.\"\"\"\n","\n","        print(\"\\nüìã EVALUATION REPORT\")\n","        print(\"=\"*50)\n","\n","        # RAGAS Summary\n","        if 'ragas' in results:\n","            print(f\"\\nüéØ RAGAS Overall Score: {results['ragas']['overall']:.3f}\")\n","            print(\"   Component Scores:\")\n","            for metric, score in results['ragas']['scores'].items():\n","                print(f\"   ‚Ä¢ {metric}: {score:.3f}\")\n","\n","        # Traditional Summary\n","        if 'traditional' in results:\n","            print(f\"\\nüìä Traditional Metrics:\")\n","            trad = results['traditional']\n","            print(f\"   ‚Ä¢ BLEU: {trad['bleu'][1]:.3f}\")\n","            print(f\"   ‚Ä¢ ROUGE-L: {trad['rouge']['rougeL'][1]:.3f}\")\n","            print(f\"   ‚Ä¢ Semantic Similarity: {trad['semantic_similarity'][1]:.3f}\")\n","\n","        # Custom Summary\n","        if 'custom' in results:\n","            print(f\"\\nüîß Custom Metrics:\")\n","            for metric_name, metric_data in results['custom'].items():\n","                print(f\"   ‚Ä¢ {metric_name}: {metric_data['average']:.3f}\")\n","\n","        # Recommendations\n","        print(f\"\\nüí° Recommendations:\")\n","        if 'ragas' in results:\n","            ragas_scores = results['ragas']['scores']\n","            if ragas_scores['faithfulness'] < 0.7:\n","                print(\"   ‚Ä¢ Address hallucination issues\")\n","            if ragas_scores['context_precision'] < 0.7:\n","                print(\"   ‚Ä¢ Improve retrieval ranking\")\n","            if ragas_scores['context_recall'] < 0.7:\n","                print(\"   ‚Ä¢ Expand knowledge base coverage\")\n","            if ragas_scores['answer_relevancy'] < 0.7:\n","                print(\"   ‚Ä¢ Enhance query understanding\")\n","\n","# Run complete evaluation pipeline\n","pipeline = ProductionEvaluationPipeline()\n","\n","# Example: Run full evaluation (uncomment to execute with API key)\n","# full_results = pipeline.evaluate_rag_system(\n","#     ragas_dataset,\n","#     include_traditional=True,\n","#     custom_metrics=None  # Add custom metrics here if needed\n","# )\n","\n","print(\"\\nüéâ Notebook Complete!\")\n","print(\"You've learned how to:\")\n","print(\"‚úÖ Implement all four core RAGAS metrics\")\n","print(\"‚úÖ Create custom domain-specific evaluation metrics\")\n","print(\"‚úÖ Apply traditional generation quality metrics\")\n","print(\"‚úÖ Build comprehensive evaluation pipelines\")\n","print(\"‚úÖ Compare different evaluation approaches\")\n"],"metadata":{"id":"JDpgjn0byxZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# ADVANCED EVALUATION TECHNIQUES**"],"metadata":{"id":"zA9ZBzyAy_YG"}},{"cell_type":"code","source":["class AdvancedEvaluationTechniques:\n","    \"\"\"Advanced techniques for sophisticated RAG evaluation.\"\"\"\n","\n","    def __init__(self):\n","        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","    def aspect_based_evaluation(self, dataset, aspects=None):\n","        \"\"\"\n","        Evaluate RAG responses across multiple specific aspects.\n","        Useful for nuanced quality assessment.\n","        \"\"\"\n","        if aspects is None:\n","            aspects = [\n","                \"Accuracy\", \"Completeness\", \"Clarity\", \"Relevance\",\n","                \"Consistency\", \"Timeliness\", \"Neutrality\"\n","            ]\n","\n","        print(f\"\\nüé≠ Aspect-Based Evaluation\")\n","        print(f\"Evaluating across {len(aspects)} aspects: {', '.join(aspects)}\")\n","\n","        aspect_prompt = \"\"\"\n","        Evaluate the following RAG response across these specific aspects:\n","\n","        Question: {question}\n","        Context: {context}\n","        Answer: {answer}\n","\n","        Rate each aspect from 1-5 (5=excellent, 1=poor):\n","        {aspect_list}\n","\n","        Format your response as:\n","        Aspect1: X | Aspect2: Y | ... | Overall: Z\n","        Brief justification: [explain your ratings]\n","        \"\"\"\n","\n","        results = {aspect: [] for aspect in aspects}\n","        results['overall'] = []\n","\n","        for i in range(min(3, len(dataset))):  # Limit for demo\n","            row = dataset[i]\n","\n","            aspect_list = \" | \".join([f\"{aspect}: [1-5]\" for aspect in aspects])\n","\n","            prompt = aspect_prompt.format(\n","                question=row['question'],\n","                context='\\n'.join(row['contexts']),\n","                answer=row['answer'],\n","                aspect_list=aspect_list\n","            )\n","\n","            # In production, you would call the LLM here\n","            # response = self.llm.predict(prompt)\n","\n","            # For demo, simulate scores\n","            import random\n","            random.seed(42 + i)  # Reproducible for demo\n","\n","            for aspect in aspects:\n","                score = random.uniform(3.5, 4.8)  # Simulate good scores\n","                results[aspect].append(score)\n","\n","            overall = random.uniform(3.8, 4.5)\n","            results['overall'].append(overall)\n","\n","        # Calculate averages\n","        avg_results = {aspect: np.mean(scores) for aspect, scores in results.items()}\n","\n","        print(\"\\nüìä Aspect-Based Results:\")\n","        for aspect, avg_score in avg_results.items():\n","            print(f\"   {aspect:12}: {avg_score:.2f}/5.0\")\n","\n","        return avg_results\n","\n","    def confidence_calibrated_evaluation(self, dataset):\n","        \"\"\"\n","        Evaluate both answer quality and model confidence.\n","        Helps identify when the model is uncertain.\n","        \"\"\"\n","        print(f\"\\nüéØ Confidence-Calibrated Evaluation\")\n","        print(\"Assessing both answer quality and model confidence\")\n","\n","        confidence_prompt = \"\"\"\n","        Evaluate this RAG response and provide both quality and confidence assessments:\n","\n","        Question: {question}\n","        Context: {context}\n","        Answer: {answer}\n","\n","        Provide:\n","        1. Quality Score (0.0-1.0): How good is this answer?\n","        2. Confidence Score (0.0-1.0): How confident should we be in this assessment?\n","        3. Uncertainty Factors: What makes this evaluation uncertain?\n","\n","        Format: Quality: X.X | Confidence: Y.Y | Factors: [list main uncertainty sources]\n","        \"\"\"\n","\n","        results = {'quality': [], 'confidence': [], 'factors': []}\n","\n","        # Simulate results for demo (in production, use LLM)\n","        for i in range(min(3, len(dataset))):\n","            import random\n","            random.seed(42 + i)\n","\n","            quality = random.uniform(0.7, 0.95)\n","            confidence = random.uniform(0.6, 0.9)\n","\n","            # Simulate uncertainty factors\n","            factor_options = [\n","                \"Ambiguous context\", \"Multiple valid interpretations\",\n","                \"Limited context\", \"Complex technical topic\", \"Subjective question\"\n","            ]\n","            factors = random.sample(factor_options, random.randint(1, 2))\n","\n","            results['quality'].append(quality)\n","            results['confidence'].append(confidence)\n","            results['factors'].append(factors)\n","\n","        avg_quality = np.mean(results['quality'])\n","        avg_confidence = np.mean(results['confidence'])\n","\n","        print(f\"\\nüìä Confidence-Calibrated Results:\")\n","        print(f\"   Average Quality:    {avg_quality:.3f}\")\n","        print(f\"   Average Confidence: {avg_confidence:.3f}\")\n","        print(f\"   Quality-Confidence Gap: {abs(avg_quality - avg_confidence):.3f}\")\n","\n","        if abs(avg_quality - avg_confidence) > 0.2:\n","            print(\"‚ö†Ô∏è  Large gap suggests calibration issues\")\n","        else:\n","            print(\"‚úÖ Good calibration between quality and confidence\")\n","\n","        return results\n","\n","    def error_categorization(self, dataset):\n","        \"\"\"\n","        Categorize different types of errors in RAG responses.\n","        Helps identify systematic issues.\n","        \"\"\"\n","        print(f\"\\nüîç Error Categorization Analysis\")\n","        print(\"Identifying and categorizing response errors\")\n","\n","        error_categories = {\n","            'Factual Error': 'Answer contains incorrect factual information',\n","            'Hallucination': 'Answer includes information not in context',\n","            'Incomplete': 'Answer misses important information from context',\n","            'Irrelevant': 'Answer does not address the question',\n","            'Inconsistent': 'Answer contradicts itself or the context',\n","            'Unclear': 'Answer is confusing or poorly structured'\n","        }\n","\n","        # Simulate error analysis (in production, use LLM)\n","        error_counts = {category: 0 for category in error_categories}\n","        total_errors = 0\n","\n","        for i in range(len(dataset)):\n","            # Simulate finding 0-2 errors per response\n","            import random\n","            random.seed(42 + i)\n","\n","            num_errors = random.choices([0, 1, 2], weights=[0.7, 0.25, 0.05])[0]\n","\n","            if num_errors > 0:\n","                errors = random.sample(list(error_categories.keys()), num_errors)\n","                for error in errors:\n","                    error_counts[error] += 1\n","                    total_errors += 1\n","\n","        print(f\"\\nüìä Error Analysis Results:\")\n","        print(f\"   Total Responses: {len(dataset)}\")\n","        print(f\"   Total Errors Found: {total_errors}\")\n","        print(f\"   Error Rate: {total_errors/len(dataset):.2f} errors per response\")\n","\n","        print(f\"\\nüè∑Ô∏è  Error Categories:\")\n","        for category, count in error_counts.items():\n","            percentage = (count / total_errors * 100) if total_errors > 0 else 0\n","            print(f\"   {category:15}: {count:2d} ({percentage:4.1f}%)\")\n","\n","        # Identify top issues\n","        if total_errors > 0:\n","            top_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n","            print(f\"\\nüéØ Top Issues to Address:\")\n","            for i, (category, count) in enumerate(top_errors, 1):\n","                if count > 0:\n","                    print(f\"   {i}. {category}: {error_categories[category]}\")\n","\n","        return error_counts\n","\n","# Run advanced evaluation techniques\n","print(\"\\n\" + \"=\"*60)\n","print(\"üî¨ ADVANCED EVALUATION TECHNIQUES\")\n","print(\"=\"*60)\n","\n","advanced_evaluator = AdvancedEvaluationTechniques()\n","\n","# Aspect-based evaluation\n","aspect_results = advanced_evaluator.aspect_based_evaluation(ragas_dataset)\n","\n","# Confidence-calibrated evaluation\n","confidence_results = advanced_evaluator.confidence_calibrated_evaluation(ragas_dataset)\n","\n","# Error categorization\n","error_analysis = advanced_evaluator.error_categorization(ragas_dataset)\n"],"metadata":{"id":"8IwdOPdIy_hU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# EVALUATION BEST PRACTICES AND TIPS**"],"metadata":{"id":"uFuOtbJNzjIv"}},{"cell_type":"code","source":["def evaluation_best_practices():\n","    \"\"\"Comprehensive guide to RAG evaluation best practices.\"\"\"\n","\n","    print(\"\\nüìö RAG EVALUATION BEST PRACTICES\")\n","    print(\"=\"*50)\n","\n","    practices = {\n","        \"üéØ Metric Selection\": [\n","            \"Use RAGAS for comprehensive RAG-specific evaluation\",\n","            \"Include traditional metrics for benchmark comparison\",\n","            \"Add domain-specific metrics for specialized applications\",\n","            \"Balance automated and human evaluation\"\n","        ],\n","\n","        \"üìä Dataset Design\": [\n","            \"Include diverse question types and difficulty levels\",\n","            \"Ensure representative coverage of your domain\",\n","            \"Include edge cases and challenging scenarios\",\n","            \"Maintain balanced positive and negative examples\"\n","        ],\n","\n","        \"üîÑ Evaluation Frequency\": [\n","            \"Continuous evaluation in development cycles\",\n","            \"Regular production monitoring (daily/weekly)\",\n","            \"Deep evaluation before major releases\",\n","            \"A/B testing for system changes\"\n","        ],\n","\n","        \"‚öñÔ∏è Bias Mitigation\": [\n","            \"Use multiple evaluators (human and automated)\",\n","            \"Rotate evaluation datasets regularly\",\n","            \"Monitor for demographic and topical biases\",\n","            \"Validate automated metrics against human judgment\"\n","        ],\n","\n","        \"üöÄ Production Considerations\": [\n","            \"Set up automated alerting for quality drops\",\n","            \"Monitor user feedback and satisfaction\",\n","            \"Track performance across user segments\",\n","            \"Implement gradual rollouts with evaluation gates\"\n","        ]\n","    }\n","\n","    for category, tips in practices.items():\n","        print(f\"\\n{category}\")\n","        for tip in tips:\n","            print(f\"   ‚Ä¢ {tip}\")\n","\n","    print(f\"\\nüí° Key Takeaway:\")\n","    print(\"Effective RAG evaluation is ongoing, multi-faceted, and combines\")\n","    print(\"automated sophisticated metrics with human insight and domain expertise.\")\n","\n","evaluation_best_practices()"],"metadata":{"id":"VERoOTc2zjRu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# EVALUATION CHECKLIST**"],"metadata":{"id":"IwKR-uZQzrKh"}},{"cell_type":"code","source":["def create_evaluation_checklist():\n","    \"\"\"Create a practical checklist for RAG evaluation implementation.\"\"\"\n","\n","    checklist = {\n","        \"Pre-Evaluation Setup\": [\n","            \"‚ñ° Define evaluation objectives and success criteria\",\n","            \"‚ñ° Prepare diverse, representative test dataset\",\n","            \"‚ñ° Set up RAGAS environment and API keys\",\n","            \"‚ñ° Establish baseline scores for comparison\",\n","            \"‚ñ° Define custom metrics for domain-specific needs\"\n","        ],\n","\n","        \"Core Evaluation\": [\n","            \"‚ñ° Run RAGAS faithfulness evaluation\",\n","            \"‚ñ° Assess answer relevancy scores\",\n","            \"‚ñ° Measure context precision and recall\",\n","            \"‚ñ° Calculate traditional metrics (BLEU, ROUGE)\",\n","            \"‚ñ° Perform semantic similarity analysis\"\n","        ],\n","\n","        \"Advanced Analysis\": [\n","            \"‚ñ° Conduct aspect-based evaluation\",\n","            \"‚ñ° Analyze confidence calibration\",\n","            \"‚ñ° Categorize and count error types\",\n","            \"‚ñ° Identify systematic failure patterns\",\n","            \"‚ñ° Validate with human evaluation sample\"\n","        ],\n","\n","        \"Production Readiness\": [\n","            \"‚ñ° Set up automated evaluation pipelines\",\n","            \"‚ñ° Configure monitoring and alerting\",\n","            \"‚ñ° Establish evaluation cadence and triggers\",\n","            \"‚ñ° Document evaluation procedures\",\n","            \"‚ñ° Train team on evaluation interpretation\"\n","        ],\n","\n","        \"Continuous Improvement\": [\n","            \"‚ñ° Regular metric review and updates\",\n","            \"‚ñ° User feedback integration\",\n","            \"‚ñ° Performance trend analysis\",\n","            \"‚ñ° Evaluation methodology refinement\",\n","            \"‚ñ° Benchmark against industry standards\"\n","        ]\n","    }\n","\n","    print(\"\\n‚úÖ RAG EVALUATION CHECKLIST\")\n","    print(\"=\"*40)\n","\n","    for phase, items in checklist.items():\n","        print(f\"\\n{phase}:\")\n","        for item in items:\n","            print(f\"   {item}\")\n","\n","    print(f\"\\nüéØ Use this checklist to ensure comprehensive RAG evaluation!\")\n","\n","create_evaluation_checklist()"],"metadata":{"id":"rPUzP0PxzrR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SUMMARY AND NEXT STEPS**"],"metadata":{"id":"046PBserzyMr"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"üéâ NOTEBOOK COMPLETE - RAGAS IMPLEMENTATION AND CORE METRICS\")\n","print(\"=\"*60)\n","\n","print(\"\\nüìã What You've Accomplished:\")\n","print(\"‚úÖ Mastered RAGAS framework and core philosophy\")\n","print(\"‚úÖ Implemented all four essential RAGAS metrics\")\n","print(\"‚úÖ Created custom domain-specific evaluation metrics\")\n","print(\"‚úÖ Applied traditional generation quality metrics\")\n","print(\"‚úÖ Built comprehensive evaluation pipelines\")\n","print(\"‚úÖ Learned advanced evaluation techniques\")\n","print(\"‚úÖ Established evaluation best practices\")\n","\n","print(\"\\nüöÄ Next Steps:\")\n","print(\"1. Set up your own RAGAS evaluation with your RAG system\")\n","print(\"2. Experiment with custom metrics for your domain\")\n","print(\"3. Establish baseline scores for your application\")\n","print(\"4. Move on to next notebook for production evaluation pipelines\")\n","print(\"5. Integrate evaluation into your development workflow\")\n","\n","print(\"\\nüí° Key Takeaways:\")\n","print(\"‚Ä¢ RAGAS provides sophisticated, automated RAG-specific evaluation\")\n","print(\"‚Ä¢ Combine multiple evaluation approaches for comprehensive assessment\")\n","print(\"‚Ä¢ Custom metrics enable domain-specific quality measurement\")\n","print(\"‚Ä¢ Evaluation should be continuous, not just one-time\")\n","print(\"‚Ä¢ Good evaluation enables iterative improvement and production confidence\")\n","\n","print(\"\\nüìñ Continue to the next notebook for production evaluation and monitoring!\")\n","\n","# ================================================================\n","# UTILITY FUNCTIONS FOR EASY REFERENCE\n","# ================================================================\n","\n","def quick_ragas_evaluation(questions, contexts, answers, ground_truths=None):\n","    \"\"\"\n","    Quick utility function for RAGAS evaluation.\n","    Use this for rapid testing in your own projects.\n","    \"\"\"\n","    from datasets import Dataset\n","\n","    # Prepare dataset\n","    eval_data = {\n","        'question': questions,\n","        'contexts': contexts,\n","        'answer': answers\n","    }\n","\n","    if ground_truths:\n","        eval_data['ground_truth'] = ground_truths\n","\n","    dataset = Dataset.from_dict(eval_data)\n","\n","    # Run evaluation\n","    evaluator = RAGASEvaluator()\n","    scores, overall = evaluator.comprehensive_evaluation(dataset)\n","\n","    return scores, overall\n","\n","def save_evaluation_results(results, filename=\"rag_evaluation_results.json\"):\n","    \"\"\"Save evaluation results to JSON file for later analysis.\"\"\"\n","    import json\n","    with open(filename, 'w') as f:\n","        json.dump(results, f, indent=2)\n","    print(f\"‚úÖ Results saved to {filename}\")\n","\n","# Example usage documentation\n","print(\"\\nüìö Utility Functions Available:\")\n","print(\"‚Ä¢ quick_ragas_evaluation() - Fast RAGAS evaluation\")\n","print(\"‚Ä¢ save_evaluation_results() - Save results to file\")\n","print(\"‚Ä¢ All class methods can be used independently\")\n","\n","print(\"\\nüîß Ready to evaluate your RAG systems with confidence!\")"],"metadata":{"id":"HR7uIwdpzyWZ"},"execution_count":null,"outputs":[]}]}