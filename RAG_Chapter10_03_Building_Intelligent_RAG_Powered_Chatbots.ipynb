{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPn1YyleDViqmxAdrWNlgqD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"ZoM_1h2J4OJb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8ooJMlD4Anc"},"outputs":[],"source":["# Install necessary packages\n","!pip install langchain langchain-openai langchain-community chromadb faiss-cpu tiktoken\n","\n","# For tool integration\n","!pip install wikipedia requests duckduckgo-search\n","\n","import os\n","import time\n","import json\n","import re\n","from typing import List, Optional, Dict, Any, Callable, Union\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set your OpenAI API key (in Colab, you should use secrets or environment variables)\n","from getpass import getpass\n","OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n","\n","# Import necessary components\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain.schema import AIMessage, HumanMessage, SystemMessage\n","from langchain_community.vectorstores import Chroma, FAISS\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.memory import (\n","    ConversationBufferMemory,\n","    ConversationBufferWindowMemory,\n","    ConversationSummaryBufferMemory,\n","    ConversationTokenBufferMemory,\n","    ConversationSummaryMemory\n",")\n","from langchain.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import get_buffer_string\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.tools import WikipediaQueryRun, DuckDuckGoSearchRun\n","from langchain_community.utilities import WikipediaAPIWrapper, DuckDuckGoSearchAPIWrapper\n","from langchain.agents import AgentType, Tool, initialize_agent, AgentExecutor\n","from langchain.tools.retriever import create_retriever_tool\n","\n","# Initialize the LLM\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","# For streaming responses\n","streaming_llm = ChatOpenAI(\n","    model=\"gpt-3.5-turbo\",\n","    temperature=0,\n","    streaming=True,\n","    callbacks=[StreamingStdOutCallbackHandler()]\n",")\n","\n","print(\"Setup complete!\")"]},{"cell_type":"markdown","source":["**Create a Sample Knowledge Base**"],"metadata":{"id":"-CBv8Mhn4vab"}},{"cell_type":"code","source":["documents = [\n","    Document(\n","        page_content=\"Retrieval Augmented Generation (RAG) combines retrieval-based and generation-based approaches in natural language processing. It enhances large language models by retrieving relevant information from external knowledge sources before generating a response.\",\n","        metadata={\"source\": \"introduction_to_rag.pdf\", \"topic\": \"rag\", \"date\": \"2023-01-15\"}\n","    ),\n","    Document(\n","        page_content=\"LLMs like GPT-4 have significant knowledge cutoff dates, after which they don't have access to information. RAG systems can overcome this limitation by retrieving up-to-date information from external sources.\",\n","        metadata={\"source\": \"llm_limitations.pdf\", \"topic\": \"rag\", \"date\": \"2023-02-20\"}\n","    ),\n","    Document(\n","        page_content=\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. It enables computers to understand, interpret, and generate human language in a valuable way.\",\n","        metadata={\"source\": \"nlp_basics.pdf\", \"topic\": \"ai\", \"date\": \"2022-12-10\"}\n","    ),\n","    Document(\n","        page_content=\"Transformer architecture revolutionized NLP with its attention mechanism, allowing models to weigh the importance of different words in a sentence. This breakthrough enabled the development of powerful language models like BERT and GPT.\",\n","        metadata={\"source\": \"transformer_architecture.pdf\", \"topic\": \"ai\", \"date\": \"2023-01-05\"}\n","    ),\n","    Document(\n","        page_content=\"Semantic search uses embeddings to understand the meaning behind search queries rather than just matching keywords. It captures the intent and contextual meaning of queries to provide more relevant results.\",\n","        metadata={\"source\": \"semantic_search.pdf\", \"topic\": \"search\", \"date\": \"2023-03-12\"}\n","    ),\n","    Document(\n","        page_content=\"Vector databases like Pinecone, Weaviate, and Chroma are specialized for storing and searching vector embeddings. They enable efficient similarity search based on semantic meaning rather than exact keyword matches.\",\n","        metadata={\"source\": \"vector_databases.pdf\", \"topic\": \"search\", \"date\": \"2023-02-28\"}\n","    ),\n","    Document(\n","        page_content=\"Machine learning models can exhibit bias based on the data they're trained on. This can lead to unfair or discriminatory outcomes. Techniques like adversarial training and data augmentation can help mitigate these biases.\",\n","        metadata={\"source\": \"ai_ethics.pdf\", \"topic\": \"ethics\", \"date\": \"2023-04-05\"}\n","    ),\n","    Document(\n","        page_content=\"Reinforcement Learning from Human Feedback (RLHF) is a technique used to align AI systems with human preferences. It involves training models using feedback from human evaluators, helping to make AI outputs more helpful, harmless, and honest.\",\n","        metadata={\"source\": \"rlhf_explained.pdf\", \"topic\": \"ethics\", \"date\": \"2023-03-18\"}\n","    ),\n","    Document(\n","        page_content=\"Edge AI involves running machine learning models directly on edge devices like smartphones or IoT devices, rather than in the cloud. This approach reduces latency, enhances privacy, and enables AI applications in environments with limited connectivity.\",\n","        metadata={\"source\": \"edge_computing.pdf\", \"topic\": \"tech\", \"date\": \"2023-02-10\"}\n","    ),\n","    Document(\n","        page_content=\"Quantum computing leverages quantum mechanics principles to process information in ways classical computers cannot. While still in early stages, it promises breakthroughs in areas like cryptography, material science, and complex system modeling.\",\n","        metadata={\"source\": \"quantum_computing.pdf\", \"topic\": \"tech\", \"date\": \"2023-04-22\"}\n","    ),\n","]\n","\n","# Create embeddings and vector store\n","embeddings = OpenAIEmbeddings()\n","vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n","\n","print(f\"Created vector store with {len(documents)} documents\")\n","\n","# Create a simple retriever\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"],"metadata":{"id":"MauuSXWs4vkx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.4.1 Memory Management Strategies**\n","\n","---\n","\n","**Short-term vs. Long-term Memory Architectures**\n"],"metadata":{"id":"8ExMDIau46XO"}},{"cell_type":"code","source":["# 1. Buffer Memory (Short-term, stores everything verbatim)\n","buffer_memory = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","# 2. Window Memory (Short-term, keeps last k exchanges)\n","window_memory = ConversationBufferWindowMemory(\n","    memory_key=\"chat_history\",\n","    k=3,  # Number of exchanges to keep\n","    return_messages=True\n",")\n","\n","# 3. Token-limited Memory (Short-term with token limits)\n","token_memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    memory_key=\"chat_history\",\n","    max_token_limit=1000,\n","    return_messages=True\n",")\n","\n","# 4. Summary Memory (Long-term, keeps a running summary)\n","summary_memory = ConversationSummaryMemory(\n","    llm=llm,\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","# 5. Summary Buffer Memory (Hybrid approach)\n","summary_buffer_memory = ConversationSummaryBufferMemory(\n","    llm=llm,\n","    memory_key=\"chat_history\",\n","    max_token_limit=1000,\n","    return_messages=True\n",")"],"metadata":{"id":"esUlhDAe46gS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Demonstration of Different Memory Types**"],"metadata":{"id":"c7UWhdQt5INf"}},{"cell_type":"code","source":["def test_memory_type(memory, questions):\n","    \"\"\"Test a memory type with a series of questions\"\"\"\n","    retrieval_chain = ConversationalRetrievalChain.from_llm(\n","        llm=llm,\n","        retriever=retriever,\n","        memory=memory,\n","        verbose=False\n","    )\n","\n","    responses = []\n","    for i, question in enumerate(questions):\n","        print(f\"Question {i+1}: {question}\")\n","        response = retrieval_chain.invoke({\"question\": question})\n","        print(f\"Response: {response['answer']}\\n\")\n","        responses.append(response['answer'])\n","\n","        # Display current memory state\n","        mem_variables = memory.load_memory_variables({})\n","\n","        if isinstance(mem_variables[\"chat_history\"], list):\n","            chat_length = len(mem_variables[\"chat_history\"])\n","            print(f\"Memory contains {chat_length//2} exchanges\")\n","        else:\n","            # For summary memory\n","            print(f\"Memory contains summary of conversation\")\n","\n","        print(\"=\" * 50)\n","\n","    return responses\n","\n","# Sample conversation flow\n","questions = [\n","    \"What is RAG?\",\n","    \"What are its advantages?\",\n","    \"How does it relate to semantic search?\",\n","    \"Can it help with LLM knowledge cutoff issues?\",\n","    \"What other technology trends are happening in AI?\"\n","]\n","\n","# Test different memory types\n","print(\"Testing Buffer Memory:\")\n","buffer_responses = test_memory_type(buffer_memory, questions[:2])  # Just test with first two questions"],"metadata":{"id":"QBFUSb4R5IWF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Implementing a Custom Memory Architecture**"],"metadata":{"id":"_pqAbFhn5UzA"}},{"cell_type":"code","source":["class PrioritizedMemory:\n","    \"\"\"Memory that prioritizes entries based on relevance to current query\"\"\"\n","\n","    def __init__(self, embedding_function, max_entries=10):\n","        self.embedding_function = embedding_function\n","        self.max_entries = max_entries\n","        self.messages = []\n","        self.embeddings = []\n","\n","    def add_message(self, message):\n","        \"\"\"Add a message to memory\"\"\"\n","        self.messages.append(message)\n","        # Get embedding for the message\n","        embedding = self.embedding_function.embed_query(message['content'])\n","        self.embeddings.append(embedding)\n","\n","        # Trim if exceeding max entries\n","        if len(self.messages) > self.max_entries:\n","            self.messages.pop(0)\n","            self.embeddings.pop(0)\n","\n","    def get_relevant_messages(self, query, k=3):\n","        \"\"\"Retrieve most relevant messages to the current query\"\"\"\n","        if not self.messages:\n","            return []\n","\n","        # Get query embedding\n","        query_embedding = self.embedding_function.embed_query(query)\n","\n","        # Calculate similarities\n","        similarities = []\n","        for i, emb in enumerate(self.embeddings):\n","            # Compute cosine similarity\n","            similarity = self._cosine_similarity(query_embedding, emb)\n","            similarities.append((i, similarity))\n","\n","        # Sort by similarity (descending)\n","        sorted_indices = sorted(similarities, key=lambda x: x[1], reverse=True)\n","\n","        # Get top k messages\n","        relevant_indices = [idx for idx, _ in sorted_indices[:k]]\n","        relevant_messages = [self.messages[i] for i in relevant_indices]\n","\n","        return relevant_messages\n","\n","    def _cosine_similarity(self, a, b):\n","        \"\"\"Compute cosine similarity between two vectors\"\"\"\n","        dot_product = sum(x * y for x, y in zip(a, b))\n","        magnitude_a = sum(x * x for x in a) ** 0.5\n","        magnitude_b = sum(x * x for x in b) ** 0.5\n","        return dot_product / (magnitude_a * magnitude_b)\n","\n","# Example of using the prioritized memory\n","prioritized_memory = PrioritizedMemory(embedding_function=embeddings)\n","\n","# We won't run a full test here as it would require deeper integration\n","print(\"Prioritized memory initialized\")"],"metadata":{"id":"RWug1Qoo5U79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.4.2 Contextual Retrieval for Natural Conversations**\n","\n","---\n","\n","**Query Formulation from Chat Context**"],"metadata":{"id":"Ztzbda2u5eNa"}},{"cell_type":"code","source":["def rewrite_query_with_context(query, chat_history):\n","    \"\"\"\n","    Rewrite query considering chat history context\n","    \"\"\"\n","    if not chat_history:\n","        return query\n","\n","    # Format chat history as a string\n","    formatted_history = \"\"\n","    for message in chat_history:\n","        if isinstance(message, HumanMessage):\n","            formatted_history += f\"Human: {message.content}\\n\"\n","        elif isinstance(message, AIMessage):\n","            formatted_history += f\"AI: {message.content}\\n\"\n","\n","    # Create prompt for query rewriting\n","    rewrite_prompt = f\"\"\"\n","    Given the chat history and the latest query, rewrite the query to be more specific and self-contained,\n","    incorporating relevant context from the chat history.\n","\n","    Chat History:\n","    {formatted_history}\n","\n","    Latest Query: {query}\n","\n","    Rewritten Query:\n","    \"\"\"\n","\n","    # Get rewritten query\n","    response = llm.invoke(rewrite_prompt)\n","    rewritten_query = response.content.strip()\n","\n","    return rewritten_query\n","\n","# Example of query rewriting\n","chat_history_example = [\n","    HumanMessage(content=\"What is a Retrieval Augmented Generation system?\"),\n","    AIMessage(content=\"Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches in natural language processing. It enhances large language models by retrieving relevant information from external knowledge sources before generating a response.\"),\n","    HumanMessage(content=\"What are its main advantages?\")\n","]\n","\n","original_query = \"What are its main advantages?\"\n","rewritten_query = rewrite_query_with_context(original_query, chat_history_example)\n","\n","print(f\"Original query: {original_query}\")\n","print(f\"Rewritten query: {rewritten_query}\\n\")"],"metadata":{"id":"0CZK2pgZ5eZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Building a Contextual RAG Chain**"],"metadata":{"id":"X___0V435sjP"}},{"cell_type":"code","source":["def contextual_rag_chain():\n","    \"\"\"Create a RAG chain with contextual query understanding\"\"\"\n","\n","    # Create conversation memory\n","    memory = ConversationBufferMemory(\n","        memory_key=\"chat_history\",\n","        return_messages=True\n","    )\n","\n","    # Create a template for context-aware retrieval\n","    prompt_template = \"\"\"\n","    Answer the question based on the following context and the chat history.\n","    If you don't know the answer, just say you don't know.\n","\n","    Chat History:\n","    {chat_history}\n","\n","    Context:\n","    {context}\n","\n","    Question: {question}\n","\n","    Answer:\n","    \"\"\"\n","\n","    qa_prompt = ChatPromptTemplate.from_template(prompt_template)\n","\n","    # Define a function to get a string representation of chat history\n","    def get_chat_history(inputs):\n","        memory_variables = memory.load_memory_variables({})\n","        chat_history = memory_variables.get(\"chat_history\", [])\n","\n","        # Format the chat history as a string\n","        formatted_history = \"\"\n","        for message in chat_history:\n","            if isinstance(message, HumanMessage):\n","                formatted_history += f\"Human: {message.content}\\n\"\n","            elif isinstance(message, AIMessage):\n","                formatted_history += f\"AI: {message.content}\\n\"\n","\n","        return formatted_history\n","\n","    # Define the retriever to get relevant documents\n","    def get_context(inputs):\n","        query = inputs[\"question\"]\n","\n","        # Get chat history\n","        memory_variables = memory.load_memory_variables({})\n","        chat_history = memory_variables.get(\"chat_history\", [])\n","\n","        # If there's chat history, rewrite the query for better context\n","        if chat_history:\n","            rewritten_query = rewrite_query_with_context(query, chat_history)\n","            print(f\"Original query: {query}\")\n","            print(f\"Rewritten query: {rewritten_query}\")\n","\n","            # Use the rewritten query for retrieval\n","            docs = retriever.get_relevant_documents(rewritten_query)\n","        else:\n","            # For the first question, use it directly\n","            docs = retriever.get_relevant_documents(query)\n","\n","        # Format the documents into a string\n","        return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","    # Build the contextual RAG chain\n","    contextual_chain = (\n","        {\n","            \"question\": lambda x: x[\"question\"],\n","            \"context\": get_context,\n","            \"chat_history\": get_chat_history\n","        }\n","        | qa_prompt\n","        | llm\n","    )\n","\n","    # Add memory updating\n","    def invoke_with_memory(inputs):\n","        result = contextual_chain.invoke(inputs)\n","        # Save interaction to memory\n","        memory.save_context(\n","            {\"input\": inputs[\"question\"]},\n","            {\"output\": result.content}\n","        )\n","        return result.content\n","\n","    return invoke_with_memory\n","\n","# Create a contextual RAG chain\n","contextual_rag = contextual_rag_chain()\n","\n","# Test with a conversation flow\n","print(\"Testing Contextual RAG Chain:\")\n","test_conversation = [\n","    \"What are Transformer models in AI?\",\n","    \"How did they change NLP?\",\n","    \"What are some applications of this technology?\"\n","]\n","\n","for question in test_conversation:\n","    print(f\"\\nQuestion: {question}\")\n","    answer = contextual_rag({\"question\": question})\n","    print(f\"Answer: {answer}\")\n","    print(\"=\" * 50)"],"metadata":{"id":"hZByWNgX5svx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Balancing Retrieval with Conversational Flow**"],"metadata":{"id":"UKusDnQP6acO"}},{"cell_type":"code","source":["def balanced_conversation_chain():\n","    \"\"\"Create a chain that balances retrieval with natural conversation\"\"\"\n","    memory = ConversationBufferMemory(\n","        memory_key=\"chat_history\",\n","        return_messages=True\n","    )\n","\n","    # Define a function to check if retrieval is needed\n","    def should_retrieve(query, chat_history):\n","        \"\"\"Determine if the query needs knowledge retrieval\"\"\"\n","        if not chat_history:\n","            return True  # Always retrieve for first question\n","\n","        # Check for question patterns that likely need retrieval\n","        info_seeking_patterns = [\n","            r\"what (is|are|was|were)\",\n","            r\"how (do|does|did)\",\n","            r\"why (is|are|was|were)\",\n","            r\"when (is|are|was|were)\",\n","            r\"where (is|are|was|were)\",\n","            r\"who (is|are|was|were)\",\n","            r\"define\",\n","            r\"explain\",\n","            r\"describe\"\n","        ]\n","\n","        # Check for conversational patterns that likely don't need retrieval\n","        conversational_patterns = [\n","            r\"^(yes|no|maybe|sure|thanks|thank you|ok|okay|great)$\",\n","            r\"can you\",\n","            r\"please\",\n","            r\"i (think|believe|feel)\",\n","            r\"(and|but|or) (what|how) about\"\n","        ]\n","\n","        # Check if query matches information-seeking patterns\n","        for pattern in info_seeking_patterns:\n","            if re.search(pattern, query.lower()):\n","                return True\n","\n","        # Check if query only matches conversational patterns\n","        only_conversational = False\n","        for pattern in conversational_patterns:\n","            if re.search(pattern, query.lower()):\n","                only_conversational = True\n","                break\n","\n","        # If query is very short, likely conversational\n","        if len(query.split()) < 4 and only_conversational:\n","            return False\n","\n","        # Default to retrieving\n","        return True\n","\n","    # Define retrieval logic\n","    def conditional_retrieval(inputs):\n","        query = inputs[\"question\"]\n","        memory_variables = memory.load_memory_variables({})\n","        chat_history = memory_variables.get(\"chat_history\", [])\n","\n","        # Determine if retrieval is needed\n","        retrieval_needed = should_retrieve(query, chat_history)\n","\n","        # If retrieval is needed, get relevant documents\n","        if retrieval_needed:\n","            print(\"Retrieval activated for this query\")\n","            # Rewrite query with context\n","            rewritten_query = rewrite_query_with_context(query, chat_history)\n","            # Get documents\n","            docs = retriever.get_relevant_documents(rewritten_query)\n","            context = \"\\n\\n\".join(doc.page_content for doc in docs)\n","        else:\n","            print(\"Conversational mode (no retrieval) for this query\")\n","            context = \"No retrieval performed - use your general knowledge for this conversational query.\"\n","\n","        return context\n","\n","    # Create prompt templates for different modes\n","    standard_prompt = \"\"\"\n","    Answer the following question based on the context and chat history.\n","\n","    Chat History:\n","    {chat_history}\n","\n","    Context from knowledge base:\n","    {context}\n","\n","    Question: {question}\n","\n","    Answer:\n","    \"\"\"\n","\n","    # Build the chain\n","    balanced_chain = (\n","        {\n","            \"question\": lambda x: x[\"question\"],\n","            \"context\": conditional_retrieval,\n","            \"chat_history\": lambda x: get_buffer_string(memory.chat_memory.messages)\n","        }\n","        | ChatPromptTemplate.from_template(standard_prompt)\n","        | llm\n","    )\n","\n","    # Add memory updating\n","    def invoke_with_memory(inputs):\n","        result = balanced_chain.invoke(inputs)\n","        # Save interaction to memory\n","        memory.save_context(\n","            {\"input\": inputs[\"question\"]},\n","            {\"output\": result.content}\n","        )\n","        return result.content\n","\n","    return invoke_with_memory\n","\n","# Create a balanced conversation chain\n","balanced_convo = balanced_conversation_chain()\n","\n","# Test with mixed retrieval and conversational questions\n","print(\"Testing Balanced Conversation Chain:\")\n","mixed_conversation = [\n","    \"What is semantic search technology?\",\n","    \"That sounds interesting!\",\n","    \"How does it relate to vector databases?\",\n","    \"Thank you for explaining that\",\n","    \"What ethical concerns should I be aware of?\"\n","]\n","\n","for question in mixed_conversation:\n","    print(f\"\\nQuestion: {question}\")\n","    answer = balanced_convo({\"question\": question})\n","    print(f\"Answer: {answer}\")\n","    print(\"=\" * 50)"],"metadata":{"id":"GUgOuilh6alV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.4.3 Tool Integration for Enhanced Capabilities**\n","\n","---\n","\n","**Combining RAG with External Tools**"],"metadata":{"id":"za6UctR76r_t"}},{"cell_type":"code","source":["# Define some external tools\n","wikipedia_tool = Tool(\n","    name=\"Wikipedia\",\n","    func=WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()).run,\n","    description=\"Useful for searching for information on Wikipedia. Input should be a search query.\"\n",")\n","\n","search_tool = Tool(\n","    name=\"DuckDuckGo Search\",\n","    func=DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper()).run,\n","    description=\"Useful for searching for current information on the web. Input should be a search query.\"\n",")\n","\n","# Create a retriever tool from our knowledge base\n","retriever_tool = create_retriever_tool(\n","    retriever,\n","    \"Knowledge Base\",\n","    \"Useful for searching our internal knowledge base for information on AI, RAG, and technology.\"\n",")\n","\n","# List of tools\n","tools = [retriever_tool, wikipedia_tool, search_tool]"],"metadata":{"id":"yOhvGhX66sJT","executionInfo":{"status":"ok","timestamp":1743621151544,"user_tz":-330,"elapsed":659,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**Building a Tool-Selection Framework**"],"metadata":{"id":"TzbR8TvZ62zX"}},{"cell_type":"code","source":["def create_tool_selection_chain():\n","    \"\"\"Create a chain that selects appropriate tools based on the query\"\"\"\n","\n","    # Tool selection prompt\n","    tool_selection_prompt = \"\"\"\n","    You need to decide which tool(s) to use for answering the user's question.\n","\n","    User Question: {question}\n","\n","    Available tools:\n","    1. Knowledge Base: Our internal knowledge base with information on AI, RAG, and technology.\n","    2. Wikipedia: Access to Wikipedia for general knowledge and concepts.\n","    3. DuckDuckGo Search: Web search for current or specific information not in our knowledge base.\n","\n","    For this question, which tool(s) would be most appropriate? Respond with a comma-separated list of tool names or \"none\" if no tools are needed.\n","\n","    Selected tool(s):\n","    \"\"\"\n","\n","    # Create the selection chain\n","    selection_chain = PromptTemplate.from_template(tool_selection_prompt) | llm\n","\n","    return selection_chain\n","\n","# Create the tool selection chain\n","tool_selector = create_tool_selection_chain()\n","\n","# Example queries\n","tool_test_queries = [\n","    \"What is RAG technology?\",\n","    \"What happened in the 2024 Olympics?\",\n","    \"Who invented the transistor?\",\n","    \"I'm feeling tired today\"\n","]\n","\n","# Test tool selection\n","for query in tool_test_queries:\n","    print(f\"Query: {query}\")\n","    selection = tool_selector.invoke({\"question\": query})\n","    print(f\"Recommended tools: {selection.content}\\n\")"],"metadata":{"id":"WzLbe54c628s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Implementing a RAG Chain with Tool Integration**"],"metadata":{"id":"IdFwl9lP7BLj"}},{"cell_type":"code","source":["def create_rag_with_tools_chain():\n","    \"\"\"Create a RAG chain that can use tools when appropriate\"\"\"\n","    # Create memory\n","    memory = ConversationBufferWindowMemory(\n","        memory_key=\"chat_history\",\n","        k=5,\n","        return_messages=True\n","    )\n","\n","    # Initialize the agent\n","    agent = initialize_agent(\n","        tools,\n","        llm,\n","        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n","        memory=memory,\n","        verbose=True  # For demonstration, we'll keep this verbose\n","    )\n","\n","    # Create a wrapper for simplified invocation\n","    def invoke_agent(inputs):\n","        question = inputs[\"question\"]\n","        result = agent.invoke({\"input\": question})\n","        return result[\"output\"]\n","\n","    return invoke_agent\n","\n","# Create the RAG with tools chain\n","rag_with_tools = create_rag_with_tools_chain()\n","\n","# We won't run this agent right now as it requires actual API calls,\n","# but below is how you would invoke it\n","print(\"RAG with Tools chain created (won't run API calls in this demo)\")\n","# result = rag_with_tools({\"question\": \"What is RAG and how does it relate to current AI trends?\"})\n","# print(result)"],"metadata":{"id":"2WVR1kuV7BTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.4.4 Optimizing for Extended Conversations**\n","\n","---\n","\n","**Managing Growing Chat Histories**\n"],"metadata":{"id":"q1JsmgXD7LGm"}},{"cell_type":"code","source":["def token_counter(text):\n","    \"\"\"Count tokens in a text using a simple approximation (not exact)\"\"\"\n","    # Approximate token count (this is a rough estimate)\n","    return len(text.split()) * 1.3  # Average words to tokens ratio\n","\n","def create_token_optimized_memory(max_tokens=2000):\n","    \"\"\"Create a memory that optimizes token usage\"\"\"\n","    # Use token-aware memory\n","    token_memory = ConversationTokenBufferMemory(\n","        llm=llm,\n","        max_token_limit=max_tokens,\n","        memory_key=\"chat_history\",\n","        return_messages=True\n","    )\n","\n","    return token_memory\n","\n","# Create token-optimized memory\n","token_optimized_memory = create_token_optimized_memory()\n","print(\"Token-optimized memory created\")"],"metadata":{"id":"fUdneE-97LRE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summarization and Compression Techniques**"],"metadata":{"id":"YsRKDoIV7UhR"}},{"cell_type":"code","source":["def create_hybrid_memory():\n","    \"\"\"Create a hybrid memory that combines recent messages with a summary\"\"\"\n","    # Use summary buffer memory\n","    summary_memory = ConversationSummaryBufferMemory(\n","        llm=llm,\n","        max_token_limit=1000,\n","        memory_key=\"chat_history\",\n","        return_messages=True\n","    )\n","\n","    return summary_memory\n","\n","def compress_history(messages, max_tokens=1000):\n","    \"\"\"Compress a long conversation history\"\"\"\n","\n","    # If messages are short enough, return as is\n","    estimated_tokens = sum(token_counter(msg.content) for msg in messages)\n","    if estimated_tokens <= max_tokens:\n","        return messages\n","\n","    # Separate into recent and old messages\n","    # Keep the most recent 3 exchanges (6 messages) as is\n","    recent_messages = messages[-6:] if len(messages) > 6 else messages\n","    old_messages = messages[:-6] if len(messages) > 6 else []\n","\n","    # If there are no old messages, return recent ones\n","    if not old_messages:\n","        return recent_messages\n","\n","    # Create a summary of old messages\n","    old_messages_text = \"\\n\".join([f\"{'Human' if i % 2 == 0 else 'AI'}: {msg.content}\"\n","                                for i, msg in enumerate(old_messages)])\n","\n","    summary_prompt = f\"\"\"\n","    Summarize the following conversation in a concise way that preserves the key information,\n","    especially topics, entities, and important details that might be referenced later.\n","    Keep your summary under 200 words.\n","\n","    Conversation:\n","    {old_messages_text}\n","\n","    Summary:\n","    \"\"\"\n","\n","    summary = llm.invoke(summary_prompt).content\n","\n","    # Create a system message with the summary\n","    system_summary = SystemMessage(content=f\"Previous conversation summary: {summary}\")\n","\n","    # Return the summary and recent messages\n","    return [system_summary] + recent_messages\n","\n","# Example of conversation compression\n","sample_history = [\n","    HumanMessage(content=\"What is RAG technology?\"),\n","    AIMessage(content=\"Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches in natural language processing. It enhances large language models by retrieving relevant information from external knowledge sources before generating a response.\"),\n","    HumanMessage(content=\"What are the main components of a RAG system?\"),\n","    AIMessage(content=\"A RAG system typically consists of: 1) A document store or knowledge base 2) An embedding model to convert documents into vector representations 3) A vector database for efficient similarity search 4) A retriever that finds relevant documents for a query 5) A large language model that generates responses based on the retrieved information and 6) A prompt template that structures the interaction between these components.\"),\n","    HumanMessage(content=\"How does the retrieval process work?\"),\n","    AIMessage(content=\"The retrieval process in RAG works by first converting the user's query into a vector embedding using the same embedding model used for the documents. Then, a similarity search is performed in the vector database to find documents whose embeddings are closest to the query embedding (using metrics like cosine similarity). The most similar documents are retrieved and provided as context to the language model along with the original query to generate an informed response.\"),\n","    HumanMessage(content=\"What are some challenges with RAG systems?\"),\n","    AIMessage(content=\"Some key challenges with RAG systems include: 1) Retrieval quality - ensuring the most relevant documents are found 2) Context window limitations - balancing comprehensive context with token limits 3) Hallucination mitigation - even with retrieval, LLMs can still generate incorrect information 4) Latency - the additional retrieval step adds processing time 5) Handling ambiguous queries that may not clearly map to specific documents 6) Keeping the knowledge base up-to-date with fresh information.\")\n","]\n","\n","# Compress the history\n","compressed_history = compress_history(sample_history)\n","\n","print(f\"Original message count: {len(sample_history)}\")\n","print(f\"Compressed message count: {len(compressed_history)}\")\n","\n","# Show the system message summary if compression happened\n","if len(compressed_history) < len(sample_history):\n","    system_msg = compressed_history[0]\n","    print(\"\\nCompressed Summary:\")\n","    print(system_msg.content)"],"metadata":{"id":"hOw-yhfo7Urf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Maintaining Coherence Across Sessions**"],"metadata":{"id":"OkMhkfU-7ho2"}},{"cell_type":"code","source":["class PersistentConversationManager:\n","    \"\"\"Manages persistent conversations across sessions\"\"\"\n","\n","    def __init__(self, user_id, storage_path=\"./conversations\"):\n","        self.user_id = user_id\n","        self.storage_path = storage_path\n","        self.current_session = []\n","        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","\n","        # Create storage directory if it doesn't exist\n","        os.makedirs(storage_path, exist_ok=True)\n","\n","    def get_conversation_file(self):\n","        \"\"\"Get the path to the conversation file for this user\"\"\"\n","        return os.path.join(self.storage_path, f\"{self.user_id}_conversation.json\")\n","\n","    def load_conversation_history(self):\n","        \"\"\"Load conversation history from storage\"\"\"\n","        file_path = self.get_conversation_file()\n","        if os.path.exists(file_path):\n","            try:\n","                with open(file_path, 'r') as f:\n","                    data = json.load(f)\n","\n","                # Convert to message objects\n","                history = []\n","                for msg in data.get('history', []):\n","                    if msg['type'] == 'human':\n","                        history.append(HumanMessage(content=msg['content']))\n","                    elif msg['type'] == 'ai':\n","                        history.append(AIMessage(content=msg['content']))\n","                    elif msg['type'] == 'system':\n","                        history.append(SystemMessage(content=msg['content']))\n","\n","                return history\n","            except Exception as e:\n","                print(f\"Error loading conversation: {e}\")\n","                return []\n","        else:\n","            return []\n","\n","    def save_conversation_history(self, history):\n","        \"\"\"Save conversation history to storage\"\"\"\n","        file_path = self.get_conversation_file()\n","\n","        # Convert to serializable format\n","        serializable_history = []\n","        for msg in history:\n","            if isinstance(msg, HumanMessage):\n","                serializable_history.append({'type': 'human', 'content': msg.content})\n","            elif isinstance(msg, AIMessage):\n","                serializable_history.append({'type': 'ai', 'content': msg.content})\n","            elif isinstance(msg, SystemMessage):\n","                serializable_history.append({'type': 'system', 'content': msg.content})\n","\n","        # Save to file\n","        with open(file_path, 'w') as f:\n","            json.dump({'history': serializable_history}, f)\n","\n","    def start_new_session(self):\n","        \"\"\"Start a new conversation session\"\"\"\n","        # Load previous history\n","        previous_history = self.load_conversation_history()\n","\n","        # If history exists, create a summary to maintain context\n","        if previous_history:\n","            # Compress the history to create a summary\n","            self.current_session = compress_history(previous_history)\n","            print(f\"Loaded previous conversation with {len(previous_history)} messages\")\n","            print(f\"Compressed to {len(self.current_session)} messages\")\n","        else:\n","            # Start fresh\n","            self.current_session = []\n","            print(\"Starting new conversation with no previous history\")\n","\n","        return self.current_session\n","\n","    def add_message(self, message, is_human=True):\n","        \"\"\"Add a message to the current session\"\"\"\n","        if is_human:\n","            self.current_session.append(HumanMessage(content=message))\n","        else:\n","            self.current_session.append(AIMessage(content=message))\n","\n","        # Save after each message\n","        self.save_conversation_history(self.current_session)\n","\n","    def get_response(self, query):\n","        \"\"\"Get a response to a query using the conversation history\"\"\"\n","        # Add the query to the session\n","        self.add_message(query, is_human=True)\n","\n","        # Create context for the query using RAG\n","        context = \"\"\n","        if retriever:\n","            docs = retriever.get_relevant_documents(query)\n","            if docs:\n","                context = \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","        # Create prompt with history and context\n","        prompt = f\"\"\"\n","        You are a helpful AI assistant. Respond to the user's query based on the conversation history and provided context.\n","\n","        Context: {context}\n","\n","        Respond to the latest message from the user.\n","        \"\"\"\n","\n","        # Generate response\n","        messages = [SystemMessage(content=prompt)] + self.current_session\n","        response = self.llm.invoke(messages)\n","\n","        # Add response to session\n","        self.add_message(response.content, is_human=False)\n","\n","        return response.content\n","\n","    def end_session(self):\n","        \"\"\"End the current session and save\"\"\"\n","        self.save_conversation_history(self.current_session)\n","        print(\"Session ended and saved\")\n","\n","# Create a conversation manager for demonstration\n","# Note: In a real application, each user would have their own ID\n","demo_manager = PersistentConversationManager(user_id=\"demo_user\")\n","\n","# Start a new session\n","history = demo_manager.start_new_session()\n","\n","# Simulate a short conversation\n","print(\"\\nSimulating conversation with persistent manager:\")\n","queries = [\n","    \"What are the key benefits of RAG systems?\",\n","    \"How do they handle up-to-date information?\"\n","]\n","\n","for query in queries:\n","    print(f\"\\nHuman: {query}\")\n","    response = demo_manager.get_response(query)\n","    print(f\"AI: {response}\")\n","\n","# End the session\n","demo_manager.end_session()"],"metadata":{"id":"6FcaQbIz7h02"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Implementing a Complete RAG Chatbot**"],"metadata":{"id":"GlC8ifS88YlS"}},{"cell_type":"code","source":["class ComprehensiveRAGChatbot:\n","    \"\"\"A complete RAG chatbot with memory, tools, and contextual understanding\"\"\"\n","\n","    def __init__(self, retriever, memory_type=\"summary_buffer\", max_tokens=2000):\n","        self.retriever = retriever\n","        self.memory_type = memory_type\n","        self.max_tokens = max_tokens\n","\n","        # Initialize LLM\n","        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n","\n","        # Initialize memory based on type\n","        self.memory = self._initialize_memory()\n","\n","        # Initialize tools\n","        self.tools = self._initialize_tools()\n","\n","        # Initialize the agent\n","        if self.tools:\n","            self.agent = initialize_agent(\n","                self.tools,\n","                self.llm,\n","                agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n","                memory=self.memory,\n","                verbose=False\n","            )\n","        else:\n","            self.agent = None\n","\n","    def _initialize_memory(self):\n","        \"\"\"Initialize the appropriate memory type\"\"\"\n","        if self.memory_type == \"buffer\":\n","            return ConversationBufferMemory(\n","                memory_key=\"chat_history\",\n","                return_messages=True\n","            )\n","        elif self.memory_type == \"window\":\n","            return ConversationBufferWindowMemory(\n","                memory_key=\"chat_history\",\n","                k=5,\n","                return_messages=True\n","            )\n","        elif self.memory_type == \"token\":\n","            return ConversationTokenBufferMemory(\n","                llm=self.llm,\n","                memory_key=\"chat_history\",\n","                max_token_limit=self.max_tokens,\n","                return_messages=True\n","            )\n","        elif self.memory_type == \"summary\":\n","            return ConversationSummaryMemory(\n","                llm=self.llm,\n","                memory_key=\"chat_history\",\n","                return_messages=True\n","            )\n","        else:  # Default to summary buffer\n","            return ConversationSummaryBufferMemory(\n","                llm=self.llm,\n","                memory_key=\"chat_history\",\n","                max_token_limit=self.max_tokens,\n","                return_messages=True\n","            )\n","\n","    def _initialize_tools(self):\n","        \"\"\"Initialize tools for the chatbot\"\"\"\n","        # Create retriever tool\n","        retriever_tool = create_retriever_tool(\n","            self.retriever,\n","            \"Knowledge Base\",\n","            \"Useful for searching our internal knowledge base for information on AI and technology.\"\n","        )\n","\n","        # For simplicity, we'll just use the retriever tool\n","        # In a real implementation, you might add more tools\n","        return [retriever_tool]\n","\n","    def _should_use_tools(self, query):\n","        \"\"\"Determine if tools should be used for this query\"\"\"\n","        # This is a simplified version - in a real implementation,\n","        # you might use more sophisticated logic\n","        info_seeking_patterns = [\n","            r\"what (is|are|was|were)\",\n","            r\"how (do|does|did)\",\n","            r\"why (is|are|was|were)\",\n","            r\"when (is|are|was|were)\",\n","            r\"where (is|are|was|were)\",\n","            r\"who (is|are|was|were)\",\n","            r\"tell me about\",\n","            r\"explain\",\n","            r\"describe\"\n","        ]\n","\n","        for pattern in info_seeking_patterns:\n","            if re.search(pattern, query.lower()):\n","                return True\n","\n","        return False\n","\n","    def _get_context_directly(self, query):\n","        \"\"\"Get context directly from the retriever (no tools)\"\"\"\n","        docs = self.retriever.get_relevant_documents(query)\n","        return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","    def chat(self, query):\n","        \"\"\"Process a user query and return a response\"\"\"\n","        # Check if we should use tools\n","        use_tools = self._should_use_tools(query) and self.agent is not None\n","\n","        if use_tools:\n","            # Use the agent with tools\n","            result = self.agent.invoke({\"input\": query})\n","            return result[\"output\"]\n","        else:\n","            # Use direct RAG (no tools)\n","            # Get context from retriever\n","            context = self._get_context_directly(query)\n","\n","            # Get chat history\n","            chat_history = self.memory.chat_memory.messages if hasattr(self.memory, 'chat_memory') else []\n","\n","            # Create messages for the LLM\n","            messages = [\n","                SystemMessage(content=f\"\"\"\n","                You are a helpful AI assistant. Answer the user's question based on the\n","                provided context and conversation history. If the context doesn't contain\n","                relevant information, use your general knowledge but be clear about what\n","                you know vs. what you're inferring.\n","\n","                Context from knowledge base:\n","                {context}\n","                \"\"\")\n","            ]\n","\n","            # Add chat history\n","            messages.extend(chat_history)\n","\n","            # Add the current query\n","            messages.append(HumanMessage(content=query))\n","\n","            # Generate response\n","            response = self.llm.invoke(messages)\n","\n","            # Update memory\n","            self.memory.chat_memory.add_user_message(query)\n","            self.memory.chat_memory.add_ai_message(response.content)\n","\n","            return response.content\n","\n","# Create a comprehensive RAG chatbot\n","comprehensive_chatbot = ComprehensiveRAGChatbot(retriever, memory_type=\"summary_buffer\")\n","\n","# Test the chatbot with a conversation\n","print(\"\\nTesting Comprehensive RAG Chatbot:\")\n","conversation = [\n","    \"What is Retrieval Augmented Generation?\",\n","    \"How does it compare to traditional question answering?\",\n","    \"Can you tell me about transformer architecture?\",\n","    \"Thanks for explaining that!\",\n","    \"What about ethical considerations in AI development?\"\n","]\n","\n","for query in conversation:\n","    print(f\"\\nHuman: {query}\")\n","    response = comprehensive_chatbot.chat(query)\n","    print(f\"AI: {response}\")"],"metadata":{"id":"5AMDdAu58Yu3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","In this notebook, we've implemented and demonstrated the key components of building intelligent RAG-powered chatbots:\n","\n","1. **Memory Management Strategies** - Different approaches to managing conversation history\n","2. **Contextual Retrieval** - Techniques for improved query understanding and relevant document retrieval\n","3. **Tool Integration** - Methods for combining RAG with external tools\n","4. **Optimizing for Extended Conversations** - Strategies for handling long interactions\n","\n","These building blocks can be combined and customized to create sophisticated conversational AI systems tailored to specific use cases and domains."],"metadata":{"id":"XUMTQQxP8pp_"}}]}