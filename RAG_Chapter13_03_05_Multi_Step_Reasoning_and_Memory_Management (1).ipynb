{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFRSwqtVaEKd30xDUzWL9C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**# INSTALLATION**"],"metadata":{"id":"c9FE71UnwiGH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKwToMg4wPQ-"},"outputs":[],"source":["!pip install langchain langchain-openai langchain-community langchain-core networkx matplotlib plotly pandas numpy python-dotenv tiktoken redis\n","\n","# IMPORTS\n","\n","import os\n","import json\n","import sqlite3\n","import hashlib\n","import networkx as nx\n","from datetime import datetime, timedelta\n","from typing import Dict, List, Any, Optional, Union, Tuple\n","from dataclasses import dataclass, field\n","from enum import Enum\n","import pandas as pd\n","import numpy as np\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","# LangChain imports\n","from langchain.agents import AgentExecutor, create_react_agent\n","from langchain.tools import tool\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import AgentAction, AgentFinish\n","from langchain.callbacks.base import BaseCallbackHandler\n","\n","# Load environment variables\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# Set your OpenAI API key here\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n"]},{"cell_type":"markdown","source":["**# SECTION 1: Query Decomposition and Planning (13.3)**"],"metadata":{"id":"1OGIpgjQxUDI"}},{"cell_type":"code","source":["print(\"=\"*60)\n","print(\"SECTION 1: QUERY DECOMPOSITION AND PLANNING\")\n","print(\"=\"*60)\n","\n","class QueryType(Enum):\n","    \"\"\"Classification of query types for appropriate planning strategies\"\"\"\n","    FACTUAL = \"factual\"\n","    ANALYTICAL = \"analytical\"\n","    COMPARATIVE = \"comparative\"\n","    RESEARCH = \"research\"\n","    SYNTHESIS = \"synthesis\"\n","\n","@dataclass\n","class PlanningStep:\n","    \"\"\"Individual step in a multi-step plan\"\"\"\n","    id: str\n","    description: str\n","    step_type: str\n","    dependencies: List[str] = field(default_factory=list)\n","    tools_required: List[str] = field(default_factory=list)\n","    expected_output: str = \"\"\n","    priority: int = 1\n","    estimated_time: int = 30  # seconds\n","    status: str = \"pending\"  # pending, in_progress, completed, failed\n","    result: Any = None\n","    confidence: float = 0.0\n","\n","@dataclass\n","class ExecutionPlan:\n","    \"\"\"Complete execution plan for complex queries\"\"\"\n","    query: str\n","    query_type: QueryType\n","    steps: List[PlanningStep]\n","    execution_order: List[str]\n","    parallel_groups: List[List[str]] = field(default_factory=list)\n","    total_estimated_time: int = 0\n","    created_at: datetime = field(default_factory=datetime.now)\n","\n","class QueryDecomposer:\n","    \"\"\"Advanced query decomposition system\"\"\"\n","\n","    def __init__(self, llm):\n","        self.llm = llm\n","        self.decomposition_prompt = PromptTemplate.from_template(\"\"\"\n","You are an expert query analyst. Decompose the following complex query into specific, actionable steps.\n","\n","Query: {query}\n","Query Type: {query_type}\n","\n","For each step, provide:\n","1. Step description (what to do)\n","2. Step type (search, analyze, calculate, synthesize, verify)\n","3. Required tools (web_search, database, calculator, etc.)\n","4. Dependencies (which previous steps must complete first)\n","5. Expected output type\n","6. Priority (1=critical, 2=important, 3=optional)\n","\n","Consider:\n","- Information gathering needs\n","- Analysis requirements\n","- Verification steps\n","- Synthesis and presentation needs\n","- Potential parallel execution opportunities\n","\n","Respond in JSON format:\n","{{\n","    \"steps\": [\n","        {{\n","            \"id\": \"step_1\",\n","            \"description\": \"Search for recent information about...\",\n","            \"step_type\": \"search\",\n","            \"dependencies\": [],\n","            \"tools_required\": [\"web_search\"],\n","            \"expected_output\": \"List of recent developments\",\n","            \"priority\": 1\n","        }}\n","    ],\n","    \"parallel_groups\": [[\"step_1\", \"step_2\"], [\"step_3\"]],\n","    \"execution_strategy\": \"sequential_with_parallel_opportunities\"\n","}}\n","        \"\"\")\n","\n","    def classify_query(self, query: str) -> QueryType:\n","        \"\"\"Classify query to determine appropriate planning strategy\"\"\"\n","        classification_prompt = f\"\"\"\n","        Classify this query into one category:\n","        - factual: Simple fact-finding\n","        - analytical: Requires analysis of data/trends\n","        - comparative: Comparing options/alternatives\n","        - research: Comprehensive investigation\n","        - synthesis: Combining multiple sources/perspectives\n","\n","        Query: {query}\n","\n","        Return only the category name.\n","        \"\"\"\n","\n","        response = self.llm.invoke(classification_prompt)\n","        try:\n","            return QueryType(response.content.strip().lower())\n","        except ValueError:\n","            return QueryType.RESEARCH  # Default fallback\n","\n","    def decompose_query(self, query: str) -> ExecutionPlan:\n","        \"\"\"Decompose complex query into structured execution plan\"\"\"\n","        query_type = self.classify_query(query)\n","\n","        prompt = self.decomposition_prompt.format(\n","            query=query,\n","            query_type=query_type.value\n","        )\n","\n","        response = self.llm.invoke(prompt)\n","\n","        try:\n","            plan_data = json.loads(response.content)\n","\n","            # Convert to PlanningStep objects\n","            steps = []\n","            for step_data in plan_data[\"steps\"]:\n","                step = PlanningStep(\n","                    id=step_data[\"id\"],\n","                    description=step_data[\"description\"],\n","                    step_type=step_data[\"step_type\"],\n","                    dependencies=step_data.get(\"dependencies\", []),\n","                    tools_required=step_data.get(\"tools_required\", []),\n","                    expected_output=step_data.get(\"expected_output\", \"\"),\n","                    priority=step_data.get(\"priority\", 1)\n","                )\n","                steps.append(step)\n","\n","            # Determine execution order\n","            execution_order = self._determine_execution_order(steps)\n","\n","            # Extract parallel groups\n","            parallel_groups = plan_data.get(\"parallel_groups\", [])\n","\n","            return ExecutionPlan(\n","                query=query,\n","                query_type=query_type,\n","                steps=steps,\n","                execution_order=execution_order,\n","                parallel_groups=parallel_groups,\n","                total_estimated_time=sum(step.estimated_time for step in steps)\n","            )\n","\n","        except (json.JSONDecodeError, KeyError) as e:\n","            print(f\"Error parsing decomposition response: {e}\")\n","            # Fallback to simple plan\n","            return self._create_fallback_plan(query, query_type)\n","\n","    def _determine_execution_order(self, steps: List[PlanningStep]) -> List[str]:\n","        \"\"\"Determine optimal execution order based on dependencies\"\"\"\n","        # Create dependency graph\n","        graph = nx.DiGraph()\n","\n","        for step in steps:\n","            graph.add_node(step.id)\n","            for dep in step.dependencies:\n","                graph.add_edge(dep, step.id)\n","\n","        try:\n","            # Topological sort for dependency order\n","            return list(nx.topological_sort(graph))\n","        except nx.NetworkXError:\n","            # Fallback to original order if cycles detected\n","            return [step.id for step in steps]\n","\n","    def _create_fallback_plan(self, query: str, query_type: QueryType) -> ExecutionPlan:\n","        \"\"\"Create simple fallback plan when decomposition fails\"\"\"\n","        fallback_step = PlanningStep(\n","            id=\"fallback_step\",\n","            description=f\"Process query: {query}\",\n","            step_type=\"general\",\n","            tools_required=[\"web_search\", \"calculator\"],\n","            expected_output=\"Response to user query\"\n","        )\n","\n","        return ExecutionPlan(\n","            query=query,\n","            query_type=query_type,\n","            steps=[fallback_step],\n","            execution_order=[\"fallback_step\"]\n","        )\n","\n","class PlanExecutor:\n","    \"\"\"Execute multi-step plans with monitoring and adaptation\"\"\"\n","\n","    def __init__(self, llm, tools: List):\n","        self.llm = llm\n","        self.tools = {tool.name: tool for tool in tools}\n","        self.execution_history = []\n","        self.active_plans = {}\n","\n","    def execute_plan(self, plan: ExecutionPlan, adaptive: bool = True) -> Dict[str, Any]:\n","        \"\"\"Execute a complete plan with optional adaptation\"\"\"\n","        print(f\"\\nðŸŽ¯ Executing plan for: {plan.query}\")\n","        print(f\"Plan type: {plan.query_type.value}\")\n","        print(f\"Total steps: {len(plan.steps)}\")\n","        print(f\"Estimated time: {plan.total_estimated_time}s\")\n","\n","        execution_results = {\n","            \"plan_id\": id(plan),\n","            \"query\": plan.query,\n","            \"steps_completed\": 0,\n","            \"steps_failed\": 0,\n","            \"total_steps\": len(plan.steps),\n","            \"start_time\": datetime.now(),\n","            \"step_results\": {},\n","            \"final_synthesis\": None,\n","            \"success\": False\n","        }\n","\n","        self.active_plans[id(plan)] = plan\n","\n","        try:\n","            # Execute steps in order\n","            for step_id in plan.execution_order:\n","                step = next(s for s in plan.steps if s.id == step_id)\n","\n","                # Check dependencies\n","                if not self._dependencies_satisfied(step, execution_results[\"step_results\"]):\n","                    print(f\"â¸ï¸ Skipping {step_id} - dependencies not satisfied\")\n","                    continue\n","\n","                print(f\"\\nâ–¶ï¸ Executing step: {step.description}\")\n","                step.status = \"in_progress\"\n","\n","                step_result = self._execute_step(step, execution_results[\"step_results\"])\n","\n","                if step_result[\"success\"]:\n","                    step.status = \"completed\"\n","                    step.result = step_result[\"result\"]\n","                    step.confidence = step_result.get(\"confidence\", 0.0)\n","                    execution_results[\"steps_completed\"] += 1\n","                    print(f\"âœ… Step completed with confidence: {step.confidence:.2f}\")\n","                else:\n","                    step.status = \"failed\"\n","                    execution_results[\"steps_failed\"] += 1\n","                    print(f\"âŒ Step failed: {step_result.get('error', 'Unknown error')}\")\n","\n","                    if adaptive:\n","                        # Attempt adaptation\n","                        adaptation_result = self._adapt_plan(plan, step, step_result)\n","                        if adaptation_result:\n","                            print(f\"ðŸ”„ Plan adapted successfully\")\n","\n","                execution_results[\"step_results\"][step_id] = step_result\n","\n","            # Synthesize final results\n","            if execution_results[\"steps_completed\"] > 0:\n","                synthesis_result = self._synthesize_results(plan, execution_results[\"step_results\"])\n","                execution_results[\"final_synthesis\"] = synthesis_result\n","                execution_results[\"success\"] = True\n","\n","            execution_results[\"end_time\"] = datetime.now()\n","            execution_results[\"total_time\"] = (execution_results[\"end_time\"] - execution_results[\"start_time\"]).total_seconds()\n","\n","            self.execution_history.append(execution_results)\n","\n","            return execution_results\n","\n","        except Exception as e:\n","            print(f\"âŒ Plan execution failed: {e}\")\n","            execution_results[\"error\"] = str(e)\n","            execution_results[\"end_time\"] = datetime.now()\n","            return execution_results\n","\n","        finally:\n","            if id(plan) in self.active_plans:\n","                del self.active_plans[id(plan)]\n","\n","    def _dependencies_satisfied(self, step: PlanningStep, completed_results: Dict) -> bool:\n","        \"\"\"Check if all step dependencies are satisfied\"\"\"\n","        return all(dep in completed_results and\n","                  completed_results[dep].get(\"success\", False)\n","                  for dep in step.dependencies)\n","\n","    def _execute_step(self, step: PlanningStep, context: Dict) -> Dict[str, Any]:\n","        \"\"\"Execute individual planning step\"\"\"\n","        try:\n","            # Select appropriate tool\n","            if not step.tools_required:\n","                # Use general reasoning if no specific tools required\n","                return self._general_reasoning_step(step, context)\n","\n","            # Try each required tool until success\n","            for tool_name in step.tools_required:\n","                if tool_name in self.tools:\n","                    tool = self.tools[tool_name]\n","\n","                    # Prepare input for tool\n","                    tool_input = self._prepare_tool_input(step, context, tool_name)\n","\n","                    try:\n","                        result = tool.func(tool_input)\n","                        confidence = self._estimate_confidence(result, step)\n","\n","                        return {\n","                            \"success\": True,\n","                            \"result\": result,\n","                            \"tool_used\": tool_name,\n","                            \"confidence\": confidence,\n","                            \"step_id\": step.id\n","                        }\n","                    except Exception as tool_error:\n","                        print(f\"Tool {tool_name} failed: {tool_error}\")\n","                        continue\n","\n","            return {\n","                \"success\": False,\n","                \"error\": f\"No working tools available from: {step.tools_required}\",\n","                \"step_id\": step.id\n","            }\n","\n","        except Exception as e:\n","            return {\n","                \"success\": False,\n","                \"error\": str(e),\n","                \"step_id\": step.id\n","            }\n","\n","    def _prepare_tool_input(self, step: PlanningStep, context: Dict, tool_name: str) -> str:\n","        \"\"\"Prepare appropriate input for specific tool based on step and context\"\"\"\n","        # Extract relevant context from previous steps\n","        context_summary = \"\"\n","        if context:\n","            context_summary = \"\\n\".join([\n","                f\"Previous step result: {result.get('result', '')[:200]}...\"\n","                for result in context.values() if result.get(\"success\")\n","            ])\n","\n","        # Create tool-specific input\n","        if tool_name == \"web_search_tool\":\n","            return step.description.replace(\"Search for\", \"\").replace(\"Find\", \"\").strip()\n","        elif tool_name == \"calculator_tool\":\n","            # Extract calculation from description\n","            return step.description\n","        else:\n","            # Generic input\n","            return f\"{step.description}\\n\\nContext: {context_summary}\"\n","\n","    def _general_reasoning_step(self, step: PlanningStep, context: Dict) -> Dict[str, Any]:\n","        \"\"\"Execute step using general LLM reasoning\"\"\"\n","        reasoning_prompt = f\"\"\"\n","        Execute this reasoning step: {step.description}\n","\n","        Context from previous steps:\n","        {json.dumps({k: v.get('result', '') for k, v in context.items()}, indent=2)}\n","\n","        Expected output: {step.expected_output}\n","\n","        Provide a clear, specific response.\n","        \"\"\"\n","\n","        try:\n","            response = self.llm.invoke(reasoning_prompt)\n","            return {\n","                \"success\": True,\n","                \"result\": response.content,\n","                \"tool_used\": \"llm_reasoning\",\n","                \"confidence\": 0.7,  # Default confidence for reasoning\n","                \"step_id\": step.id\n","            }\n","        except Exception as e:\n","            return {\n","                \"success\": False,\n","                \"error\": str(e),\n","                \"step_id\": step.id\n","            }\n","\n","    def _estimate_confidence(self, result: str, step: PlanningStep) -> float:\n","        \"\"\"Estimate confidence in step result\"\"\"\n","        # Simple heuristic-based confidence estimation\n","        confidence = 0.5  # Base confidence\n","\n","        # Length-based confidence (longer results often more informative)\n","        if len(str(result)) > 100:\n","            confidence += 0.2\n","\n","        # Step type based confidence\n","        if step.step_type in [\"search\", \"calculate\"]:\n","            confidence += 0.2\n","        elif step.step_type in [\"analyze\", \"synthesize\"]:\n","            confidence += 0.1\n","\n","        # Keyword-based confidence indicators\n","        positive_indicators = [\"found\", \"discovered\", \"confirmed\", \"verified\", \"calculated\"]\n","        negative_indicators = [\"not found\", \"unclear\", \"uncertain\", \"failed\", \"error\"]\n","\n","        result_lower = str(result).lower()\n","        for indicator in positive_indicators:\n","            if indicator in result_lower:\n","                confidence += 0.1\n","                break\n","\n","        for indicator in negative_indicators:\n","            if indicator in result_lower:\n","                confidence -= 0.2\n","                break\n","\n","        return max(0.0, min(1.0, confidence))\n","\n","    def _adapt_plan(self, plan: ExecutionPlan, failed_step: PlanningStep, error_result: Dict) -> bool:\n","        \"\"\"Adapt plan when step fails\"\"\"\n","        adaptation_prompt = f\"\"\"\n","        A planning step failed. Suggest adaptations:\n","\n","        Failed step: {failed_step.description}\n","        Error: {error_result.get('error', 'Unknown error')}\n","        Original tools: {failed_step.tools_required}\n","        Available tools: {list(self.tools.keys())}\n","\n","        Suggest:\n","        1. Alternative tools to try\n","        2. Modified step description\n","        3. Whether to skip this step\n","\n","        Respond with JSON: {{\"action\": \"retry|modify|skip\", \"new_tools\": [], \"new_description\": \"\"}}\n","        \"\"\"\n","\n","        try:\n","            response = self.llm.invoke(adaptation_prompt)\n","            adaptation = json.loads(response.content)\n","\n","            if adaptation[\"action\"] == \"retry\" and adaptation[\"new_tools\"]:\n","                failed_step.tools_required = adaptation[\"new_tools\"]\n","                return True\n","            elif adaptation[\"action\"] == \"modify\":\n","                failed_step.description = adaptation.get(\"new_description\", failed_step.description)\n","                failed_step.tools_required = adaptation.get(\"new_tools\", failed_step.tools_required)\n","                return True\n","\n","        except Exception as e:\n","            print(f\"Adaptation failed: {e}\")\n","\n","        return False\n","\n","    def _synthesize_results(self, plan: ExecutionPlan, step_results: Dict) -> str:\n","        \"\"\"Synthesize results from all completed steps\"\"\"\n","        synthesis_prompt = f\"\"\"\n","        Synthesize the following step results into a comprehensive answer for the original query.\n","\n","        Original Query: {plan.query}\n","\n","        Step Results:\n","        {json.dumps({k: v.get('result', '') for k, v in step_results.items() if v.get('success')}, indent=2)}\n","\n","        Provide a clear, comprehensive answer that:\n","        1. Directly addresses the original query\n","        2. Integrates information from multiple steps\n","        3. Highlights key findings and insights\n","        4. Notes any limitations or uncertainties\n","        \"\"\"\n","\n","        try:\n","            response = self.llm.invoke(synthesis_prompt)\n","            return response.content\n","        except Exception as e:\n","            return f\"Synthesis failed: {e}\""],"metadata":{"id":"nlL8O5lLxUMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 2: Memory Systems and Context Management (13.5)**"],"metadata":{"id":"ysc9nJS0xfdv"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 2: MEMORY SYSTEMS AND CONTEXT MANAGEMENT\")\n","print(\"=\"*60)\n","\n","class MemoryType(Enum):\n","    \"\"\"Types of memory in agentic systems\"\"\"\n","    WORKING = \"working\"          # Short-term task memory\n","    EPISODIC = \"episodic\"        # Memory of past interactions\n","    SEMANTIC = \"semantic\"        # Factual knowledge\n","    PROCEDURAL = \"procedural\"    # Learned procedures and strategies\n","\n","@dataclass\n","class MemoryEntry:\n","    \"\"\"Individual memory entry\"\"\"\n","    id: str\n","    memory_type: MemoryType\n","    content: Any\n","    context: Dict[str, Any]\n","    timestamp: datetime\n","    access_count: int = 0\n","    last_accessed: Optional[datetime] = None\n","    importance: float = 0.5\n","    tags: List[str] = field(default_factory=list)\n","\n","    def access(self):\n","        \"\"\"Record memory access\"\"\"\n","        self.access_count += 1\n","        self.last_accessed = datetime.now()\n","\n","class MemoryManager:\n","    \"\"\"Advanced memory management system for agentic RAG\"\"\"\n","\n","    def __init__(self, db_path: str = \":memory:\", max_working_memory: int = 50):\n","        self.db_path = db_path\n","        self.max_working_memory = max_working_memory\n","        self.working_memory = {}  # Recent context\n","        self.importance_threshold = 0.3\n","\n","        # Initialize database\n","        self._init_database()\n","\n","        # Memory consolidation settings\n","        self.consolidation_interval = timedelta(hours=1)\n","        self.last_consolidation = datetime.now()\n","\n","    def _init_database(self):\n","        \"\"\"Initialize SQLite database for persistent memory\"\"\"\n","        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n","        self.conn.execute(\"\"\"\n","            CREATE TABLE IF NOT EXISTS memory_entries (\n","                id TEXT PRIMARY KEY,\n","                memory_type TEXT NOT NULL,\n","                content TEXT NOT NULL,\n","                context TEXT,\n","                timestamp TEXT NOT NULL,\n","                access_count INTEGER DEFAULT 0,\n","                last_accessed TEXT,\n","                importance REAL DEFAULT 0.5,\n","                tags TEXT\n","            )\n","        \"\"\")\n","\n","        self.conn.execute(\"\"\"\n","            CREATE TABLE IF NOT EXISTS memory_relationships (\n","                id INTEGER PRIMARY KEY AUTOINCREMENT,\n","                source_id TEXT NOT NULL,\n","                target_id TEXT NOT NULL,\n","                relationship_type TEXT NOT NULL,\n","                strength REAL DEFAULT 1.0,\n","                created_at TEXT NOT NULL,\n","                FOREIGN KEY (source_id) REFERENCES memory_entries (id),\n","                FOREIGN KEY (target_id) REFERENCES memory_entries (id)\n","            )\n","        \"\"\")\n","\n","        self.conn.commit()\n","\n","    def store_memory(self, memory_type: MemoryType, content: Any,\n","                    context: Dict = None, tags: List[str] = None) -> str:\n","        \"\"\"Store new memory entry\"\"\"\n","        memory_id = hashlib.md5(f\"{content}{datetime.now()}\".encode()).hexdigest()\n","\n","        entry = MemoryEntry(\n","            id=memory_id,\n","            memory_type=memory_type,\n","            content=content,\n","            context=context or {},\n","            timestamp=datetime.now(),\n","            tags=tags or []\n","        )\n","\n","        # Store in working memory if appropriate\n","        if memory_type == MemoryType.WORKING:\n","            self.working_memory[memory_id] = entry\n","            self._manage_working_memory_size()\n","\n","        # Store in persistent database\n","        self._store_persistent_memory(entry)\n","\n","        return memory_id\n","\n","    def _store_persistent_memory(self, entry: MemoryEntry):\n","        \"\"\"Store memory entry in persistent database\"\"\"\n","        self.conn.execute(\"\"\"\n","            INSERT OR REPLACE INTO memory_entries\n","            (id, memory_type, content, context, timestamp, access_count,\n","             last_accessed, importance, tags)\n","            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n","        \"\"\", (\n","            entry.id,\n","            entry.memory_type.value,\n","            json.dumps(entry.content) if not isinstance(entry.content, str) else entry.content,\n","            json.dumps(entry.context),\n","            entry.timestamp.isoformat(),\n","            entry.access_count,\n","            entry.last_accessed.isoformat() if entry.last_accessed else None,\n","            entry.importance,\n","            json.dumps(entry.tags)\n","        ))\n","        self.conn.commit()\n","\n","    def retrieve_memory(self, query: str, memory_types: List[MemoryType] = None,\n","                       limit: int = 10) -> List[MemoryEntry]:\n","        \"\"\"Retrieve relevant memories based on query\"\"\"\n","        if memory_types is None:\n","            memory_types = list(MemoryType)\n","\n","        type_filter = \" OR \".join([f\"memory_type = '{mt.value}'\" for mt in memory_types])\n","\n","        # Simple keyword matching (in production, use vector similarity)\n","        query_words = query.lower().split()\n","\n","        results = self.conn.execute(f\"\"\"\n","            SELECT * FROM memory_entries\n","            WHERE ({type_filter})\n","            ORDER BY importance DESC, access_count DESC, timestamp DESC\n","            LIMIT ?\n","        \"\"\", (limit * 2,)).fetchall()  # Get more to filter\n","\n","        # Convert to MemoryEntry objects and filter by relevance\n","        relevant_memories = []\n","        for row in results:\n","            entry = self._row_to_memory_entry(row)\n","\n","            # Simple relevance scoring\n","            content_lower = str(entry.content).lower()\n","            context_lower = json.dumps(entry.context).lower()\n","            tags_lower = \" \".join(entry.tags).lower()\n","\n","            relevance_score = 0\n","            for word in query_words:\n","                if word in content_lower:\n","                    relevance_score += 2\n","                if word in context_lower:\n","                    relevance_score += 1\n","                if word in tags_lower:\n","                    relevance_score += 1\n","\n","            if relevance_score > 0 or entry.memory_type == MemoryType.WORKING:\n","                entry.access()  # Record access\n","                self._update_memory_access(entry)\n","                relevant_memories.append(entry)\n","\n","        return sorted(relevant_memories,\n","                     key=lambda x: (x.importance, x.access_count),\n","                     reverse=True)[:limit]\n","\n","    def _row_to_memory_entry(self, row) -> MemoryEntry:\n","        \"\"\"Convert database row to MemoryEntry object\"\"\"\n","        return MemoryEntry(\n","            id=row[0],\n","            memory_type=MemoryType(row[1]),\n","            content=json.loads(row[2]) if row[2].startswith(('{', '[')) else row[2],\n","            context=json.loads(row[3]) if row[3] else {},\n","            timestamp=datetime.fromisoformat(row[4]),\n","            access_count=row[5] or 0,\n","            last_accessed=datetime.fromisoformat(row[6]) if row[6] else None,\n","            importance=row[7] or 0.5,\n","            tags=json.loads(row[8]) if row[8] else []\n","        )\n","\n","    def _update_memory_access(self, entry: MemoryEntry):\n","        \"\"\"Update memory access statistics in database\"\"\"\n","        self.conn.execute(\"\"\"\n","            UPDATE memory_entries\n","            SET access_count = ?, last_accessed = ?\n","            WHERE id = ?\n","        \"\"\", (entry.access_count, entry.last_accessed.isoformat(), entry.id))\n","        self.conn.commit()\n","\n","    def _manage_working_memory_size(self):\n","        \"\"\"Manage working memory size by removing least important entries\"\"\"\n","        if len(self.working_memory) > self.max_working_memory:\n","            # Sort by importance and recency\n","            sorted_entries = sorted(\n","                self.working_memory.items(),\n","                key=lambda x: (x[1].importance, x[1].timestamp),\n","                reverse=True\n","            )\n","\n","            # Keep most important entries\n","            keep_entries = dict(sorted_entries[:self.max_working_memory])\n","\n","            # Move removed entries to long-term memory if important enough\n","            for entry_id, entry in self.working_memory.items():\n","                if entry_id not in keep_entries and entry.importance > self.importance_threshold:\n","                    # Convert to episodic memory\n","                    entry.memory_type = MemoryType.EPISODIC\n","                    self._store_persistent_memory(entry)\n","\n","            self.working_memory = keep_entries\n","\n","    def consolidate_memory(self, llm):\n","        \"\"\"Periodic memory consolidation to extract insights and patterns\"\"\"\n","        if datetime.now() - self.last_consolidation < self.consolidation_interval:\n","            return\n","\n","        print(\"ðŸ§  Starting memory consolidation...\")\n","\n","        # Get recent memories for consolidation\n","        recent_cutoff = datetime.now() - timedelta(hours=24)\n","        recent_memories = self.conn.execute(\"\"\"\n","            SELECT * FROM memory_entries\n","            WHERE timestamp > ? AND memory_type IN ('working', 'episodic')\n","            ORDER BY importance DESC, access_count DESC\n","            LIMIT 20\n","        \"\"\", (recent_cutoff.isoformat(),)).fetchall()\n","\n","        if len(recent_memories) < 3:\n","            return\n","\n","        # Extract patterns and insights\n","        memory_contents = []\n","        for row in recent_memories:\n","            entry = self._row_to_memory_entry(row)\n","            memory_contents.append({\n","                \"content\": entry.content,\n","                \"context\": entry.context,\n","                \"tags\": entry.tags\n","            })\n","\n","        consolidation_prompt = f\"\"\"\n","        Analyze these recent memory entries and extract:\n","        1. Common patterns or themes\n","        2. Important facts that should be remembered long-term\n","        3. Procedural knowledge or strategies that worked well\n","        4. Connections between different pieces of information\n","\n","        Memory entries:\n","        {json.dumps(memory_contents, indent=2)}\n","\n","        Provide response as JSON:\n","        {{\n","            \"patterns\": [\"pattern1\", \"pattern2\"],\n","            \"important_facts\": [\"fact1\", \"fact2\"],\n","            \"procedures\": [\"procedure1\", \"procedure2\"],\n","            \"connections\": [[\"item1\", \"item2\", \"relationship\"]]\n","        }}\n","        \"\"\"\n","\n","        try:\n","            response = llm.invoke(consolidation_prompt)\n","            insights = json.loads(response.content)\n","\n","            # Store consolidated insights as semantic memory\n","            for pattern in insights.get(\"patterns\", []):\n","                self.store_memory(\n","                    MemoryType.SEMANTIC,\n","                    f\"Pattern: {pattern}\",\n","                    {\"source\": \"consolidation\", \"type\": \"pattern\"},\n","                    [\"pattern\", \"insight\"]\n","                )\n","\n","            for fact in insights.get(\"important_facts\", []):\n","                self.store_memory(\n","                    MemoryType.SEMANTIC,\n","                    f\"Fact: {fact}\",\n","                    {\"source\": \"consolidation\", \"type\": \"fact\"},\n","                    [\"fact\", \"knowledge\"]\n","                )\n","\n","            for procedure in insights.get(\"procedures\", []):\n","                self.store_memory(\n","                    MemoryType.PROCEDURAL,\n","                    f\"Procedure: {procedure}\",\n","                    {\"source\": \"consolidation\", \"type\": \"procedure\"},\n","                    [\"procedure\", \"strategy\"]\n","                )\n","\n","            # Create memory relationships\n","            for connection in insights.get(\"connections\", []):\n","                if len(connection) >= 3:\n","                    self._create_memory_relationship(connection[0], connection[1], connection[2])\n","\n","            self.last_consolidation = datetime.now()\n","            print(f\"âœ… Consolidated {len(recent_memories)} memories into {len(insights.get('patterns', [])) + len(insights.get('important_facts', [])) + len(insights.get('procedures', []))} insights\")\n","\n","        except Exception as e:\n","            print(f\"âŒ Memory consolidation failed: {e}\")\n","\n","    def _create_memory_relationship(self, source_content: str, target_content: str, relationship: str):\n","        \"\"\"Create relationship between memory entries\"\"\"\n","        # Find memory IDs by content (simplified - in production use better matching)\n","        source_ids = self.conn.execute(\"\"\"\n","            SELECT id FROM memory_entries WHERE content LIKE ?\n","        \"\"\", (f\"%{source_content}%\",)).fetchall()\n","\n","        target_ids = self.conn.execute(\"\"\"\n","            SELECT id FROM memory_entries WHERE content LIKE ?\n","        \"\"\", (f\"%{target_content}%\",)).fetchall()\n","\n","        if source_ids and target_ids:\n","            self.conn.execute(\"\"\"\n","                INSERT INTO memory_relationships\n","                (source_id, target_id, relationship_type, created_at)\n","                VALUES (?, ?, ?, ?)\n","            \"\"\", (source_ids[0][0], target_ids[0][0], relationship, datetime.now().isoformat()))\n","            self.conn.commit()\n","\n","    def get_memory_context(self, query: str, max_entries: int = 5) -> str:\n","        \"\"\"Get formatted memory context for agent prompts\"\"\"\n","        relevant_memories = self.retrieve_memory(query, limit=max_entries)\n","\n","        if not relevant_memories:\n","            return \"No relevant previous context found.\"\n","\n","        context_parts = []\n","        context_parts.append(\"=== RELEVANT MEMORY CONTEXT ===\")\n","\n","        for memory in relevant_memories:\n","            context_parts.append(f\"\\n[{memory.memory_type.value.upper()}] {memory.content}\")\n","            if memory.tags:\n","                context_parts.append(f\"Tags: {', '.join(memory.tags)}\")\n","\n","        context_parts.append(\"\\n=== END CONTEXT ===\")\n","\n","        return \"\\n\".join(context_parts)\n","\n","    def get_memory_statistics(self) -> Dict[str, Any]:\n","        \"\"\"Get comprehensive memory system statistics\"\"\"\n","        stats = {}\n","\n","        # Count by memory type\n","        for memory_type in MemoryType:\n","            count = self.conn.execute(\"\"\"\n","                SELECT COUNT(*) FROM memory_entries WHERE memory_type = ?\n","            \"\"\", (memory_type.value,)).fetchone()[0]\n","            stats[f\"{memory_type.value}_count\"] = count\n","\n","        # Working memory stats\n","        stats[\"working_memory_size\"] = len(self.working_memory)\n","        stats[\"working_memory_max\"] = self.max_working_memory\n","\n","        # Access patterns\n","        most_accessed = self.conn.execute(\"\"\"\n","            SELECT content, access_count FROM memory_entries\n","            ORDER BY access_count DESC LIMIT 3\n","        \"\"\").fetchall()\n","        stats[\"most_accessed\"] = most_accessed\n","\n","        # Recent activity\n","        recent_count = self.conn.execute(\"\"\"\n","            SELECT COUNT(*) FROM memory_entries\n","            WHERE timestamp > ?\n","        \"\"\", ((datetime.now() - timedelta(hours=24)).isoformat(),)).fetchone()[0]\n","        stats[\"recent_memories\"] = recent_count\n","\n","        return stats\n","\n","class ContextOptimizer:\n","    \"\"\"Optimize context for agent prompts based on memory and current task\"\"\"\n","\n","    def __init__(self, memory_manager: MemoryManager, max_context_length: int = 4000):\n","        self.memory_manager = memory_manager\n","        self.max_context_length = max_context_length\n","\n","    def optimize_context(self, query: str, conversation_history: List[Dict] = None) -> str:\n","        \"\"\"Create optimized context for current query\"\"\"\n","        context_parts = []\n","        current_length = 0\n","\n","        # Add conversation history (most recent first)\n","        if conversation_history:\n","            context_parts.append(\"=== RECENT CONVERSATION ===\")\n","            for entry in reversed(conversation_history[-3:]):  # Last 3 exchanges\n","                entry_text = f\"User: {entry.get('user', '')}\\nAssistant: {entry.get('assistant', '')}\\n\"\n","                if current_length + len(entry_text) < self.max_context_length // 2:\n","                    context_parts.append(entry_text)\n","                    current_length += len(entry_text)\n","\n","        # Add relevant memories\n","        memory_context = self.memory_manager.get_memory_context(query)\n","        if current_length + len(memory_context) < self.max_context_length:\n","            context_parts.append(memory_context)\n","            current_length += len(memory_context)\n","        else:\n","            # Truncate memory context if needed\n","            available_space = self.max_context_length - current_length - 100  # Buffer\n","            if available_space > 200:\n","                truncated_context = memory_context[:available_space] + \"... [truncated]\"\n","                context_parts.append(truncated_context)\n","\n","        return \"\\n\\n\".join(context_parts)\n"],"metadata":{"id":"mX3bmRiJxflc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 3: Integration - Memory-Enhanced Planning Agent**"],"metadata":{"id":"VOxb5ntmxqMa"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 3: MEMORY-ENHANCED PLANNING AGENT\")\n","print(\"=\"*60)\n","\n","# Import tools from previous notebook (simplified versions)\n","@tool\n","def web_search_tool(query: str) -> str:\n","    \"\"\"Search the web for information\"\"\"\n","    # Simplified web search simulation\n","    search_results = {\n","        \"quantum computing\": \"Recent advances in quantum error correction and IBM's 1000-qubit roadmap\",\n","        \"AI development\": \"Latest developments in transformer architectures and multimodal AI systems\",\n","        \"stock market\": \"Current market trends show volatility in tech stocks with inflation concerns\",\n","        \"climate change\": \"New IPCC report highlights urgent need for emission reductions\"\n","    }\n","\n","    for topic, result in search_results.items():\n","        if topic.lower() in query.lower():\n","            return f\"Search results for '{query}': {result}\"\n","\n","    return f\"Search results for '{query}': General information found, but no specific match in simulation.\"\n","\n","@tool\n","def calculator_tool(expression: str) -> str:\n","    \"\"\"Perform calculations\"\"\"\n","    try:\n","        # Simple whitelist for safety\n","        allowed_chars = set('0123456789+-*/()., ')\n","        if not all(c in allowed_chars for c in expression):\n","            return \"Error: Invalid characters in expression\"\n","\n","        result = eval(expression, {\"__builtins__\": {}}, {})\n","        return f\"Calculation result: {result}\"\n","    except Exception as e:\n","        return f\"Calculation error: {e}\"\n","\n","@tool\n","def data_analysis_tool(data_description: str) -> str:\n","    \"\"\"Analyze data trends and patterns\"\"\"\n","    analysis_responses = {\n","        \"trend\": \"Analysis shows upward trend with 15% growth over the period\",\n","        \"correlation\": \"Strong positive correlation (r=0.85) between variables\",\n","        \"forecast\": \"Predictive model indicates continued growth with 70% confidence\",\n","        \"comparison\": \"Comparative analysis reveals significant differences between groups\"\n","    }\n","\n","    for key, response in analysis_responses.items():\n","        if key in data_description.lower():\n","            return f\"Data analysis: {response}\"\n","\n","    return f\"Data analysis completed for: {data_description}\"\n","\n","class MemoryEnhancedPlanningAgent:\n","    \"\"\"Advanced agent combining planning, execution, and memory management\"\"\"\n","\n","    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n","        self.llm = ChatOpenAI(model=model_name, temperature=0.1)\n","        self.memory_manager = MemoryManager()\n","        self.context_optimizer = ContextOptimizer(self.memory_manager)\n","        self.query_decomposer = QueryDecomposer(self.llm)\n","\n","        # Available tools\n","        self.tools = [web_search_tool, calculator_tool, data_analysis_tool]\n","        self.plan_executor = PlanExecutor(self.llm, self.tools)\n","\n","        # Conversation history\n","        self.conversation_history = []\n","\n","    def process_query(self, query: str, use_memory: bool = True, adaptive_planning: bool = True) -> Dict[str, Any]:\n","        \"\"\"Process query with full planning, execution, and memory integration\"\"\"\n","        print(f\"\\nðŸŽ¯ Processing query: {query}\")\n","\n","        # Store query in working memory\n","        if use_memory:\n","            self.memory_manager.store_memory(\n","                MemoryType.WORKING,\n","                f\"User query: {query}\",\n","                {\"timestamp\": datetime.now().isoformat(), \"type\": \"user_query\"},\n","                [\"query\", \"user_input\"]\n","            )\n","\n","        # Get optimized context\n","        context = self.context_optimizer.optimize_context(query, self.conversation_history) if use_memory else \"\"\n","\n","        # Create execution plan\n","        print(\"ðŸ“‹ Creating execution plan...\")\n","        plan = self.query_decomposer.decompose_query(query)\n","\n","        # Store plan in memory\n","        if use_memory:\n","            self.memory_manager.store_memory(\n","                MemoryType.WORKING,\n","                f\"Execution plan: {len(plan.steps)} steps\",\n","                {\"plan_type\": plan.query_type.value, \"steps\": len(plan.steps)},\n","                [\"plan\", \"execution\"]\n","            )\n","\n","        # Execute plan\n","        print(\"âš¡ Executing plan...\")\n","        execution_result = self.plan_executor.execute_plan(plan, adaptive=adaptive_planning)\n","\n","        # Store execution results in memory\n","        if use_memory and execution_result[\"success\"]:\n","            self.memory_manager.store_memory(\n","                MemoryType.EPISODIC,\n","                f\"Successfully completed query: {query}\",\n","                {\n","                    \"query\": query,\n","                    \"steps_completed\": execution_result[\"steps_completed\"],\n","                    \"total_time\": execution_result.get(\"total_time\", 0),\n","                    \"final_answer\": execution_result.get(\"final_synthesis\", \"\")\n","                },\n","                [\"success\", \"completion\", plan.query_type.value]\n","            )\n","\n","            # Store successful strategies as procedural memory\n","            if execution_result[\"steps_completed\"] >= 2:\n","                successful_strategy = f\"For {plan.query_type.value} queries, use {execution_result['steps_completed']}-step approach\"\n","                self.memory_manager.store_memory(\n","                    MemoryType.PROCEDURAL,\n","                    successful_strategy,\n","                    {\"query_type\": plan.query_type.value, \"success_rate\": 1.0},\n","                    [\"strategy\", \"procedure\", \"successful\"]\n","                )\n","\n","        # Update conversation history\n","        self.conversation_history.append({\n","            \"user\": query,\n","            \"assistant\": execution_result.get(\"final_synthesis\", \"No response generated\"),\n","            \"timestamp\": datetime.now().isoformat()\n","        })\n","\n","        # Periodic memory consolidation\n","        if len(self.conversation_history) % 5 == 0:  # Every 5 interactions\n","            self.memory_manager.consolidate_memory(self.llm)\n","\n","        return {\n","            \"query\": query,\n","            \"plan\": plan,\n","            \"execution_result\": execution_result,\n","            \"memory_context_used\": bool(context),\n","            \"conversation_turn\": len(self.conversation_history)\n","        }\n","\n","    def get_system_status(self) -> Dict[str, Any]:\n","        \"\"\"Get comprehensive system status\"\"\"\n","        return {\n","            \"memory_stats\": self.memory_manager.get_memory_statistics(),\n","            \"conversation_turns\": len(self.conversation_history),\n","            \"execution_history\": len(self.plan_executor.execution_history),\n","            \"active_plans\": len(self.plan_executor.active_plans)\n","        }\n","\n","    def export_memory_insights(self) -> Dict[str, Any]:\n","        \"\"\"Export insights from accumulated memory\"\"\"\n","        insights = {\n","            \"successful_strategies\": [],\n","            \"common_patterns\": [],\n","            \"learned_facts\": [],\n","            \"performance_metrics\": {}\n","        }\n","\n","        # Get procedural memories (strategies)\n","        procedural_memories = self.memory_manager.retrieve_memory(\n","            \"strategy procedure\",\n","            [MemoryType.PROCEDURAL],\n","            limit=10\n","        )\n","        insights[\"successful_strategies\"] = [mem.content for mem in procedural_memories]\n","\n","        # Get semantic memories (facts and patterns)\n","        semantic_memories = self.memory_manager.retrieve_memory(\n","            \"pattern fact\",\n","            [MemoryType.SEMANTIC],\n","            limit=10\n","        )\n","        for mem in semantic_memories:\n","            if \"Pattern:\" in str(mem.content):\n","                insights[\"common_patterns\"].append(mem.content)\n","            elif \"Fact:\" in str(mem.content):\n","                insights[\"learned_facts\"].append(mem.content)\n","\n","        # Calculate performance metrics\n","        execution_history = self.plan_executor.execution_history\n","        if execution_history:\n","            success_rate = sum(1 for exec in execution_history if exec[\"success\"]) / len(execution_history)\n","            avg_steps = sum(exec[\"steps_completed\"] for exec in execution_history) / len(execution_history)\n","            avg_time = sum(exec.get(\"total_time\", 0) for exec in execution_history) / len(execution_history)\n","\n","            insights[\"performance_metrics\"] = {\n","                \"success_rate\": success_rate,\n","                \"average_steps_per_query\": avg_steps,\n","                \"average_execution_time\": avg_time,\n","                \"total_queries_processed\": len(execution_history)\n","            }\n","\n","        return insights"],"metadata":{"id":"VBRaYs9wxqUX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 4: Demonstration and Testing**"],"metadata":{"id":"TsT11mv8xzAp"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 4: DEMONSTRATION AND TESTING\")\n","print(\"=\"*60)\n","\n","# Initialize the memory-enhanced planning agent\n","print(\"ðŸš€ Initializing Memory-Enhanced Planning Agent...\")\n","agent = MemoryEnhancedPlanningAgent()\n","\n","# Test queries to demonstrate multi-step reasoning and memory\n","test_queries = [\n","    {\n","        \"query\": \"What are the latest developments in quantum computing and how might they impact the semiconductor industry?\",\n","        \"description\": \"Complex research query requiring multi-step investigation and synthesis\"\n","    },\n","    {\n","        \"query\": \"If a quantum computer can perform certain calculations 1000x faster than classical computers, and a classical computer takes 10 hours for a specific task, how long would the quantum computer take? What are the implications for cryptography?\",\n","        \"description\": \"Mixed analytical query combining calculation and reasoning\"\n","    },\n","    {\n","        \"query\": \"Based on our previous discussion about quantum computing, what should investors consider when evaluating quantum technology companies?\",\n","        \"description\": \"Context-dependent query that should leverage memory\"\n","    }\n","]\n","\n","# Execute test queries\n","print(\"\\nðŸ§ª Running test queries...\")\n","for i, test in enumerate(test_queries, 1):\n","    print(f\"\\n{'='*60}\")\n","    print(f\"TEST QUERY {i}: {test['description']}\")\n","    print(f\"{'='*60}\")\n","\n","    result = agent.process_query(test[\"query\"])\n","\n","    if result[\"execution_result\"][\"success\"]:\n","        print(f\"\\nâœ… Query completed successfully!\")\n","        print(f\"ðŸ“‹ Plan type: {result['plan'].query_type.value}\")\n","        print(f\"ðŸ”§ Steps executed: {result['execution_result']['steps_completed']}\")\n","        print(f\"â±ï¸ Execution time: {result['execution_result'].get('total_time', 0):.1f}s\")\n","        print(f\"ðŸ§  Memory context used: {result['memory_context_used']}\")\n","        print(f\"\\nðŸ’¡ Final Answer:\\n{result['execution_result']['final_synthesis']}\")\n","    else:\n","        print(f\"\\nâŒ Query failed: {result['execution_result'].get('error', 'Unknown error')}\")\n","\n","    # Show system status\n","    status = agent.get_system_status()\n","    print(f\"\\nðŸ“Š System Status:\")\n","    print(f\"   Memory entries: {sum(status['memory_stats'][k] for k in status['memory_stats'] if k.endswith('_count'))}\")\n","    print(f\"   Conversation turns: {status['conversation_turns']}\")\n","    print(f\"   Working memory: {status['memory_stats']['working_memory_size']}/{status['memory_stats']['working_memory_max']}\")\n"],"metadata":{"id":"HzJKoD7pxzJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 5: Memory Analysis and Visualization**"],"metadata":{"id":"Hvxzrcqfx-c1"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 5: MEMORY ANALYSIS AND VISUALIZATION\")\n","print(\"=\"*60)\n","\n","def visualize_memory_distribution(memory_manager: MemoryManager):\n","    \"\"\"Visualize memory distribution across types\"\"\"\n","    stats = memory_manager.get_memory_statistics()\n","\n","    memory_types = []\n","    counts = []\n","\n","    for memory_type in MemoryType:\n","        key = f\"{memory_type.value}_count\"\n","        if key in stats:\n","            memory_types.append(memory_type.value.title())\n","            counts.append(stats[key])\n","\n","    if not counts:\n","        print(\"No memory data to visualize\")\n","        return\n","\n","    # Create visualization\n","    fig = go.Figure(data=[\n","        go.Bar(\n","            x=memory_types,\n","            y=counts,\n","            marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(counts)]\n","        )\n","    ])\n","\n","    fig.update_layout(\n","        title=\"Memory Distribution by Type\",\n","        xaxis_title=\"Memory Type\",\n","        yaxis_title=\"Number of Entries\",\n","        showlegend=False\n","    )\n","\n","    fig.show()\n","\n","    return fig\n","\n","def analyze_planning_performance(plan_executor: PlanExecutor):\n","    \"\"\"Analyze planning and execution performance\"\"\"\n","    history = plan_executor.execution_history\n","\n","    if not history:\n","        print(\"No execution history to analyze\")\n","        return\n","\n","    # Extract performance data\n","    success_rates = []\n","    execution_times = []\n","    steps_completed = []\n","\n","    for exec in history:\n","        success_rates.append(1 if exec[\"success\"] else 0)\n","        execution_times.append(exec.get(\"total_time\", 0))\n","        steps_completed.append(exec[\"steps_completed\"])\n","\n","    # Create performance dashboard\n","    fig = make_subplots(\n","        rows=2, cols=2,\n","        subplot_titles=('Success Rate Over Time', 'Execution Time Distribution',\n","                       'Steps Completed Distribution', 'Performance Correlation'),\n","        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n","               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n","    )\n","\n","    # Success rate over time\n","    fig.add_trace(\n","        go.Scatter(y=success_rates, mode='lines+markers', name='Success Rate'),\n","        row=1, col=1\n","    )\n","\n","    # Execution time distribution\n","    fig.add_trace(\n","        go.Histogram(x=execution_times, name='Execution Time'),\n","        row=1, col=2\n","    )\n","\n","    # Steps completed distribution\n","    fig.add_trace(\n","        go.Histogram(x=steps_completed, name='Steps Completed'),\n","        row=2, col=1\n","    )\n","\n","    # Performance correlation\n","    fig.add_trace(\n","        go.Scatter(x=steps_completed, y=execution_times, mode='markers',\n","                  name='Steps vs Time'),\n","        row=2, col=2\n","    )\n","\n","    fig.update_layout(height=600, showlegend=False, title_text=\"Planning Performance Analysis\")\n","    fig.show()\n","\n","    # Print summary statistics\n","    print(f\"\\nðŸ“Š Performance Summary:\")\n","    print(f\"   Overall success rate: {sum(success_rates)/len(success_rates):.2%}\")\n","    print(f\"   Average execution time: {sum(execution_times)/len(execution_times):.1f}s\")\n","    print(f\"   Average steps per query: {sum(steps_completed)/len(steps_completed):.1f}\")\n","    print(f\"   Total queries processed: {len(history)}\")\n","\n","    return fig\n","\n","# Generate visualizations\n","print(\"ðŸ“Š Generating memory distribution visualization...\")\n","memory_viz = visualize_memory_distribution(agent.memory_manager)\n","\n","print(\"\\nðŸ“ˆ Analyzing planning performance...\")\n","performance_viz = analyze_planning_performance(agent.plan_executor)\n","\n","# Export memory insights\n","print(\"\\nðŸ§  Exporting memory insights...\")\n","insights = agent.export_memory_insights()\n","\n","print(\"\\nðŸ’¡ Memory Insights Summary:\")\n","print(f\"   Successful strategies learned: {len(insights['successful_strategies'])}\")\n","print(f\"   Common patterns identified: {len(insights['common_patterns'])}\")\n","print(f\"   Facts accumulated: {len(insights['learned_facts'])}\")\n","\n","if insights['performance_metrics']:\n","    metrics = insights['performance_metrics']\n","    print(f\"   Current success rate: {metrics['success_rate']:.2%}\")\n","    print(f\"   Average query complexity: {metrics['average_steps_per_query']:.1f} steps\")\n"],"metadata":{"id":"ifUw54Zmx-ko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 6: Advanced Memory Patterns**"],"metadata":{"id":"OWAbQFOnyHUJ"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"SECTION 6: ADVANCED MEMORY PATTERNS\")\n","print(\"=\"*60)\n","\n","class MemoryPattern:\n","    \"\"\"Advanced memory pattern detection and utilization\"\"\"\n","\n","    def __init__(self, memory_manager: MemoryManager):\n","        self.memory_manager = memory_manager\n","\n","    def detect_query_patterns(self) -> Dict[str, List[str]]:\n","        \"\"\"Detect patterns in user queries\"\"\"\n","        # Get episodic memories (completed interactions)\n","        episodic_memories = self.memory_manager.retrieve_memory(\n","            \"query\", [MemoryType.EPISODIC], limit=50\n","        )\n","\n","        patterns = {\n","            \"recurring_topics\": [],\n","            \"query_complexity_trends\": [],\n","            \"successful_approaches\": []\n","        }\n","\n","        # Simple pattern detection (in production, use more sophisticated NLP)\n","        topic_counts = {}\n","        for memory in episodic_memories:\n","            content = str(memory.content).lower()\n","            # Extract potential topics\n","            if \"quantum\" in content:\n","                topic_counts[\"quantum_computing\"] = topic_counts.get(\"quantum_computing\", 0) + 1\n","            if \"market\" in content or \"stock\" in content:\n","                topic_counts[\"financial_analysis\"] = topic_counts.get(\"financial_analysis\", 0) + 1\n","            if \"ai\" in content or \"machine learning\" in content:\n","                topic_counts[\"artificial_intelligence\"] = topic_counts.get(\"artificial_intelligence\", 0) + 1\n","\n","        # Sort by frequency\n","        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n","        patterns[\"recurring_topics\"] = [f\"{topic}: {count} occurrences\" for topic, count in sorted_topics]\n","\n","        return patterns\n","\n","    def suggest_proactive_insights(self, llm) -> List[str]:\n","        \"\"\"Generate proactive insights based on memory patterns\"\"\"\n","        patterns = self.detect_query_patterns()\n","\n","        if not patterns[\"recurring_topics\"]:\n","            return [\"No sufficient interaction history for proactive insights\"]\n","\n","        insight_prompt = f\"\"\"\n","        Based on the user's interaction patterns, suggest proactive insights or recommendations.\n","\n","        Recurring topics: {patterns[\"recurring_topics\"]}\n","\n","        Provide 3-5 actionable insights or suggestions that might be valuable to the user.\n","        Focus on:\n","        1. Emerging trends in their areas of interest\n","        2. Connections between different topics they've explored\n","        3. Potential next steps for their research or analysis\n","\n","        Format as a simple list.\n","        \"\"\"\n","\n","        try:\n","            response = llm.invoke(insight_prompt)\n","            insights = [line.strip() for line in response.content.split('\\n') if line.strip() and not line.strip().startswith('#')]\n","            return insights[:5]  # Limit to 5 insights\n","        except Exception as e:\n","            return [f\"Could not generate proactive insights: {e}\"]\n","\n","# Test advanced memory patterns\n","print(\"ðŸ§  Testing advanced memory patterns...\")\n","memory_pattern_analyzer = MemoryPattern(agent.memory_manager)\n","\n","patterns = memory_pattern_analyzer.detect_query_patterns()\n","print(f\"\\nðŸ” Detected Patterns:\")\n","for pattern_type, pattern_list in patterns.items():\n","    print(f\"   {pattern_type.replace('_', ' ').title()}:\")\n","    for pattern in pattern_list[:3]:  # Show top 3\n","        print(f\"     - {pattern}\")\n","\n","print(f\"\\nðŸ’¡ Generating proactive insights...\")\n","proactive_insights = memory_pattern_analyzer.suggest_proactive_insights(agent.llm)\n","print(\"   Proactive Insights:\")\n","for insight in proactive_insights:\n","    print(f\"     - {insight}\")\n"],"metadata":{"id":"tt5P6ZCCyHcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**# SECTION 7: Key Takeaways and Next Steps**"],"metadata":{"id":"FcOArbIzyOtv"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"KEY TAKEAWAYS AND NEXT STEPS\")\n","print(\"=\"*60)\n","\n","print(\"\"\"\n","ðŸŽ¯ Key Takeaways from Notebook 13.2:\n","\n","1. QUERY DECOMPOSITION & PLANNING:\n","   - Sophisticated query classification enables appropriate planning strategies\n","   - Dependency-aware execution order optimizes resource utilization\n","   - Adaptive planning allows recovery from failed steps\n","\n","2. MEMORY SYSTEMS:\n","   - Multi-type memory (working, episodic, semantic, procedural) captures different knowledge\n","   - Automatic consolidation extracts patterns and insights from interactions\n","   - Context optimization balances relevance with computational constraints\n","\n","3. INTEGRATION BENEFITS:\n","   - Memory-enhanced planning learns from past successes and failures\n","   - Context-aware execution improves response quality and consistency\n","   - Pattern detection enables proactive assistance\n","\n","4. PERFORMANCE CONSIDERATIONS:\n","   - Memory management requires balance between retention and performance\n","   - Consolidation processes add computational overhead but improve long-term capability\n","   - Context length limits require intelligent selection and truncation\n","\n","ðŸš€ Next Steps:\n","- Explore multi-agent collaboration patterns in Notebook 13.3\n","- Implement production-ready memory backends (Redis, vector databases)\n","- Develop more sophisticated pattern recognition and learning algorithms\n","- Build evaluation frameworks for memory-enhanced agents\n","\n","ðŸ“Š Current System Performance:\n","\"\"\")\n","\n","# Final system status\n","final_status = agent.get_system_status()\n","final_insights = agent.export_memory_insights()\n","\n","print(f\"- Total Queries Processed: {final_insights['performance_metrics'].get('total_queries_processed', 0)}\")\n","print(f\"- Memory Entries: {sum(final_status['memory_stats'][k] for k in final_status['memory_stats'] if k.endswith('_count'))}\")\n","print(f\"- Success Rate: {final_insights['performance_metrics'].get('success_rate', 0):.2%}\")\n","print(f\"- Strategies Learned: {len(final_insights['successful_strategies'])}\")\n","print(f\"- Patterns Identified: {len(final_insights['common_patterns'])}\")\n","\n","print(f\"\\nâœ¨ Notebook Complete! Ready for multi-agent collaboration and production deployment.\")"],"metadata":{"id":"29vry645yO00"},"execution_count":null,"outputs":[]}]}