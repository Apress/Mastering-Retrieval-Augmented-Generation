{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM12KH9uz+VKr9bQTN3C7Ep"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"4Vr8pzcMVpPr"}},{"cell_type":"code","source":["!pip install langchain langchain-openai llama-index haystack-ai tiktoken openai\n","\n","import os\n","import json\n","import tiktoken\n","from typing import List, Dict, Any, Optional, Union\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","from langchain_core.documents import Document"],"metadata":{"id":"be3TYTGJZd1v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"V4KQZvz0WHJt"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Create some sample documents for testing\n","sample_docs = [\n","    Document(page_content=\"Paris is the capital of France. It is known for the Eiffel Tower and Louvre Museum.\",\n","             metadata={\"source\": \"geography_textbook\", \"page\": 42}),\n","    Document(page_content=\"France is a country in Western Europe with a population of about 67 million people.\",\n","             metadata={\"source\": \"world_almanac\", \"page\": 156}),\n","    Document(page_content=\"The Eiffel Tower was completed in 1889 and stands 330 meters tall.\",\n","             metadata={\"source\": \"landmarks_guide\", \"page\": 28})\n","]"],"metadata":{"id":"Olb2BWk_WHSn","executionInfo":{"status":"ok","timestamp":1741263727941,"user_tz":-330,"elapsed":8,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: LangChain Prompt Templates**"],"metadata":{"id":"LPndbT4hWQUy"}},{"cell_type":"code","source":["print(\"Section 1: LangChain Prompt Templates\")\n","\n","try:\n","    from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","    from langchain.prompts.few_shot import FewShotPromptTemplate\n","    from langchain_core.output_parsers import StrOutputParser\n","    from langchain_core.prompts import MessagesPlaceholder\n","    from langchain_openai import ChatOpenAI\n","\n","    print(\"Successfully imported LangChain libraries\")\n","\n","    # Basic Prompt Template\n","    basic_template = PromptTemplate.from_template(\n","        \"\"\"Answer the question based on the context.\n","\n","        Context: {context}\n","        Question: {question}\n","\n","        Answer:\"\"\"\n","    )\n","\n","    print(\"\\nBasic Template Example:\")\n","    context = \"\\n\".join([doc.page_content for doc in sample_docs])\n","    question = \"What is the height of the Eiffel Tower?\"\n","\n","    formatted_prompt = basic_template.format(context=context, question=question)\n","    print(formatted_prompt)\n","    print(f\"Tokens: {count_tokens(formatted_prompt)}\")\n","\n","    # Few-shot Template\n","    examples = [\n","        {\"context\": \"Paris is the capital of France.\",\n","         \"question\": \"What is the capital of France?\",\n","         \"answer\": \"Paris is the capital of France.\"},\n","        {\"context\": \"Berlin is the capital of Germany.\",\n","         \"question\": \"What is the capital of Germany?\",\n","         \"answer\": \"Berlin is the capital of Germany.\"}\n","    ]\n","\n","    example_prompt = PromptTemplate.from_template(\n","        \"\"\"Context: {context}\n","        Question: {question}\n","        Answer: {answer}\"\"\"\n","    )\n","\n","    few_shot_prompt = FewShotPromptTemplate(\n","        examples=examples,\n","        example_prompt=example_prompt,\n","        prefix=\"Answer the question based on the provided context:\\n\\n\",\n","        suffix=\"\\n\\nContext: {context}\\nQuestion: {question}\\nAnswer:\",\n","        input_variables=[\"context\", \"question\"]\n","    )\n","\n","    print(\"\\nFew-shot Template Example:\")\n","    formatted_few_shot = few_shot_prompt.format(context=context, question=question)\n","    print(formatted_few_shot)\n","    print(f\"Tokens: {count_tokens(formatted_few_shot)}\")\n","\n","    # Chat Prompt Template\n","    chat_template = ChatPromptTemplate.from_messages([\n","        (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n","        (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n","    ])\n","\n","    print(\"\\nChat Template Example:\")\n","    formatted_chat = chat_template.format_messages(context=context, question=question)\n","    print(formatted_chat)\n","\n","    # RAG-specific Template\n","    rag_template = PromptTemplate.from_template(\n","        \"\"\"You are an assistant for question-answering tasks.\n","        Use the following pieces of retrieved context to answer the question at the end.\n","        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","        Use three sentences maximum and keep the answer concise.\n","\n","        CONTEXT:\n","        {context}\n","\n","        QUESTION:\n","        {question}\n","\n","        ANSWER:\"\"\"\n","    )\n","\n","    print(\"\\nRAG-specific Template Example:\")\n","    formatted_rag = rag_template.format(context=context, question=question)\n","    print(formatted_rag)\n","    print(f\"Tokens: {count_tokens(formatted_rag)}\")\n","\n","    # Creating a chain with the template\n","    print(\"\\nBuilding a LangChain chain with the template:\")\n","\n","    if os.environ.get(\"OPENAI_API_KEY\"):\n","        try:\n","            from langchain.chains import LLMChain\n","            from langchain_openai import OpenAI\n","\n","            llm = OpenAI(temperature=0)\n","            chain = LLMChain(llm=llm, prompt=rag_template)\n","\n","            print(\"Chain created successfully\")\n","            print(\"Chain run would be: chain.run(context=context, question=question)\")\n","        except Exception as e:\n","            print(f\"Could not create LangChain chain: {e}\")\n","    else:\n","        print(\"OpenAI API key not set, skipping chain creation\")\n","\n","except ImportError as e:\n","    print(f\"Could not import LangChain: {e}\")\n","    print(\"Install with: pip install langchain langchain-openai\")\n","\n","print_separator()"],"metadata":{"id":"eP_GjoaQaD7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: LlamaIndex Prompt Templates**"],"metadata":{"id":"WbBCICbYWZax"}},{"cell_type":"code","source":["print(\"Section 2: LlamaIndex Prompt Templates\")\n","\n","try:\n","    from llama_index.prompts import PromptTemplate as LlamaPromptTemplate\n","\n","    print(\"Successfully imported LlamaIndex\")\n","\n","    # QA template\n","    qa_template = LlamaPromptTemplate(\n","        \"Context information is below.\\n\"\n","        \"---------------------\\n\"\n","        \"{context_str}\\n\"\n","        \"---------------------\\n\"\n","        \"Given the context information and not prior knowledge, \"\n","        \"answer the query.\\n\"\n","        \"Query: {query_str}\\n\"\n","        \"Answer: \"\n","    )\n","\n","    # Refine template\n","    refine_template = LlamaPromptTemplate(\n","        \"The original query is as follows: {query_str}\\n\"\n","        \"We have provided an existing answer: {existing_answer}\\n\"\n","        \"We have the opportunity to refine the existing answer \"\n","        \"with some more context below.\\n\"\n","        \"------------\\n\"\n","        \"{context_msg}\\n\"\n","        \"------------\\n\"\n","        \"Given the new context, refine the original answer to better \"\n","        \"answer the query. If the context isn't useful, return the original answer.\"\n","    )\n","\n","    # Tree summarize template\n","    tree_summarize_template = LlamaPromptTemplate(\n","        \"Context information is below.\\n\"\n","        \"---------------------\\n\"\n","        \"{context_str}\\n\"\n","        \"---------------------\\n\"\n","        \"Given the context information, generate a summary that captures the key points.\"\n","    )\n","\n","    print(\"\\nLlamaIndex QA Template:\")\n","    context_str = \"\\n\".join([doc.page_content for doc in sample_docs])\n","    query_str = \"What is the height of the Eiffel Tower?\"\n","\n","    formatted_qa = qa_template.format(context_str=context_str, query_str=query_str)\n","    print(formatted_qa)\n","    print(f\"Tokens: {count_tokens(formatted_qa)}\")\n","\n","    print(\"\\nLlamaIndex Refine Template:\")\n","    existing_answer = \"The Eiffel Tower is in Paris, France.\"\n","    context_msg = \"The Eiffel Tower was completed in 1889 and stands 330 meters tall.\"\n","\n","    formatted_refine = refine_template.format(\n","        query_str=query_str,\n","        existing_answer=existing_answer,\n","        context_msg=context_msg\n","    )\n","    print(formatted_refine)\n","    print(f\"Tokens: {count_tokens(formatted_refine)}\")\n","\n","    print(\"\\nLlamaIndex allows easy customization of built-in templates:\")\n","    custom_qa_template = LlamaPromptTemplate(\n","        \"You are a knowledgeable assistant.\\n\"\n","        \"Context information is below.\\n\"\n","        \"---------------------\\n\"\n","        \"{context_str}\\n\"\n","        \"---------------------\\n\"\n","        \"Answer the following query based ONLY on the context provided.\\n\"\n","        \"If the context doesn't contain relevant information, say so.\\n\"\n","        \"Query: {query_str}\\n\"\n","        \"Answer: \"\n","    )\n","\n","    formatted_custom_qa = custom_qa_template.format(\n","        context_str=context_str,\n","        query_str=query_str\n","    )\n","    print(formatted_custom_qa)\n","\n","except ImportError as e:\n","    print(f\"Could not import LlamaIndex: {e}\")\n","    print(\"Install with: pip install llama-index\")\n","\n","print_separator()"],"metadata":{"id":"Qf2RzJ_YaI-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Haystack Prompt Templates**"],"metadata":{"id":"pyv1bKdLWmzI"}},{"cell_type":"code","source":["print(\"Section 3: Haystack Prompt Templates\")\n","\n","try:\n","    from haystack.nodes import PromptNode, PromptTemplate\n","\n","    print(\"Successfully imported Haystack\")\n","\n","    # Basic Haystack template\n","    haystack_template = PromptTemplate(\n","        prompt=\"\"\"\n","        Answer the question based on the given documents.\n","        Documents: {documents}\n","        Question: {query}\n","        Answer:\n","        \"\"\",\n","        output_parser=None\n","    )\n","\n","    print(\"\\nHaystack Template Example:\")\n","    documents = \"\\n\".join([doc.page_content for doc in sample_docs])\n","    query = \"What is the height of the Eiffel Tower?\"\n","\n","    formatted_haystack = haystack_template.prepare(documents=documents, query=query)\n","    print(formatted_haystack)\n","    print(f\"Tokens: {count_tokens(formatted_haystack)}\")\n","\n","    # Using PromptNode\n","    if os.environ.get(\"OPENAI_API_KEY\"):\n","        try:\n","            prompt_node = PromptNode(\n","                model_name_or_path=\"gpt-3.5-turbo\",\n","                default_prompt_template=haystack_template,\n","                api_key=os.environ.get(\"OPENAI_API_KEY\")\n","            )\n","            print(\"\\nHaystack PromptNode created successfully\")\n","        except Exception as e:\n","            print(f\"Could not create Haystack PromptNode: {e}\")\n","    else:\n","        print(\"\\nOpenAI API key not set, skipping PromptNode creation\")\n","\n","except ImportError as e:\n","    print(f\"Could not import Haystack: {e}\")\n","    print(\"Install with: pip install haystack-ai\")\n","\n","print_separator()"],"metadata":{"id":"qgeDMDDmaQPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Custom Template Library for Domain-Specific Applications**"],"metadata":{"id":"uWPYeKa0WxWT"}},{"cell_type":"code","source":["print(\"Section 4: Custom Template Library for Domain-Specific Applications\")\n","\n","class MedicalRAGTemplates:\n","    \"\"\"Custom template library for medical RAG applications.\"\"\"\n","\n","    @staticmethod\n","    def diagnosis_template(context, symptoms):\n","        return f\"\"\"\n","        You are a medical assistant providing information based on medical literature.\n","        Analyze the following symptoms using only the provided medical context.\n","\n","        MEDICAL CONTEXT:\n","        {context}\n","\n","        REPORTED SYMPTOMS:\n","        {symptoms}\n","\n","        Based strictly on the medical context, provide:\n","        1. Possible conditions consistent with these symptoms\n","        2. Important missing information that would help narrow the possibilities\n","        3. Appropriate next steps based on medical guidelines\n","\n","        Include citations to specific documents using [Doc X] notation.\n","        Emphasize that this is informational only and not a diagnosis.\n","        \"\"\"\n","\n","    @staticmethod\n","    def medication_info_template(context, medication):\n","        return f\"\"\"\n","        Provide information about the following medication using only the provided context.\n","\n","        CONTEXT:\n","        {context}\n","\n","        MEDICATION:\n","        {medication}\n","\n","        Include information on:\n","        - Approved uses\n","        - Common side effects\n","        - Typical dosing\n","        - Major contraindications\n","\n","        Cite sources as [Doc X] and include appropriate medical disclaimers.\n","        \"\"\"\n","\n","    @staticmethod\n","    def treatment_comparison_template(context, condition, treatments):\n","        return f\"\"\"\n","        Compare the following treatments for {condition} based only on the provided context.\n","\n","        MEDICAL CONTEXT:\n","        {context}\n","\n","        TREATMENTS TO COMPARE:\n","        {treatments}\n","\n","        For each treatment, discuss:\n","        1. Efficacy based on clinical evidence\n","        2. Common side effects\n","        3. Contraindications\n","        4. Typical treatment protocol\n","\n","        Format as a comparison table followed by a brief summary.\n","        Include citations and medical disclaimers.\n","        \"\"\"\n","\n","# Create medical document examples\n","medical_docs = [\n","    Document(page_content=\"Aspirin is commonly used for pain relief and prevention of heart attacks. Common side effects include stomach irritation and increased risk of bleeding. Typical dosing is 81mg to 325mg daily for cardiac prevention.\",\n","             metadata={\"source\": \"drug_reference\", \"page\": 112}),\n","    Document(page_content=\"Symptoms of the common cold include runny nose, cough, sore throat, and mild fever. Treatment is typically supportive, including rest, hydration, and over-the-counter medications for symptom relief.\",\n","             metadata={\"source\": \"clinical_guidelines\", \"page\": 43}),\n","    Document(page_content=\"Migraine treatment options include NSAIDs, triptans, and anti-nausea medications. Preventive treatments include beta-blockers, antidepressants, and anti-seizure medications.\",\n","             metadata={\"source\": \"neurology_textbook\", \"page\": 215})\n","]\n","\n","print(\"Medical RAG Templates Example:\")\n","\n","# Diagnosis template example\n","medical_context = \"\\n\".join([doc.page_content for doc in medical_docs])\n","symptoms = \"Severe headache, light sensitivity, nausea\"\n","\n","diagnosis_prompt = MedicalRAGTemplates.diagnosis_template(medical_context, symptoms)\n","print(\"\\nDiagnosis Template Example:\")\n","print(diagnosis_prompt)\n","print(f\"Tokens: {count_tokens(diagnosis_prompt)}\")\n","\n","# Medication info template example\n","medication = \"Aspirin\"\n","medication_prompt = MedicalRAGTemplates.medication_info_template(medical_context, medication)\n","print(\"\\nMedication Info Template Example:\")\n","print(medication_prompt)\n","print(f\"Tokens: {count_tokens(medication_prompt)}\")\n","\n","print(\"\\nCreating domain-specific template libraries allows for:\")\n","print(\"1. Consistent prompt structure across applications\")\n","print(\"2. Domain-appropriate language and constraints\")\n","print(\"3. Built-in safety mechanisms (disclaimers, citation requirements)\")\n","print(\"4. Easier maintenance and updates to prompting strategy\")\n","\n","print_separator()\n"],"metadata":{"id":"a-999rrkacuz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Template Versioning and Management**"],"metadata":{"id":"3ZmYv9NsW6-L"}},{"cell_type":"code","source":["print(\"Section 5: Template Versioning and Management\")\n","\n","class TemplateRegistry:\n","    \"\"\"Registry for managing and versioning prompt templates.\"\"\"\n","\n","    def __init__(self):\n","        self.templates = {}\n","        self.version_history = {}\n","\n","    def register_template(self, name, template, version=\"1.0.0\"):\n","        \"\"\"Register a new template version.\"\"\"\n","        if name not in self.templates:\n","            self.templates[name] = template\n","            self.version_history[name] = {version: template}\n","        else:\n","            self.templates[name] = template\n","            self.version_history[name][version] = template\n","\n","    def get_template(self, name, version=None):\n","        \"\"\"Get a template by name and optional version.\"\"\"\n","        if name not in self.templates:\n","            raise KeyError(f\"Template '{name}' not found\")\n","\n","        if version is None:\n","            return self.templates[name]\n","\n","        if version not in self.version_history[name]:\n","            raise KeyError(f\"Version '{version}' not found for template '{name}'\")\n","\n","        return self.version_history[name][version]\n","\n","    def list_versions(self, name):\n","        \"\"\"List all versions of a template.\"\"\"\n","        if name not in self.version_history:\n","            raise KeyError(f\"Template '{name}' not found\")\n","\n","        return list(self.version_history[name].keys())\n","\n","    def get_latest_version(self, name):\n","        \"\"\"Get the latest version of a template based on semantic versioning.\"\"\"\n","        if name not in self.version_history:\n","            raise KeyError(f\"Template '{name}' not found\")\n","\n","        versions = list(self.version_history[name].keys())\n","        versions.sort(key=lambda s: [int(u) for u in s.split('.')])\n","        return versions[-1]\n","\n","# Create a template registry and add templates\n","registry = TemplateRegistry()\n","\n","# Register templates with versioning\n","qa_template_v1 = \"\"\"\n","Answer the question based on the context.\n","\n","Context: {context}\n","Question: {question}\n","\n","Answer:\n","\"\"\"\n","\n","qa_template_v2 = \"\"\"\n","Answer the question based only on the provided context.\n","If the answer isn't in the context, say \"I don't have enough information.\"\n","\n","Context: {context}\n","Question: {question}\n","\n","Answer:\n","\"\"\"\n","\n","qa_template_v3 = \"\"\"\n","You are an assistant for question-answering tasks.\n","Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer concise.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","registry.register_template(\"qa\", qa_template_v1, \"1.0.0\")\n","registry.register_template(\"qa\", qa_template_v2, \"1.1.0\")\n","registry.register_template(\"qa\", qa_template_v3, \"2.0.0\")\n","\n","# Test registry functions\n","print(\"Template Registry Example:\")\n","\n","print(\"\\nList of QA template versions:\")\n","versions = registry.list_versions(\"qa\")\n","print(versions)\n","\n","print(\"\\nLatest QA template version:\")\n","latest = registry.get_latest_version(\"qa\")\n","print(latest)\n","\n","print(\"\\nQA Template v1.0.0:\")\n","v1 = registry.get_template(\"qa\", \"1.0.0\")\n","print(v1)\n","\n","print(\"\\nQA Template v2.0.0:\")\n","v2 = registry.get_template(\"qa\", \"2.0.0\")\n","print(v2)\n","\n","print(\"\\nDefault QA Template (latest):\")\n","default = registry.get_template(\"qa\")\n","print(default)\n","\n","# Demonstrate saving and loading the registry\n","print(\"\\nSaving and loading the registry:\")\n","\n","# Save to JSON\n","registry_data = {\n","    \"templates\": {name: template for name, template in registry.templates.items()},\n","    \"version_history\": registry.version_history\n","}\n","\n","# Write to disk (uncommenting to save to file)\n","# with open(\"template_registry.json\", \"w\") as f:\n","#     json.dump(registry_data, f, indent=2)\n","\n","# Load from JSON (for illustration)\n","loaded_registry = TemplateRegistry()\n","for name, template in registry_data[\"templates\"].items():\n","    for version, template_text in registry_data[\"version_history\"][name].items():\n","        loaded_registry.register_template(name, template_text, version)\n","\n","print(\"Registry can be saved and loaded successfully\")\n","\n","print_separator()"],"metadata":{"id":"StNPJfdqagnm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Integration with RAG Pipelines**"],"metadata":{"id":"w1zpIWPwXFBX"}},{"cell_type":"code","source":["print(\"Section 6: Integration with RAG Pipelines\")\n","\n","# Skip actual integration if LangChain isn't available\n","print(\"Integration with RAG Pipelines (conceptual overview):\")\n","\n","print(\"\"\"\n","# Typical steps for integrating templates with a RAG pipeline:\n","\n","1. Define your prompt template:\n","   prompt = PromptTemplate.from_template(\n","       \"Answer based on context: {context}\\\\nQ: {question}\\\\nA:\"\n","   )\n","\n","2. Create a retriever (typically from a vector store):\n","   retriever = vector_store.as_retriever()\n","\n","3. Create an LLM instance:\n","   llm = ChatOpenAI(temperature=0)\n","\n","4. Combine into a RAG chain:\n","   rag_chain = (\n","       {\"context\": retriever, \"question\": RunnablePassthrough()}\n","       | prompt\n","       | llm\n","   )\n","\n","5. Invoke the chain:\n","   response = rag_chain.invoke(\"What is the height of the Eiffel Tower?\")\n","\"\"\")\n","\n","print(\"\\nThis basic pattern can be adapted for different frameworks and complex scenarios.\")\n","\n","print(\"\"\"\n","# More advanced integration patterns include:\n","\n","1. Multi-step reasoning chains:\n","   - Question reformulation → Retrieval → Answer generation\n","\n","2. Retrieval customization:\n","   - Filtering results by metadata\n","   - Re-ranking documents by relevance\n","   - Dynamically adjusting retrieval parameters\n","\n","3. Post-processing:\n","   - Citation extraction\n","   - Answer validation\n","   - Response formatting\n","\"\"\")\n","\n","print_separator()"],"metadata":{"id":"rK-3WJihapQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 7: Comparing Framework Capabilities**"],"metadata":{"id":"M2-E7FEzXPpG"}},{"cell_type":"code","source":["print(\"Section 7: Comparing Framework Capabilities\")\n","\n","print(\"Comparison of Template Framework Features:\")\n","\n","# Create a comparison table\n","comparison = {\n","    \"Feature\": [\n","        \"Basic Templates\",\n","        \"Few-Shot Learning\",\n","        \"Chat Templates\",\n","        \"XML/JSON Output Parsing\",\n","        \"Template Validation\",\n","        \"Template Composition\",\n","        \"Integration with Retrievers\",\n","        \"Custom Output Parsers\",\n","        \"Template Reuse\",\n","        \"Memory Integration\"\n","    ],\n","    \"LangChain\": [\n","        \"✅ Excellent\",\n","        \"✅ Built-in\",\n","        \"✅ Native Support\",\n","        \"✅ Strong Support\",\n","        \"✅ Available\",\n","        \"✅ Excellent\",\n","        \"✅ Seamless\",\n","        \"✅ Extensive\",\n","        \"✅ Excellent\",\n","        \"✅ Built-in\"\n","    ],\n","    \"LlamaIndex\": [\n","        \"✅ Excellent\",\n","        \"⚠️ Limited\",\n","        \"✅ Available\",\n","        \"⚠️ Basic\",\n","        \"❌ Limited\",\n","        \"✅ Good\",\n","        \"✅ Native\",\n","        \"⚠️ Basic\",\n","        \"✅ Good\",\n","        \"✅ Available\"\n","    ],\n","    \"Haystack\": [\n","        \"✅ Good\",\n","        \"❌ Limited\",\n","        \"⚠️ Basic\",\n","        \"⚠️ Basic\",\n","        \"❌ Limited\",\n","        \"✅ Pipeline-based\",\n","        \"✅ Native\",\n","        \"⚠️ Basic\",\n","        \"⚠️ Limited\",\n","        \"⚠️ Basic\"\n","    ]\n","}\n","\n","# Print comparison table\n","print(\"\\nTemplate Framework Feature Comparison:\")\n","header = f\"{'Feature':<25} | {'LangChain':<15} | {'LlamaIndex':<15} | {'Haystack':<15}\"\n","separator = \"-\" * len(header)\n","print(separator)\n","print(header)\n","print(separator)\n","\n","for i in range(len(comparison[\"Feature\"])):\n","    row = f\"{comparison['Feature'][i]:<25} | {comparison['LangChain'][i]:<15} | {comparison['LlamaIndex'][i]:<15} | {comparison['Haystack'][i]:<15}\"\n","    print(row)\n","\n","print(separator)\n","\n","print(\"\\nFramework Selection Guidelines:\")\n","print(\"1. LangChain: Best for complex applications requiring flexible prompt engineering and chain composition\")\n","print(\"2. LlamaIndex: Excellent for document-centric applications with complex retrieval needs\")\n","print(\"3. Haystack: Good for production deployments requiring stable, modular pipelines\")\n","\n","print(\"\\nConsiderations for Framework Selection:\")\n","print(\"- Project complexity and scale\")\n","print(\"- Integration with existing systems\")\n","print(\"- Team familiarity with the framework\")\n","print(\"- Specific RAG features needed (retrieval methods, prompt flexibility, etc.)\")\n","print(\"- Production deployment requirements\")\n","\n","print_separator()"],"metadata":{"id":"1_u-wXfrawQy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 8: Putting It All Together**"],"metadata":{"id":"xRmBjx42XcrY"}},{"cell_type":"code","source":["print(\"Section 8: Putting It All Together\")\n","\n","class RAGPromptManager:\n","    \"\"\"\n","    Comprehensive prompt manager for RAG applications.\n","    Combines template registry, versioning, and framework integration.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.registry = TemplateRegistry()\n","        self._register_default_templates()\n","\n","    def _register_default_templates(self):\n","        \"\"\"Register default templates for common RAG tasks.\"\"\"\n","        # QA template\n","        qa_template = \"\"\"\n","        You are an assistant for question-answering tasks.\n","        Use the following pieces of context to answer the question at the end.\n","        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","        CONTEXT:\n","        {context}\n","\n","        QUESTION:\n","        {question}\n","\n","        ANSWER:\n","        \"\"\"\n","\n","        # Summarization template\n","        summarize_template = \"\"\"\n","        Create a comprehensive summary of the following information.\n","\n","        CONTEXT:\n","        {context}\n","\n","        Provide a concise summary that captures the key points.\n","        Focus on the main ideas and significant details.\n","\n","        SUMMARY:\n","        \"\"\"\n","\n","        # Comparison template\n","        compare_template = \"\"\"\n","        Compare and contrast the following entities based on the provided information:\n","\n","        CONTEXT:\n","        {context}\n","\n","        ENTITIES TO COMPARE:\n","        {entity_1}\n","        {entity_2}\n","\n","        Provide a structured comparison highlighting key similarities and differences.\n","\n","        COMPARISON:\n","        \"\"\"\n","\n","        # Register templates\n","        self.registry.register_template(\"qa\", qa_template, \"1.0.0\")\n","        self.registry.register_template(\"summarize\", summarize_template, \"1.0.0\")\n","        self.registry.register_template(\"compare\", compare_template, \"1.0.0\")\n","\n","    def register_custom_template(self, name, template, version=\"1.0.0\"):\n","        \"\"\"Register a custom template.\"\"\"\n","        self.registry.register_template(name, template, version)\n","\n","    def get_template(self, name, version=None):\n","        \"\"\"Get a template by name and optional version.\"\"\"\n","        return self.registry.get_template(name, version)\n","\n","    def create_prompt(self, template_name, version=None, **kwargs):\n","        \"\"\"Create a formatted prompt from a template with provided variables.\"\"\"\n","        template = self.get_template(template_name, version)\n","        formatted = template.format(**kwargs)\n","\n","        # Print token usage stats\n","        token_count = count_tokens(formatted)\n","        print(f\"Prompt '{template_name}' generated with {token_count} tokens\")\n","\n","        return formatted\n","\n","    def integrate_with_langchain(self, template_name, version=None):\n","        \"\"\"Create a LangChain PromptTemplate from the registry.\"\"\"\n","        try:\n","            from langchain.prompts import PromptTemplate as LangChainPrompt\n","\n","            template_str = self.get_template(template_name, version)\n","\n","            # Parse input variables\n","            import re\n","            input_vars = re.findall(r'\\{([^{}]*)\\}', template_str)\n","\n","            return LangChainPrompt(\n","                template=template_str,\n","                input_variables=input_vars\n","            )\n","        except ImportError:\n","            print(\"LangChain not available\")\n","            return None\n","\n","# Create and demonstrate the RAG Prompt Manager\n","prompt_manager = RAGPromptManager()\n","\n","print(\"RAG Prompt Manager Demo:\")\n","\n","# Get a template\n","qa_template = prompt_manager.get_template(\"qa\")\n","print(\"\\nQA Template:\")\n","print(qa_template)\n","\n","# Create a formatted prompt\n","context = \"\\n\".join([doc.page_content for doc in sample_docs])\n","formatted_qa = prompt_manager.create_prompt(\"qa\", context=context, question=\"What is the height of the Eiffel Tower?\")\n","print(\"\\nFormatted QA prompt:\")\n","print(formatted_qa)\n","\n","# Register a custom template\n","custom_template = \"\"\"\n","You are a specialized assistant for {domain} questions.\n","Use the following information to provide a detailed response.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Provide a comprehensive answer with detailed {domain} analysis.\n","Include any relevant {domain} terminology and concepts.\n","\n","DETAILED RESPONSE:\n","\"\"\"\n","\n","prompt_manager.register_custom_template(\"domain_specific\", custom_template)\n","\n","formatted_custom = prompt_manager.create_prompt(\n","    \"domain_specific\",\n","    domain=\"architectural\",\n","    context=context,\n","    question=\"Describe the Eiffel Tower's design.\"\n",")\n","\n","print(\"\\nCustom domain-specific prompt:\")\n","print(formatted_custom)\n","\n","# Demonstrate framework integration (conceptual)\n","print(\"\\nIntegrating with LangChain (conceptual overview):\")\n","print(\"\"\"\n","To integrate the templates with a complete RAG pipeline:\n","\n","1. Convert the template to a LangChain PromptTemplate:\n","   lc_prompt = prompt_manager.integrate_with_langchain(\"qa\")\n","\n","2. Create a retriever from a vector store:\n","   retriever = vector_store.as_retriever()\n","\n","3. Create an LLM:\n","   llm = ChatOpenAI()\n","\n","4. Build the RAG chain:\n","   from langchain.chains import RetrievalQA\n","\n","   rag_chain = RetrievalQA.from_chain_type(\n","       llm=llm,\n","       chain_type=\"stuff\",\n","       retriever=retriever,\n","       chain_type_kwargs={\"prompt\": lc_prompt}\n","   )\n","\n","5. Run the chain:\n","   rag_chain.invoke({\"query\": \"What is the Eiffel Tower?\"})\n","\"\"\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")"],"metadata":{"id":"5yCzgBKsXc1l"},"execution_count":null,"outputs":[]}]}