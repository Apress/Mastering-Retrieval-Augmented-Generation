{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtnZAy7SbfGEAKzBUOzKQY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"bahxUAkdzZnN"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"S9MmepbLzPel","executionInfo":{"status":"ok","timestamp":1740297754115,"user_tz":-330,"elapsed":19504,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"outputs":[],"source":["!pip install -q langchain\n","!pip install -q langchain_community\n","!pip install -q transformers\n","!pip install -q torch\n","!pip install -q numpy\n","!pip install -q scikit-learn\n","!pip install -q sentence-transformers"]},{"cell_type":"markdown","source":["**Import required libraries**"],"metadata":{"id":"IwvfUg7y0JsH"}},{"cell_type":"code","source":["import time\n","import numpy as np\n","from typing import List, Dict, Any\n","import torch\n","from langchain_core.embeddings import Embeddings\n","from transformers import AutoTokenizer, AutoModel\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"iz2Kizb70Pkt","executionInfo":{"status":"ok","timestamp":1740297757811,"user_tz":-330,"elapsed":9,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Basic Custom Embeddings**"],"metadata":{"id":"bLpKiz330btF"}},{"cell_type":"code","source":["#First, let's implement a simple custom embedding model that demonstrates the basic structure:\n","\n","class SimpleCustomEmbeddings(Embeddings):\n","    \"\"\"A simple custom embedding model for demonstration.\"\"\"\n","\n","    def __init__(self, dimension: int = 512):\n","        self.dimension = dimension\n","        self.metrics: Dict[str, List[Any]] = {\n","            'processing_times': [],\n","            'text_lengths': []\n","        }\n","\n","    def _validate_input(self, text: str) -> str:\n","        \"\"\"Validate and clean input text.\"\"\"\n","        if not isinstance(text, str):\n","            raise ValueError(\"Input must be a string\")\n","        cleaned_text = text.strip()\n","        if not cleaned_text:\n","            raise ValueError(\"Input text cannot be empty\")\n","        return cleaned_text\n","\n","    def _compute_embedding(self, text: str) -> List[float]:\n","        \"\"\"Compute a deterministic embedding based on text characteristics.\"\"\"\n","        # Create a simple hash of the text\n","        hash_value = sum(ord(c) for c in text)\n","        # Ensure seed is within valid range (0 to 2**32 - 1)\n","        seed = hash_value % (2**32 - 1)\n","\n","        # Use the seed to generate a deterministic embedding\n","        np.random.seed(seed)\n","        embedding = np.random.uniform(-1, 1, self.dimension)\n","\n","        # Normalize the embedding\n","        embedding = embedding / np.linalg.norm(embedding)\n","        return embedding.tolist()\n","\n","    def _monitor_performance(self, text: str, start_time: float):\n","        \"\"\"Monitor embedding generation performance.\"\"\"\n","        end_time = time.time()\n","        self.metrics['processing_times'].append(end_time - start_time)\n","        self.metrics['text_lengths'].append(len(text))\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        \"\"\"Generate embeddings for a list of documents.\"\"\"\n","        embeddings = []\n","        for text in texts:\n","            start_time = time.time()\n","            validated_text = self._validate_input(text)\n","            embedding = self._compute_embedding(validated_text)\n","            self._monitor_performance(text, start_time)\n","            embeddings.append(embedding)\n","        return embeddings\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        \"\"\"Generate embedding for a single query text.\"\"\"\n","        start_time = time.time()\n","        validated_text = self._validate_input(text)\n","        embedding = self._compute_embedding(validated_text)\n","        self._monitor_performance(text, start_time)\n","        return embedding\n","\n","\"\"\"Let's test our simple custom embeddings:\"\"\"\n","\n","# Initialize the simple custom embeddings\n","simple_embedder = SimpleCustomEmbeddings(dimension=64)\n","\n","# Test texts\n","test_texts = [\n","    \"Machine learning is fascinating\",\n","    \"AI is transforming industries\",\n","    \"Neural networks are powerful\",\n","    \"Data science is the future\"\n","]\n","\n","print(\"Testing Simple Custom Embeddings:\")\n","print(\"\\nGenerating embeddings for multiple documents...\")\n","doc_embeddings = simple_embedder.embed_documents(test_texts)\n","\n","print(\"\\nGenerating embedding for a single query...\")\n","query_embedding = simple_embedder.embed_query(\"What is machine learning?\")\n","\n","print(\"\\nEmbedding Statistics:\")\n","print(f\"Document embedding dimension: {len(doc_embeddings[0])}\")\n","print(f\"Query embedding dimension: {len(query_embedding)}\")\n","\n","# Print performance metrics\n","print(\"\\nPerformance Metrics:\")\n","avg_time = np.mean(simple_embedder.metrics['processing_times'])\n","avg_length = np.mean(simple_embedder.metrics['text_lengths'])\n","print(f\"Average processing time: {avg_time:.4f} seconds\")\n","print(f\"Average text length: {avg_length:.1f} characters\")\n"],"metadata":{"id":"TZcGPOr-0d5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Transformer-Based Custom Embeddings**"],"metadata":{"id":"rq64Yvbu1Pzr"}},{"cell_type":"code","source":["#Now let's implement a more sophisticated embedding model using transformers:\n","\n","class TransformerCustomEmbeddings(Embeddings):\n","    \"\"\"Custom embeddings using transformer models.\"\"\"\n","\n","    def __init__(self, model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n","        self.model = SentenceTransformer(model_name)\n","        self.metrics = {\n","            'processing_times': [],\n","            'text_lengths': []\n","        }\n","\n","    def _validate_input(self, text: str) -> str:\n","        \"\"\"Validate and clean input text.\"\"\"\n","        if not isinstance(text, str):\n","            raise ValueError(\"Input must be a string\")\n","        cleaned_text = text.strip()\n","        if not cleaned_text:\n","            raise ValueError(\"Input text cannot be empty\")\n","        return cleaned_text\n","\n","    def _compute_embedding(self, text: str) -> List[float]:\n","        \"\"\"Compute embedding using the transformer model.\"\"\"\n","        embedding = self.model.encode(text)\n","        return embedding.tolist()\n","\n","    def _monitor_performance(self, text: str, start_time: float):\n","        \"\"\"Monitor embedding generation performance.\"\"\"\n","        end_time = time.time()\n","        self.metrics['processing_times'].append(end_time - start_time)\n","        self.metrics['text_lengths'].append(len(text))\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        \"\"\"Generate embeddings for a list of documents.\"\"\"\n","        start_time = time.time()\n","        validated_texts = [self._validate_input(text) for text in texts]\n","        embeddings = self.model.encode(validated_texts)\n","\n","        for text in texts:\n","            self._monitor_performance(text, start_time)\n","\n","        return embeddings.tolist()\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        \"\"\"Generate embedding for a single query text.\"\"\"\n","        start_time = time.time()\n","        validated_text = self._validate_input(text)\n","        embedding = self._compute_embedding(validated_text)\n","        self._monitor_performance(text, start_time)\n","        return embedding\n","\n","\"\"\"Test the transformer-based embeddings:\"\"\"\n","\n","print(\"Testing Transformer-Based Embeddings:\")\n","try:\n","    # Initialize the transformer-based embeddings\n","    transformer_embedder = TransformerCustomEmbeddings()\n","\n","    print(\"\\nGenerating embeddings using transformer model...\")\n","    transformer_doc_embeddings = transformer_embedder.embed_documents(test_texts)\n","    transformer_query_embedding = transformer_embedder.embed_query(\"What is machine learning?\")\n","\n","    print(\"\\nEmbedding Statistics:\")\n","    print(f\"Document embedding dimension: {len(transformer_doc_embeddings[0])}\")\n","    print(f\"Query embedding dimension: {len(transformer_query_embedding)}\")\n","\n","    # Print performance metrics\n","    print(\"\\nPerformance Metrics:\")\n","    avg_time = np.mean(transformer_embedder.metrics['processing_times'])\n","    avg_length = np.mean(transformer_embedder.metrics['text_lengths'])\n","    print(f\"Average processing time: {avg_time:.4f} seconds\")\n","    print(f\"Average text length: {avg_length:.1f} characters\")\n","except Exception as e:\n","    print(f\"Error testing transformer embeddings: {str(e)}\")"],"metadata":{"id":"jtmLekrG1Ty-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Domain-Specific Custom Embeddings**"],"metadata":{"id":"jSLcwoVT1hnT"}},{"cell_type":"code","source":["#Let's implement a domain-specific embedding model that combines multiple embedding sources:\n","\n","class DomainSpecificEmbeddings(Embeddings):\n","    \"\"\"Domain-specific embeddings combining multiple sources.\"\"\"\n","\n","    def __init__(self, base_model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n","        self.base_model = SentenceTransformer(base_model_name)\n","        self.metrics = {\n","            'processing_times': [],\n","            'text_lengths': []\n","        }\n","\n","        # Domain-specific vocabulary (example)\n","        self.domain_vocab = {\n","            'ml': 'machine learning',\n","            'ai': 'artificial intelligence',\n","            'dl': 'deep learning',\n","            'nn': 'neural network'\n","        }\n","\n","    def _preprocess_text(self, text: str) -> str:\n","        \"\"\"Apply domain-specific preprocessing.\"\"\"\n","        text = text.lower()\n","        for abbrev, full in self.domain_vocab.items():\n","            text = text.replace(f\" {abbrev} \", f\" {full} \")\n","        return text\n","\n","    def _enhance_embedding(self, base_embedding: List[float], text: str) -> List[float]:\n","        \"\"\"Enhance base embedding with domain-specific features.\"\"\"\n","        # Simple example: Adjust embeddings based on domain term presence\n","        embedding = np.array(base_embedding)\n","\n","        # Count domain terms\n","        domain_term_count = sum(1 for term in self.domain_vocab.values()\n","                              if term in text.lower())\n","\n","        # Slightly adjust embedding based on domain term presence\n","        if domain_term_count > 0:\n","            adjustment = 0.1 * domain_term_count\n","            embedding = embedding * (1 + adjustment)\n","            embedding = embedding / np.linalg.norm(embedding)\n","\n","        return embedding.tolist()\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        \"\"\"Generate domain-aware embeddings for documents.\"\"\"\n","        start_time = time.time()\n","\n","        # Preprocess texts\n","        processed_texts = [self._preprocess_text(text) for text in texts]\n","\n","        # Get base embeddings\n","        base_embeddings = self.base_model.encode(processed_texts)\n","\n","        # Enhance embeddings\n","        enhanced_embeddings = [\n","            self._enhance_embedding(emb.tolist(), text)\n","            for emb, text in zip(base_embeddings, texts)\n","        ]\n","\n","        for text in texts:\n","            self._monitor_performance(text, start_time)\n","\n","        return enhanced_embeddings\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        \"\"\"Generate domain-aware embedding for query.\"\"\"\n","        start_time = time.time()\n","        processed_text = self._preprocess_text(text)\n","        base_embedding = self.base_model.encode(processed_text)\n","        enhanced_embedding = self._enhance_embedding(base_embedding.tolist(), text)\n","        self._monitor_performance(text, start_time)\n","        return enhanced_embedding\n","\n","\"\"\"Test the domain-specific embeddings:\"\"\"\n","\n","print(\"Testing Domain-Specific Embeddings:\")\n","try:\n","    # Initialize the domain-specific embeddings\n","    domain_embedder = DomainSpecificEmbeddings()\n","\n","    # Test with domain-specific texts\n","    domain_texts = [\n","        \"ML and DL are advancing rapidly\",\n","        \"AI is transforming industries\",\n","        \"NNs are the foundation of deep learning\",\n","        \"The future of ML looks promising\"\n","    ]\n","\n","    print(\"\\nGenerating domain-specific embeddings...\")\n","    domain_doc_embeddings = domain_embedder.embed_documents(domain_texts)\n","    domain_query_embedding = domain_embedder.embed_query(\"What is ML and AI?\")\n","\n","    print(\"\\nEmbedding Statistics:\")\n","    print(f\"Document embedding dimension: {len(domain_doc_embeddings[0])}\")\n","    print(f\"Query embedding dimension: {len(domain_query_embedding)}\")\n","except Exception as e:\n","    print(f\"Error testing domain embeddings: {str(e)}\")"],"metadata":{"id":"FMeu9jsK1oG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Comparing Different Embedding Models**"],"metadata":{"id":"HjCwFSpg10Kj"}},{"cell_type":"code","source":["#Let's compare the different embedding approaches:\n","\n","def compare_embeddings(texts: List[str], query: str, embedders: Dict[str, Embeddings]):\n","    \"\"\"Compare different embedding models.\"\"\"\n","    results = {}\n","\n","    for name, embedder in embedders.items():\n","        try:\n","            start_time = time.time()\n","\n","            # Generate embeddings\n","            doc_embeddings = embedder.embed_documents(texts)\n","            query_embedding = embedder.embed_query(query)\n","\n","            # Calculate similarities\n","            similarities = [\n","                cosine_similarity(\n","                    np.array(query_embedding).reshape(1, -1),\n","                    np.array(doc_emb).reshape(1, -1)\n","                )[0][0]\n","                for doc_emb in doc_embeddings\n","            ]\n","\n","            processing_time = time.time() - start_time\n","\n","            results[name] = {\n","                'similarities': similarities,\n","                'processing_time': processing_time,\n","                'status': 'success'\n","            }\n","        except Exception as e:\n","            results[name] = {\n","                'status': 'error',\n","                'error': str(e)\n","            }\n","\n","    return results\n","\n","print(\"Comparing Embedding Models:\")\n","\n","# Compare embedding models that were successfully initialized\n","embedders = {\n","    'Simple': simple_embedder\n","}\n","\n","# Add transformer embedder if initialization was successful\n","if 'transformer_embedder' in locals():\n","    embedders['Transformer'] = transformer_embedder\n","\n","# Add domain embedder if initialization was successful\n","if 'domain_embedder' in locals():\n","    embedders['Domain-Specific'] = domain_embedder\n","\n","query = \"What is machine learning?\"\n","comparison_results = compare_embeddings(test_texts, query, embedders)\n","\n","# Print comparison results\n","print(\"\\nEmbedding Models Comparison:\")\n","for model_name, result in comparison_results.items():\n","    print(f\"\\n{model_name} Embeddings:\")\n","    if result['status'] == 'success':\n","        print(f\"Processing time: {result['processing_time']:.4f} seconds\")\n","        print(\"Similarities with query:\")\n","        for text, sim in zip(test_texts, result['similarities']):\n","            print(f\"  {text}: {sim:.4f}\")\n","    else:\n","        print(f\"Error: {result['error']}\")"],"metadata":{"id":"jlgGwrQ614qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save Results**"],"metadata":{"id":"Jt56JS0X2Ea0"}},{"cell_type":"code","source":["#Save the comparison results for later analysis:\n","\n","import json\n","from datetime import datetime\n","\n","# Prepare results for saving\n","save_results = {\n","    'timestamp': datetime.now().isoformat(),\n","    'comparison_results': {\n","        name: {\n","            'status': results['status'],\n","            'processing_time': results['processing_time'] if results['status'] == 'success' else None,\n","            'similarities': results['similarities'] if results['status'] == 'success' else None\n","        }\n","        for name, results in comparison_results.items()\n","    },\n","    'test_texts': test_texts,\n","    'query': query\n","}\n","\n","# Save results to file\n","with open('embedding_comparison_results.json', 'w') as f:\n","    json.dump(save_results, f, indent=2)\n","\n","print(\"\\nResults saved to embedding_comparison_results.json\")"],"metadata":{"id":"TN9ci7Au2KFj"},"execution_count":null,"outputs":[]}]}