{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFSft9Z+uTymXkkN0tOAu5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook demonstrates how to extract entities and relationships from documents using modern NLP techniques and LLMs. You'll learn to:\n","- Use LLMs for entity and relationship extraction\n","- Process document collections systematically\n","- Handle different document types and formats\n","- Validate and clean extracted knowledge\n","- Prepare data for knowledge graph construction"],"metadata":{"id":"fi9Nsept8kO2"}},{"cell_type":"markdown","source":["**## Prerequisites and Setup**"],"metadata":{"id":"Af-RPIA18k_r"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1b8yhg67JWS","executionInfo":{"status":"ok","timestamp":1748186899979,"user_tz":-330,"elapsed":14509,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"8d9d9f3a-9ace-4261-c908-db52341b396e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","âœ… spaCy loaded successfully\n","âœ… NLTK loaded successfully\n","âœ… LangChain loaded successfully\n","âœ… tiktoken loaded successfully\n","âœ… OpenAI LLM initialized successfully\n","\n","ðŸš€ Setup complete! All packages loaded in one cell.\n"]}],"source":["# Install and import all packages in one cell\n","!pip install -q openai langchain langchain-openai spacy pandas matplotlib seaborn nltk tiktoken\n","\n","# Download spacy model\n","!python -m spacy download en_core_web_sm\n","\n","# Import packages immediately after installation\n","import importlib\n","import sys\n","\n","# Force reload of modules if already imported\n","if 'langchain_openai' in sys.modules:\n","    importlib.reload(sys.modules['langchain_openai'])\n","\n","# Import all required libraries\n","import openai\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import json\n","import re\n","from typing import List, Dict, Any, Tuple\n","\n","# Import with error handling\n","try:\n","    import spacy\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    print(\"âœ… spaCy loaded successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ spaCy issue: {e}\")\n","\n","try:\n","    import nltk\n","    nltk.download('punkt_tab', quiet=True)  # Updated for newer NLTK versions\n","    nltk.download('punkt', quiet=True)      # Fallback for older versions\n","    nltk.download('stopwords', quiet=True)\n","    nltk.download('averaged_perceptron_tagger', quiet=True)\n","    print(\"âœ… NLTK loaded successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ NLTK issue: {e}\")\n","\n","try:\n","    from langchain_openai import ChatOpenAI\n","    from langchain.prompts import PromptTemplate\n","    from langchain_core.output_parsers import StrOutputParser\n","    from langchain.schema import Document\n","    print(\"âœ… LangChain loaded successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ LangChain issue: {e}\")\n","\n","try:\n","    import tiktoken\n","    print(\"âœ… tiktoken loaded successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ tiktoken issue: {e}\")\n","\n","# Set up OpenAI API key\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your actual key\n","\n","# Initialize LLM with error handling\n","try:\n","    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","    print(\"âœ… OpenAI LLM initialized successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸ OpenAI initialization issue: {e}\")\n","    print(\"Please check your API key is set correctly\")\n","\n","print(\"\\nðŸš€ Setup complete! All packages loaded in one cell.\")"]},{"cell_type":"markdown","source":["**## Part 1: Understanding Entity and Relationship Extraction**"],"metadata":{"id":"YtJSH26M_2by"}},{"cell_type":"code","source":["### Sample Document Collection\n","\n","# Sample research paper abstracts for demonstration\n","sample_documents = [\n","    {\n","        \"id\": \"paper_1\",\n","        \"title\": \"Attention Is All You Need\",\n","        \"abstract\": \"\"\"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show that these models are superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU points. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight P100 GPUs, a small fraction of the training costs of the best models from the literature.\"\"\",\n","        \"authors\": [\"Ashish Vaswani\", \"Noam Shazeer\", \"Niki Parmar\", \"Jakob Uszkoreit\"],\n","        \"year\": 2017,\n","        \"venue\": \"NIPS\"\n","    },\n","    {\n","        \"id\": \"paper_2\",\n","        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n","        \"abstract\": \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\"\"\",\n","        \"authors\": [\"Jacob Devlin\", \"Ming-Wei Chang\", \"Kenton Lee\", \"Kristina Toutanova\"],\n","        \"year\": 2018,\n","        \"venue\": \"NAACL\"\n","    },\n","    {\n","        \"id\": \"paper_3\",\n","        \"title\": \"GPT-3: Language Models are Few-Shot Learners\",\n","        \"abstract\": \"\"\"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. However, this paradigm requires task-specific fine-tuning datasets of thousands or tens of thousands of examples, and still often fails to match human performance. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.\"\"\",\n","        \"authors\": [\"Tom B. Brown\", \"Benjamin Mann\", \"Nick Ryder\", \"Melanie Subbiah\"],\n","        \"year\": 2020,\n","        \"venue\": \"NeurIPS\"\n","    }\n","]\n","\n","print(f\"ðŸ“š Loaded {len(sample_documents)} sample documents\")\n","for doc in sample_documents:\n","    print(f\"   â€¢ {doc['title']} ({doc['year']})\")"],"metadata":{"id":"FkeArH0W_2oD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Document Processing Utilities**"],"metadata":{"id":"ajgmDRmaAHUu"}},{"cell_type":"code","source":["class DocumentProcessor:\n","    \"\"\"Utility class for processing and analyzing documents.\"\"\"\n","\n","    def __init__(self):\n","        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","\n","    def count_tokens(self, text: str) -> int:\n","        \"\"\"Count tokens in text for LLM processing.\"\"\"\n","        return len(self.encoding.encode(text))\n","\n","    def chunk_text(self, text: str, max_tokens: int = 1000, overlap: int = 100) -> List[str]:\n","        \"\"\"Split text into chunks with specified token limits.\"\"\"\n","        tokens = self.encoding.encode(text)\n","        chunks = []\n","\n","        start = 0\n","        while start < len(tokens):\n","            end = min(start + max_tokens, len(tokens))\n","            chunk_tokens = tokens[start:end]\n","            chunk_text = self.encoding.decode(chunk_tokens)\n","            chunks.append(chunk_text)\n","\n","            if end >= len(tokens):\n","                break\n","            start = end - overlap\n","\n","        return chunks\n","\n","    def clean_text(self, text: str) -> str:\n","        \"\"\"Clean and normalize text for processing.\"\"\"\n","        # Remove excessive whitespace\n","        text = re.sub(r'\\s+', ' ', text)\n","        # Remove special characters but keep punctuation\n","        text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n","        return text.strip()\n","\n","    def extract_sentences(self, text: str) -> List[str]:\n","        \"\"\"Extract sentences from text using NLTK.\"\"\"\n","        sentences = nltk.sent_tokenize(text)\n","        return [s.strip() for s in sentences if len(s.strip()) > 10]\n","\n","# Initialize processor\n","doc_processor = DocumentProcessor()\n","\n","# Demonstrate text processing\n","sample_text = sample_documents[0][\"abstract\"]\n","print(\"ðŸ“Š Document Processing Analysis:\")\n","print(f\"Token count: {doc_processor.count_tokens(sample_text)}\")\n","print(f\"Sentence count: {len(doc_processor.extract_sentences(sample_text))}\")\n","print(f\"First sentence: {doc_processor.extract_sentences(sample_text)[0]}\")"],"metadata":{"id":"xLQnn8HiAHfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 2: LLM-Based Entity Extraction**"],"metadata":{"id":"VJcJTCnlAv3K"}},{"cell_type":"code","source":["### Entity Extraction with Prompts\n","\n","class EntityExtractor:\n","    \"\"\"Extract entities from text using LLM prompting.\"\"\"\n","\n","    def __init__(self, llm):\n","        self.llm = llm\n","        self.entity_prompt = PromptTemplate.from_template(\"\"\"\n","You are an expert at extracting entities from academic research papers.\n","Extract the following types of entities from the given text:\n","\n","ENTITY TYPES:\n","- PERSON: Names of researchers, authors, scientists\n","- ORGANIZATION: Universities, companies, research labs\n","- CONCEPT: Technical concepts, methods, algorithms\n","- DATASET: Named datasets used in research\n","- METRIC: Performance metrics, evaluation measures\n","- TECHNOLOGY: Software, hardware, tools\n","\n","TEXT:\n","{text}\n","\n","INSTRUCTIONS:\n","1. Extract entities exactly as they appear in the text\n","2. Classify each entity with one of the types above\n","3. Return results in JSON format\n","4. Only extract entities that are clearly mentioned\n","\n","OUTPUT FORMAT:\n","{\n","  \"entities\": [\n","    {\"text\": \"entity name\", \"type\": \"ENTITY_TYPE\", \"context\": \"surrounding context\"},\n","    ...\n","  ]\n","}\n","\n","JSON OUTPUT:\n","\"\"\")\n","\n","    def extract_entities(self, text: str) -> Dict[str, Any]:\n","        \"\"\"Extract entities from text using LLM.\"\"\"\n","        try:\n","            # Create and execute the prompt\n","            chain = self.entity_prompt | self.llm | StrOutputParser()\n","            result = chain.invoke({\"text\": text})\n","\n","            # Parse JSON response\n","            try:\n","                entities = json.loads(result)\n","                return entities\n","            except json.JSONDecodeError:\n","                # Handle cases where LLM doesn't return valid JSON\n","                print(\"âš ï¸ JSON parsing failed, attempting to extract entities from text\")\n","                return self._parse_text_response(result)\n","\n","        except Exception as e:\n","            print(f\"âŒ Error in entity extraction: {e}\")\n","            return {\"entities\": []}\n","\n","    def _parse_text_response(self, response: str) -> Dict[str, Any]:\n","        \"\"\"Parse non-JSON responses from LLM.\"\"\"\n","        # Simple fallback parsing - in practice, you'd want more robust parsing\n","        entities = []\n","        lines = response.split('\\n')\n","\n","        for line in lines:\n","            if ':' in line and any(entity_type in line.upper() for entity_type in\n","                                 ['PERSON', 'ORGANIZATION', 'CONCEPT', 'DATASET', 'METRIC', 'TECHNOLOGY']):\n","                try:\n","                    # Extract entity information from line\n","                    parts = line.split(':', 1)\n","                    if len(parts) == 2:\n","                        entity_info = parts[1].strip()\n","                        # This is a simplified parser - extend based on your needs\n","                        entities.append({\n","                            \"text\": entity_info,\n","                            \"type\": \"CONCEPT\",  # Default type\n","                            \"context\": line\n","                        })\n","                except:\n","                    continue\n","\n","        return {\"entities\": entities}\n","\n","    def process_document(self, document: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Process an entire document and extract entities.\"\"\"\n","        # Combine title and abstract for processing\n","        text = f\"Title: {document['title']}\\n\\nAbstract: {document['abstract']}\"\n","\n","        # Extract entities\n","        entities = self.extract_entities(text)\n","\n","        # Add document metadata\n","        entities['document_id'] = document['id']\n","        entities['document_title'] = document['title']\n","\n","        return entities\n","\n","# Initialize entity extractor\n","entity_extractor = EntityExtractor(llm)\n","\n","# Extract entities from first document\n","print(\"ðŸ” Extracting entities from sample document...\")\n","sample_entities = entity_extractor.process_document(sample_documents[0])\n","\n","print(f\"âœ… Extracted {len(sample_entities.get('entities', []))} entities\")\n","for entity in sample_entities.get('entities', [])[:5]:  # Show first 5\n","    print(f\"   â€¢ {entity.get('text', 'Unknown')} ({entity.get('type', 'Unknown')})\")"],"metadata":{"id":"lqmfzccQAwDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 3: Relationship Extraction**"],"metadata":{"id":"SmBbrgfeA6TI"}},{"cell_type":"code","source":["class RelationshipExtractor:\n","    \"\"\"Extract relationships between entities using LLM prompting.\"\"\"\n","\n","    def __init__(self, llm):\n","        self.llm = llm\n","        self.relationship_prompt = PromptTemplate.from_template(\"\"\"\n","You are an expert at extracting relationships from academic research papers.\n","Given the text and a list of entities, identify relationships between these entities.\n","\n","RELATIONSHIP TYPES:\n","- AUTHORED: Person authored/wrote paper\n","- AFFILIATED_WITH: Person affiliated with organization\n","- BASED_ON: Concept/method based on another concept\n","- IMPROVES_ON: Method improves on previous method\n","- USES: Method/system uses technology/dataset\n","- EVALUATES_ON: Method evaluated on dataset\n","- ACHIEVES: Method achieves specific metric/performance\n","- CITES: Paper references another work\n","- INTRODUCES: Paper introduces new concept/method\n","\n","TEXT:\n","{text}\n","\n","ENTITIES:\n","{entities}\n","\n","INSTRUCTIONS:\n","1. Only extract relationships that are clearly stated in the text\n","2. Use entities from the provided list\n","3. Specify relationship type, source entity, target entity\n","4. Include confidence score (0.0-1.0)\n","5. Return results in JSON format\n","\n","OUTPUT FORMAT:\n","{\n","  \"relationships\": [\n","    {\n","      \"source\": \"source entity\",\n","      \"target\": \"target entity\",\n","      \"relationship\": \"RELATIONSHIP_TYPE\",\n","      \"confidence\": 0.9,\n","      \"evidence\": \"text snippet supporting this relationship\"\n","    },\n","    ...\n","  ]\n","}\n","\n","JSON OUTPUT:\n","\"\"\")\n","\n","    def extract_relationships(self, text: str, entities: List[Dict]) -> Dict[str, Any]:\n","        \"\"\"Extract relationships from text given entities.\"\"\"\n","        try:\n","            # Format entities for the prompt\n","            entity_list = []\n","            for entity in entities:\n","                entity_list.append(f\"- {entity['text']} ({entity['type']})\")\n","            entities_text = \"\\n\".join(entity_list)\n","\n","            # Create and execute the prompt\n","            chain = self.relationship_prompt | self.llm | StrOutputParser()\n","            result = chain.invoke({\n","                \"text\": text,\n","                \"entities\": entities_text\n","            })\n","\n","            # Parse JSON response\n","            try:\n","                relationships = json.loads(result)\n","                return relationships\n","            except json.JSONDecodeError:\n","                print(\"âš ï¸ JSON parsing failed for relationships\")\n","                return self._parse_relationship_text(result)\n","\n","        except Exception as e:\n","            print(f\"âŒ Error in relationship extraction: {e}\")\n","            return {\"relationships\": []}\n","\n","    def _parse_relationship_text(self, response: str) -> Dict[str, Any]:\n","        \"\"\"Parse non-JSON relationship responses.\"\"\"\n","        relationships = []\n","        # Simple fallback parsing - extend based on your needs\n","        lines = response.split('\\n')\n","\n","        for line in lines:\n","            if '->' in line or 'relationship' in line.lower():\n","                # This is a simplified parser\n","                relationships.append({\n","                    \"source\": \"extracted_entity\",\n","                    \"target\": \"extracted_entity\",\n","                    \"relationship\": \"UNKNOWN\",\n","                    \"confidence\": 0.5,\n","                    \"evidence\": line.strip()\n","                })\n","\n","        return {\"relationships\": relationships}\n","\n","# Initialize relationship extractor\n","relationship_extractor = RelationshipExtractor(llm)\n","\n","# Extract relationships for the first document\n","if sample_entities:\n","    first_doc_entities = sample_entities.get('entities', [])\n","    if first_doc_entities:\n","        print(\"ðŸ”— Extracting relationships from sample document...\")\n","        sample_relationships = relationship_extractor.extract_relationships(\n","            f\"Title: {sample_documents[0]['title']}\\n\\nAbstract: {sample_documents[0]['abstract']}\",\n","            first_doc_entities\n","        )\n","\n","        print(f\"âœ… Extracted {len(sample_relationships.get('relationships', []))} relationships\")\n","        for rel in sample_relationships.get('relationships', [])[:3]:  # Show first 3\n","            print(f\"   â€¢ {rel.get('source', '')} --[{rel.get('relationship', '')}]--> {rel.get('target', '')}\")"],"metadata":{"id":"c0ZuK4_KA6bM","executionInfo":{"status":"ok","timestamp":1748186183137,"user_tz":-330,"elapsed":29,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**## Part 4: Complete Knowledge Extraction Pipeline**"],"metadata":{"id":"JnkNTe7wBCTT"}},{"cell_type":"code","source":["class KnowledgeExtractor:\n","    \"\"\"Combined entity and relationship extraction for documents.\"\"\"\n","\n","    def __init__(self, llm):\n","        self.entity_extractor = EntityExtractor(llm)\n","        self.relationship_extractor = RelationshipExtractor(llm)\n","        self.doc_processor = DocumentProcessor()\n","\n","    def extract_knowledge(self, document: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Extract complete knowledge (entities + relationships) from a document.\"\"\"\n","\n","        print(f\"ðŸ” Processing: {document['title'][:50]}...\")\n","\n","        # Step 1: Extract entities\n","        entity_result = self.entity_extractor.process_document(document)\n","        entities = entity_result.get('entities', [])\n","\n","        # Step 2: Extract relationships\n","        text = f\"Title: {document['title']}\\n\\nAbstract: {document['abstract']}\"\n","        relationship_result = self.relationship_extractor.extract_relationships(text, entities)\n","        relationships = relationship_result.get('relationships', [])\n","\n","        # Step 3: Compile complete knowledge structure\n","        knowledge = {\n","            'document_id': document['id'],\n","            'document_title': document['title'],\n","            'document_metadata': {\n","                'authors': document.get('authors', []),\n","                'year': document.get('year'),\n","                'venue': document.get('venue')\n","            },\n","            'entities': entities,\n","            'relationships': relationships,\n","            'extraction_stats': {\n","                'entity_count': len(entities),\n","                'relationship_count': len(relationships),\n","                'token_count': self.doc_processor.count_tokens(document['abstract'])\n","            }\n","        }\n","\n","        return knowledge\n","\n","# Initialize comprehensive knowledge extractor\n","knowledge_extractor = KnowledgeExtractor(llm)\n","\n","# Extract knowledge from all sample documents\n","print(\"ðŸš€ Starting comprehensive knowledge extraction...\")\n","extracted_knowledge = []\n","\n","for doc in sample_documents:\n","    try:\n","        knowledge = knowledge_extractor.extract_knowledge(doc)\n","        extracted_knowledge.append(knowledge)\n","    except Exception as e:\n","        print(f\"âŒ Error processing {doc['id']}: {e}\")\n","\n","print(f\"\\nðŸ“Š Knowledge Extraction Complete!\")\n","print(f\"   Documents processed: {len(extracted_knowledge)}\")\n","\n","# Summary statistics\n","total_entities = sum(len(doc.get('entities', [])) for doc in extracted_knowledge)\n","total_relationships = sum(len(doc.get('relationships', [])) for doc in extracted_knowledge)\n","\n","print(f\"   Total entities: {total_entities}\")\n","print(f\"   Total relationships: {total_relationships}\")"],"metadata":{"id":"jJAzTjznBHOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 5: Export for Graph Construction**"],"metadata":{"id":"MaTXtUIpBM5W"}},{"cell_type":"code","source":["class KnowledgeExporter:\n","    \"\"\"Export extracted knowledge for graph construction.\"\"\"\n","\n","    def to_graph_format(self, extracted_knowledge: List[Dict]) -> Dict[str, Any]:\n","        \"\"\"Convert extracted knowledge to graph-ready format.\"\"\"\n","\n","        nodes = []\n","        edges = []\n","        node_id_map = {}\n","\n","        for knowledge in extracted_knowledge:\n","            doc_id = knowledge['document_id']\n","\n","            # Add document node\n","            doc_node_id = f\"doc_{doc_id}\"\n","            nodes.append({\n","                'id': doc_node_id,\n","                'label': knowledge['document_title'],\n","                'type': 'DOCUMENT',\n","                'properties': knowledge.get('document_metadata', {})\n","            })\n","\n","            # Add entity nodes\n","            for entity in knowledge.get('entities', []):\n","                entity_text = entity['text']\n","                entity_id = f\"entity_{hash(entity_text) % 100000}\"\n","\n","                if entity_id not in node_id_map:\n","                    nodes.append({\n","                        'id': entity_id,\n","                        'label': entity_text,\n","                        'type': entity['type'],\n","                        'properties': {\n","                            'context': entity.get('context', ''),\n","                            'source_document': doc_id\n","                        }\n","                    })\n","                    node_id_map[entity_id] = entity_text\n","\n","                # Add document-entity relationship\n","                edges.append({\n","                    'source': doc_node_id,\n","                    'target': entity_id,\n","                    'relationship': 'CONTAINS',\n","                    'properties': {}\n","                })\n","\n","            # Add relationship edges\n","            for relationship in knowledge.get('relationships', []):\n","                source_text = relationship.get('source', '')\n","                target_text = relationship.get('target', '')\n","\n","                # Find corresponding node IDs\n","                source_id = None\n","                target_id = None\n","\n","                for node_id, node_text in node_id_map.items():\n","                    if source_text == node_text:\n","                        source_id = node_id\n","                    if target_text == node_text:\n","                        target_id = node_id\n","\n","                if source_id and target_id:\n","                    edges.append({\n","                        'source': source_id,\n","                        'target': target_id,\n","                        'relationship': relationship.get('relationship', 'RELATED'),\n","                        'properties': {\n","                            'confidence': relationship.get('confidence', 0.5),\n","                            'evidence': relationship.get('evidence', ''),\n","                            'source_document': doc_id\n","                        }\n","                    })\n","\n","        return {\n","            'nodes': nodes,\n","            'edges': edges,\n","            'metadata': {\n","                'total_nodes': len(nodes),\n","                'total_edges': len(edges),\n","                'extraction_timestamp': pd.Timestamp.now().isoformat()\n","            }\n","        }\n","\n","    def save_for_next_notebook(self, extracted_knowledge: List[Dict]):\n","        \"\"\"Save processed data for graph construction notebook.\"\"\"\n","\n","        graph_data = self.to_graph_format(extracted_knowledge)\n","\n","        final_export = {\n","            'extracted_knowledge': extracted_knowledge,\n","            'graph_data': graph_data,\n","            'extraction_metadata': {\n","                'extraction_date': pd.Timestamp.now().isoformat(),\n","                'source_documents': len(sample_documents),\n","                'total_entities': sum(len(k.get('entities', [])) for k in extracted_knowledge),\n","                'total_relationships': sum(len(k.get('relationships', [])) for k in extracted_knowledge)\n","            }\n","        }\n","\n","        # Save to JSON for next notebook\n","        with open('processed_knowledge_for_graph.json', 'w') as f:\n","            json.dump(final_export, f, indent=2)\n","\n","        print(\"âœ… Saved processed knowledge to 'processed_knowledge_for_graph.json'\")\n","        return final_export\n","\n","# Export data for next notebook\n","exporter = KnowledgeExporter()\n","final_data = exporter.save_for_next_notebook(extracted_knowledge)\n","\n","print(f\"\\nðŸ“‹ Ready for Graph Construction:\")\n","print(f\"   â€¢ {final_data['extraction_metadata']['total_entities']} entities extracted\")\n","print(f\"   â€¢ {final_data['extraction_metadata']['total_relationships']} relationships identified\")\n","print(f\"   â€¢ Data saved for Notebook 12.3.2 (Graph Construction)\")\n","\n","print(f\"\\nðŸŽ¯ Next Steps:\")\n","print(\"   1. Load this data in Notebook 12.3.2\")\n","print(\"   2. Create Neo4j nodes and relationships\")\n","print(\"   3. Implement entity linking and deduplication\")\n","print(\"   4. Build complete knowledge graph\")"],"metadata":{"id":"FVpBGxlYBMrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Key Achievements\n","\n","âœ… **Automated Knowledge Extraction**: LLM-powered extraction of entities and relationships\n","âœ… **Structured Output**: Well-formatted data ready for graph construction  \n","âœ… **Quality Processing**: Error handling and validation throughout the pipeline\n","âœ… **Scalable Approach**: Designed to handle large document collections\n","âœ… **Export Ready**: Multiple formats for downstream graph construction\n","\n","Continue to Notebook 12.3.2 to transform this extracted knowledge into a complete knowledge graph!"],"metadata":{"id":"OERH85BeBnNP"}}]}