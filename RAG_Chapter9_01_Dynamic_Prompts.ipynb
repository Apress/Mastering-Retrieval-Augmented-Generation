{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGgcBGUH5ier2iNafe8tsJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"By6sLyqqJc1C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUTybbXMJIHY"},"outputs":[],"source":["!pip install langchain langchain-openai tiktoken faiss-cpu\n","\n","import os\n","import re\n","import json\n","import tiktoken\n","from typing import List, Dict, Any\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","from langchain.chains import LLMChain\n","from langchain_core.documents import Document"]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"Za9mUjzeJqDD"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")"],"metadata":{"id":"guOyLI9AJqP7","executionInfo":{"status":"ok","timestamp":1740840064880,"user_tz":-330,"elapsed":21,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: Variable Substitution**"],"metadata":{"id":"0V1Js5XLJ2Cf"}},{"cell_type":"code","source":["print(\"Section 1: Variable Substitution - Basic Template Example\")\n","\n","template = \"\"\"\n","Answer the question based on the context below.\n","\n","Context: {context}\n","\n","Question: {question}\n","\n","Answer:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")\n","\n","# Example usage\n","formatted_prompt = prompt.format(\n","    context=\"The capital of France is Paris. It is known for the Eiffel Tower.\",\n","    question=\"What is the capital of France?\"\n",")\n","\n","print(formatted_prompt)\n","print_separator()"],"metadata":{"id":"39DfiiVQJ2LL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: Programmatic Template Selection**"],"metadata":{"id":"A2lsu25yJ_I8"}},{"cell_type":"code","source":["print(\"Section 2: Programmatic Template Selection\")\n","\n","# Define different template types\n","qa_template = \"\"\"\n","Answer the question based solely on the provided context.\n","\n","Context: {context}\n","\n","Question: {question}\n","\n","Answer:\n","\"\"\"\n","\n","comparison_template = \"\"\"\n","Compare and contrast the following entities based on the provided context.\n","\n","Context: {context}\n","\n","Entities to compare: {entities}\n","\n","Comparison:\n","\"\"\"\n","\n","summarization_template = \"\"\"\n","Summarize the following information.\n","\n","Context: {context}\n","\n","Summary:\n","\"\"\"\n","\n","def select_template(query: str) -> PromptTemplate:\n","    \"\"\"Select the appropriate template based on query analysis.\"\"\"\n","    comparison_terms = [\"compare\", \"difference\", \"versus\", \"vs\", \"similarities\", \"different\"]\n","    summarization_terms = [\"summarize\", \"summary\", \"overview\", \"brief\"]\n","\n","    # Convert templates to PromptTemplate objects\n","    templates = {\n","        \"qa\": PromptTemplate(\n","            input_variables=[\"context\", \"question\"],\n","            template=qa_template\n","        ),\n","        \"comparison\": PromptTemplate(\n","            input_variables=[\"context\", \"entities\"],\n","            template=comparison_template\n","        ),\n","        \"summarization\": PromptTemplate(\n","            input_variables=[\"context\"],\n","            template=summarization_template\n","        )\n","    }\n","\n","    # Template selection logic\n","    if any(term in query.lower() for term in comparison_terms):\n","        print(f\"Selected template: comparison\")\n","        return templates[\"comparison\"]\n","    elif any(term in query.lower() for term in summarization_terms):\n","        print(f\"Selected template: summarization\")\n","        return templates[\"summarization\"]\n","    else:\n","        print(f\"Selected template: question-answering\")\n","        return templates[\"qa\"]\n","\n","# Test with different queries\n","test_queries = [\n","    \"What is the capital of France?\",\n","    \"Compare the benefits of Python and JavaScript\",\n","    \"Summarize the key points about climate change\"\n","]\n","\n","for query in test_queries:\n","    print(f\"\\nQuery: {query}\")\n","    template = select_template(query)\n","    print(f\"Template variables: {template.input_variables}\")\n","\n","print_separator()"],"metadata":{"id":"cRuW4tyjJ_6F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Context Window Management**"],"metadata":{"id":"izOJxLUsKJ9q"}},{"cell_type":"code","source":["print(\"Section 3: Context Window Management\")\n","\n","def fit_documents_to_token_limit(\n","    docs: List[Document],\n","    token_limit: int,\n","    model: str = \"gpt-3.5-turbo\"\n",") -> str:\n","    \"\"\"Process documents to fit within token limit.\"\"\"\n","    processed_docs = []\n","    current_tokens = 0\n","\n","    for i, doc in enumerate(docs):\n","        # Add document separator\n","        doc_text = f\"\\n[Document {i+1}]: {doc.page_content}\\n\"\n","        doc_tokens = count_tokens(doc_text, model)\n","\n","        if current_tokens + doc_tokens <= token_limit:\n","            processed_docs.append(doc_text)\n","            current_tokens += doc_tokens\n","        else:\n","            # If the first document is already too large, truncate it\n","            if i == 0:\n","                encoder = tiktoken.encoding_for_model(model)\n","                tokens = encoder.encode(doc_text)\n","                truncated_tokens = tokens[:token_limit]\n","                truncated_text = encoder.decode(truncated_tokens)\n","                processed_docs.append(truncated_text)\n","                current_tokens = token_limit\n","                break\n","            else:\n","                # Otherwise, we've reached our limit with previous docs\n","                break\n","\n","    return \"\".join(processed_docs)\n","\n","def create_dynamic_prompt(\n","    query: str,\n","    retrieved_docs: List[Document],\n","    max_tokens: int = 3500,\n","    model: str = \"gpt-3.5-turbo\"\n",") -> str:\n","    \"\"\"Create a dynamic prompt, fitting retrieved docs into the token limit.\"\"\"\n","    # Basic template parts\n","    system_instruction = \"You are an assistant that answers based on context.\"\n","    query_section = f\"Question: {query}\"\n","    answer_section = \"Answer:\"\n","\n","    # Calculate tokens for the base template\n","    template_skeleton = f\"{system_instruction}\\n\\nContext: [PLACEHOLDER]\\n\\n{query_section}\\n\\n{answer_section}\"\n","    base_tokens = count_tokens(template_skeleton.replace(\"[PLACEHOLDER]\", \"\"), model)\n","\n","    # Calculate available tokens for context\n","    available_context_tokens = max_tokens - base_tokens\n","\n","    # Process documents to fit within available tokens\n","    processed_docs = fit_documents_to_token_limit(retrieved_docs, available_context_tokens, model)\n","\n","    # Construct the final prompt\n","    prompt = template_skeleton.replace(\"[PLACEHOLDER]\", processed_docs)\n","\n","    return prompt\n","\n","# Sample documents for testing\n","sample_docs = [\n","    Document(page_content=\"Paris is the capital of France. It is known for the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\"),\n","    Document(page_content=\"France is a country in Western Europe. Its capital is Paris. France is known for its cuisine, culture, and history.\"),\n","    Document(page_content=\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after engineer Gustave Eiffel.\")\n","]\n","\n","# Test the dynamic prompt construction\n","query = \"What is the capital of France and what is it known for?\"\n","dynamic_prompt = create_dynamic_prompt(query, sample_docs, max_tokens=1000)\n","print(f\"Dynamic prompt (token count: {count_tokens(dynamic_prompt)}):\")\n","print(dynamic_prompt)\n","\n","print_separator()"],"metadata":{"id":"1VP-dHiPKKGv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Document Prioritization**"],"metadata":{"id":"uwR1yEhQKVAr"}},{"cell_type":"code","source":["print(\"Section 4: Document Prioritization\")\n","\n","def add_relevance_scores(docs: List[Document], query: str) -> List[Document]:\n","    \"\"\"\n","    Simulate relevance scoring for documents.\n","    In a real system, this would come from the vector store retrieval.\n","    \"\"\"\n","    scored_docs = []\n","\n","    for doc in docs:\n","        # Simple mock relevance calculation (just for demonstration)\n","        relevance = 0.0\n","        doc_text = doc.page_content.lower()\n","        query_terms = query.lower().split()\n","\n","        for term in query_terms:\n","            if term in doc_text:\n","                relevance += 0.2 + (doc_text.count(term) * 0.05)\n","\n","        # Ensure the score is between 0 and 1\n","        relevance = min(max(relevance, 0.1), 1.0)\n","\n","        # Add the score to the document metadata\n","        metadata = doc.metadata.copy() if hasattr(doc, 'metadata') else {}\n","        metadata['relevance_score'] = relevance\n","\n","        # Create a new document with the updated metadata\n","        scored_doc = Document(page_content=doc.page_content, metadata=metadata)\n","        scored_docs.append(scored_doc)\n","\n","    return scored_docs\n","\n","def prioritize_documents(docs: List[Document]) -> List[Document]:\n","    \"\"\"Prioritize documents based on relevance score.\"\"\"\n","    if not docs:\n","        return []\n","\n","    # Check if documents have relevance scores\n","    if 'relevance_score' not in docs[0].metadata:\n","        return docs  # Return original if no scores\n","\n","    # Sort by relevance score (descending)\n","    sorted_docs = sorted(docs, key=lambda x: x.metadata.get('relevance_score', 0), reverse=True)\n","\n","    # Group documents by relevance score\n","    high_relevance = [doc for doc in sorted_docs if doc.metadata.get('relevance_score', 0) > 0.7]\n","    medium_relevance = [doc for doc in sorted_docs if 0.4 <= doc.metadata.get('relevance_score', 0) <= 0.7]\n","    low_relevance = [doc for doc in sorted_docs if doc.metadata.get('relevance_score', 0) < 0.4]\n","\n","    # Combine with priority weighting\n","    return high_relevance + medium_relevance + low_relevance\n","\n","# Test document prioritization\n","query = \"What is Paris known for?\"\n","scored_docs = add_relevance_scores(sample_docs, query)\n","\n","print(\"Documents with relevance scores:\")\n","for i, doc in enumerate(scored_docs):\n","    print(f\"Document {i+1} (score: {doc.metadata.get('relevance_score', 0):.2f}):\")\n","    print(f\"  {doc.page_content}\")\n","\n","prioritized_docs = prioritize_documents(scored_docs)\n","\n","print(\"\\nPrioritized documents:\")\n","for i, doc in enumerate(prioritized_docs):\n","    print(f\"Document {i+1} (score: {doc.metadata.get('relevance_score', 0):.2f}):\")\n","    print(f\"  {doc.page_content}\")\n","\n","print_separator()"],"metadata":{"id":"cVe24VKkKVKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Content Transformation**"],"metadata":{"id":"PaiKTRWaKhdi"}},{"cell_type":"code","source":["print(\"Section 5: Content Transformation\")\n","\n","def extract_relevant_passages(doc: Document, query: str, max_length: int = 200) -> str:\n","    \"\"\"\n","    Extract the most relevant passage from a document.\n","    In a real system, this might use more sophisticated techniques.\n","    \"\"\"\n","    if not doc or not doc.page_content:\n","        return \"\"\n","\n","    text = doc.page_content\n","\n","    # If text is already short enough, return it all\n","    if len(text) <= max_length:\n","        return text\n","\n","    # Simple approach: find query terms in text\n","    query_terms = set(query.lower().split())\n","    best_passage = \"\"\n","    highest_term_count = 0\n","\n","    # Create overlapping windows of text\n","    window_size = min(max_length, len(text))\n","    step_size = max(window_size // 4, 1)\n","\n","    for i in range(0, len(text) - window_size + 1, step_size):\n","        passage = text[i:i + window_size]\n","        passage_lower = passage.lower()\n","        term_count = sum(1 for term in query_terms if term in passage_lower)\n","\n","        if term_count > highest_term_count:\n","            highest_term_count = term_count\n","            best_passage = passage\n","\n","    # If no terms found, take the beginning of the document\n","    if not best_passage:\n","        best_passage = text[:max_length]\n","\n","    return best_passage\n","\n","def transform_document(doc: Document, query: str) -> str:\n","    \"\"\"Transform a document for inclusion in a prompt.\"\"\"\n","    if not doc:\n","        return \"\"\n","\n","    # Extract relevant passage\n","    relevant_passage = extract_relevant_passages(doc, query)\n","\n","    # Add source metadata\n","    doc_id = doc.metadata.get('doc_id', 'unknown')\n","    source = doc.metadata.get('source', 'unknown')\n","\n","    return f\"[Document {doc_id} from {source}]:\\n{relevant_passage}\"\n","\n","# Add some metadata to our test documents\n","for i, doc in enumerate(sample_docs):\n","    doc.metadata = {'doc_id': i+1, 'source': f'source_{i+1}'}\n","\n","# Test content transformation\n","query = \"What is the Eiffel Tower?\"\n","for doc in sample_docs:\n","    transformed_content = transform_document(doc, query)\n","    print(transformed_content)\n","    print(\"-\" * 30)\n","\n","print_separator()"],"metadata":{"id":"WAniwUFDKhlq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Adaptive Instruction Calibration**"],"metadata":{"id":"p7cDPRDzKqkq"}},{"cell_type":"code","source":["print(\"Section 6: Adaptive Instruction Calibration\")\n","\n","def retrieval_confidence(docs: List[Document]) -> float:\n","    \"\"\"Calculate overall confidence in retrieved documents.\"\"\"\n","    if not docs:\n","        return 0.0\n","\n","    # Get average relevance score\n","    scores = [doc.metadata.get('relevance_score', 0) for doc in docs]\n","    avg_score = sum(scores) / len(scores) if scores else 0\n","\n","    return avg_score\n","\n","def requires_reasoning(query: str) -> bool:\n","    \"\"\"Determine if a query requires explicit reasoning.\"\"\"\n","    reasoning_indicators = [\n","        \"why\", \"how\", \"explain\", \"reason\", \"analyze\", \"compare\",\n","        \"evaluate\", \"synthesize\", \"interpret\", \"assess\"\n","    ]\n","\n","    return any(indicator in query.lower() for indicator in reasoning_indicators)\n","\n","def calibrate_instructions(query: str, retrieved_docs: List[Document]) -> str:\n","    \"\"\"Dynamically calibrate system instructions based on query and retrieval.\"\"\"\n","    # Base instructions\n","    instructions = \"Answer based on the context provided.\"\n","\n","    # Assess retrieval quality\n","    confidence = retrieval_confidence(retrieved_docs)\n","\n","    if confidence < 0.5:\n","        instructions += \" If the context doesn't contain sufficient information, clearly state what's missing.\"\n","\n","    # Analyze query complexity\n","    if requires_reasoning(query):\n","        instructions += \" Break down your reasoning process step by step.\"\n","\n","    # Check for numerical/factual questions\n","    if any(word in query.lower() for word in [\"how many\", \"when\", \"where\", \"who\", \"which\"]):\n","        instructions += \" Be precise with factual information.\"\n","\n","    # Check for definitional questions\n","    if query.lower().startswith(\"what is\") or \"define\" in query.lower():\n","        instructions += \" Provide a clear, concise definition before elaborating.\"\n","\n","    return instructions\n","\n","# Test instruction calibration\n","test_queries = [\n","    \"What is the population of Paris?\",\n","    \"Why is the Eiffel Tower significant?\",\n","    \"Where is the Louvre Museum located?\",\n","    \"How does French culture influence its cuisine?\"\n","]\n","\n","for query in test_queries:\n","    print(f\"Query: {query}\")\n","    instructions = calibrate_instructions(query, scored_docs)\n","    print(f\"Calibrated instructions: {instructions}\")\n","    print(\"-\" * 30)\n","\n","print_separator()"],"metadata":{"id":"yWmLuZwAKqtZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 7: Putting It All Together**"],"metadata":{"id":"KTWiLlxdK0AF"}},{"cell_type":"code","source":["print(\"Section 7: Putting It All Together\")\n","\n","# This function integrates all the components\n","def build_dynamic_rag_prompt(query: str, docs: List[Document], max_tokens: int = 3500) -> str:\n","    \"\"\"\n","    Comprehensive function to build a dynamic RAG prompt:\n","    1. Score and prioritize documents\n","    2. Transform documents for inclusion\n","    3. Calibrate instructions\n","    4. Fit within token limits\n","    \"\"\"\n","    # If no API key, just show the process without making API calls\n","    if not os.environ.get(\"OPENAI_API_KEY\"):\n","        print(\"No OpenAI API key found - demonstrating prompt building only\")\n","\n","    # 1. Score and prioritize documents\n","    if 'relevance_score' not in (docs[0].metadata if docs else {}):\n","        scored_docs = add_relevance_scores(docs, query)\n","    else:\n","        scored_docs = docs\n","\n","    prioritized_docs = prioritize_documents(scored_docs)\n","\n","    # 2. Transform documents\n","    transformed_docs = [\n","        transform_document(doc, query) for doc in prioritized_docs\n","    ]\n","\n","    # 3. Calibrate instructions\n","    system_instruction = calibrate_instructions(query, scored_docs)\n","\n","    # 4. Calculate token budgets\n","    query_section = f\"Question: {query}\"\n","    answer_section = \"Answer:\"\n","\n","    # Calculate tokens for the base template\n","    template_skeleton = f\"{system_instruction}\\n\\nContext: [PLACEHOLDER]\\n\\n{query_section}\\n\\n{answer_section}\"\n","    base_tokens = count_tokens(template_skeleton.replace(\"[PLACEHOLDER]\", \"\"))\n","\n","    # Calculate available tokens for context\n","    available_context_tokens = max_tokens - base_tokens\n","\n","    # 5. Fit documents within token limit\n","    context_text = \"\"\n","    current_tokens = 0\n","\n","    for doc_text in transformed_docs:\n","        doc_tokens = count_tokens(doc_text)\n","        if current_tokens + doc_tokens <= available_context_tokens:\n","            context_text += doc_text + \"\\n\\n\"\n","            current_tokens += doc_tokens\n","        else:\n","            break\n","\n","    # 6. Construct the final prompt\n","    final_prompt = template_skeleton.replace(\"[PLACEHOLDER]\", context_text.strip())\n","\n","    return final_prompt\n","\n","# Test the integrated approach\n","query = \"Why is the Eiffel Tower important to Paris and what are its key features?\"\n","final_prompt = build_dynamic_rag_prompt(query, sample_docs, max_tokens=2000)\n","\n","print(\"Final dynamic RAG prompt:\")\n","print(final_prompt)\n","print(f\"\\nTotal token count: {count_tokens(final_prompt)}\")\n","\n","print_separator()"],"metadata":{"id":"EKAaWHhRK0Jr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bonus: LangChain Implementation Example**"],"metadata":{"id":"P-QYYKjrK-GW"}},{"cell_type":"code","source":["print(\"Bonus: LangChain Implementation Example\")\n","\n","# Note: This will only run if you have set your OpenAI API key\n","try:\n","    llm = OpenAI(temperature=0)\n","\n","    def generate_rag_response(query: str, docs: List[Document]) -> str:\n","        \"\"\"Generate a response using a dynamic RAG prompt.\"\"\"\n","        # Build dynamic prompt\n","        prompt_text = build_dynamic_rag_prompt(query, docs)\n","\n","        # Create prompt template\n","        prompt = PromptTemplate.from_template(\"{prompt}\")\n","\n","        # Create chain\n","        chain = LLMChain(llm=llm, prompt=prompt)\n","\n","        # Run chain\n","        response = chain.run(prompt=prompt_text)\n","\n","        return response\n","\n","    # Only execute if API key is available\n","    if os.environ.get(\"OPENAI_API_KEY\"):\n","        print(\"Using OpenAI to generate response...\")\n","        response = generate_rag_response(\n","            \"What features make the Eiffel Tower unique?\",\n","            sample_docs\n","        )\n","        print(\"\\nGenerated Response:\")\n","        print(response)\n","    else:\n","        print(\"OpenAI API key not set. Skipping response generation.\")\n","except Exception as e:\n","    print(f\"Error in LangChain implementation: {e}\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")"],"metadata":{"id":"-li_NOFXK-QD"},"execution_count":null,"outputs":[]}]}