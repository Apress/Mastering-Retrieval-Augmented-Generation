{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPETFI/a3df1Kv8D7xMRDvu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gLIUPzfe5x-u"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Web Content Processing in RAG Systems - Part 1: Setup and Examples\n","\n","This notebook is Part 1 of our web content processing series, accompanying Chapter 4\n","of \"Mastering Retrieval Augmented Generation\". We'll establish our development\n","environment and create example web content that demonstrates various scenarios\n","you'll encounter in real-world applications.\n","\n","## What's in this Series\n","1. Part 1 (Current): Setup and Web Content Examples\n","2. Part 2: Processing Static HTML Content\n","3. Part 3: Handling Dynamic Web Content\n","4. Part 4: Working with Web APIs and JSON\n","5. Part 5: Performance Optimization and Error Handling\n","\n","Let's begin by setting up our environment with the necessary tools for web content processing.\n","\"\"\"\n","\n","# First, let's install all necessary packages\n","!pip install beautifulsoup4 requests html5lib selenium webdriver_manager\n","!pip install playwright\n","!playwright install chromium\n","\n","import os\n","import json\n","import requests\n","from bs4 import BeautifulSoup\n","from typing import List, Dict, Any, Optional\n","import logging\n","from pathlib import Path\n","from datetime import datetime\n","import asyncio\n","from playwright.async_api import async_playwright\n","import re\n","\n","# Set up logging for better visibility into our operations\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Understanding Our Tools\n","\n","Before we create sample content, let's understand the key libraries we'll be using:\n","\n","1. BeautifulSoup4: For parsing and navigating HTML/XML content\n","2. Playwright: For handling dynamic web content and JavaScript\n","3. Requests: For making HTTP requests and handling static content\n","4. html5lib: For robust HTML parsing\n","\n","Each tool serves a specific purpose in our web content processing toolkit.\n","\"\"\"\n","\n","# Create a directory for our sample content\n","!mkdir -p rag_web_samples\n","\n","\"\"\"## Creating Sample Web Content\n","\n","We'll create various types of web content that demonstrate different scenarios\n","you'll encounter in real-world RAG systems. This includes static HTML, dynamic\n","content, and different structural patterns.\n","\"\"\"\n","\n","def create_static_html_sample():\n","    \"\"\"\n","    Creates a static HTML file demonstrating common web content patterns:\n","    - Article content with metadata\n","    - Navigation structures\n","    - Lists and tables\n","    - Semantic HTML elements\n","    \"\"\"\n","    static_html = \"\"\"\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"description\" content=\"Sample article about AI technology\">\n","    <meta name=\"keywords\" content=\"AI, Machine Learning, Technology\">\n","    <title>Understanding AI Technologies</title>\n","</head>\n","<body>\n","    <header>\n","        <nav>\n","            <ul>\n","                <li><a href=\"#intro\">Introduction</a></li>\n","                <li><a href=\"#main\">Main Content</a></li>\n","                <li><a href=\"#conclusion\">Conclusion</a></li>\n","            </ul>\n","        </nav>\n","    </header>\n","\n","    <main>\n","        <article>\n","            <h1>Understanding AI Technologies</h1>\n","            <div class=\"metadata\">\n","                <p>Author: Jane Smith</p>\n","                <p>Published: 2025-02-08</p>\n","                <p>Category: Technology</p>\n","            </div>\n","\n","            <section id=\"intro\">\n","                <h2>Introduction</h2>\n","                <p>Artificial Intelligence has transformed various industries...</p>\n","            </section>\n","\n","            <section id=\"main\">\n","                <h2>Key AI Technologies</h2>\n","                <table>\n","                    <thead>\n","                        <tr>\n","                            <th>Technology</th>\n","                            <th>Description</th>\n","                            <th>Use Cases</th>\n","                        </tr>\n","                    </thead>\n","                    <tbody>\n","                        <tr>\n","                            <td>Machine Learning</td>\n","                            <td>Systems that learn from data</td>\n","                            <td>Prediction, Classification</td>\n","                        </tr>\n","                        <tr>\n","                            <td>Natural Language Processing</td>\n","                            <td>Processing human language</td>\n","                            <td>Translation, Chatbots</td>\n","                        </tr>\n","                    </tbody>\n","                </table>\n","            </section>\n","\n","            <section id=\"conclusion\">\n","                <h2>Conclusion</h2>\n","                <p>The future of AI technology looks promising...</p>\n","            </section>\n","        </article>\n","    </main>\n","\n","    <footer>\n","        <p>Â© 2025 AI Technology Review</p>\n","    </footer>\n","</body>\n","</html>\n","\"\"\".strip()\n","\n","    with open('rag_web_samples/static_article.html', 'w') as f:\n","        f.write(static_html)\n","    logger.info(\"Created static HTML sample\")\n","\n","def create_dynamic_html_sample():\n","    \"\"\"\n","    Creates an HTML file with dynamic content loaded via JavaScript:\n","    - Async data loading\n","    - Interactive elements\n","    - Dynamic updates\n","    \"\"\"\n","    dynamic_html = \"\"\"\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <title>Dynamic Content Example</title>\n","</head>\n","<body>\n","    <div id=\"app\">\n","        <h1>Real-time Data Dashboard</h1>\n","\n","        <!-- Dynamically loaded content -->\n","        <div id=\"data-container\">\n","            Loading data...\n","        </div>\n","\n","        <!-- Interactive elements -->\n","        <div class=\"controls\">\n","            <button onclick=\"loadData()\">Refresh Data</button>\n","        </div>\n","    </div>\n","\n","    <script>\n","        // Simulate dynamic data loading\n","        async function loadData() {\n","            const container = document.getElementById('data-container');\n","            container.innerHTML = 'Loading...';\n","\n","            // Simulate API call\n","            const data = {\n","                timestamp: new Date().toISOString(),\n","                metrics: {\n","                    users: Math.floor(Math.random() * 1000),\n","                    transactions: Math.floor(Math.random() * 500),\n","                    revenue: Math.floor(Math.random() * 10000)\n","                }\n","            };\n","\n","            // Update display\n","            container.innerHTML = `\n","                <div class=\"metrics\">\n","                    <p>Last Updated: ${data.timestamp}</p>\n","                    <ul>\n","                        <li>Active Users: ${data.metrics.users}</li>\n","                        <li>Transactions: ${data.metrics.transactions}</li>\n","                        <li>Revenue: $${data.metrics.revenue}</li>\n","                    </ul>\n","                </div>\n","            `;\n","        }\n","\n","        // Initial load\n","        document.addEventListener('DOMContentLoaded', loadData);\n","    </script>\n","</body>\n","</html>\n","\"\"\".strip()\n","\n","    with open('rag_web_samples/dynamic_dashboard.html', 'w') as f:\n","        f.write(dynamic_html)\n","    logger.info(\"Created dynamic HTML sample\")\n","\n","def create_embedded_json_sample():\n","    \"\"\"\n","    Creates an HTML file with embedded JSON data:\n","    - JSON-LD structured data\n","    - Application state\n","    - Configuration data\n","    \"\"\"\n","    embedded_json_html = \"\"\"\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <title>Product Catalog</title>\n","\n","    <!-- JSON-LD structured data -->\n","    <script type=\"application/ld+json\">\n","    {\n","        \"@context\": \"https://schema.org\",\n","        \"@type\": \"Product\",\n","        \"name\": \"Smart Home Hub\",\n","        \"description\": \"Central control for your smart home devices\",\n","        \"brand\": {\n","            \"@type\": \"Brand\",\n","            \"name\": \"TechHome\"\n","        },\n","        \"offers\": {\n","            \"@type\": \"Offer\",\n","            \"price\": \"199.99\",\n","            \"priceCurrency\": \"USD\"\n","        }\n","    }\n","    </script>\n","</head>\n","<body>\n","    <div id=\"product-catalog\">\n","        <h1>Product Catalog</h1>\n","\n","        <!-- Product data will be loaded here -->\n","        <div id=\"products\"></div>\n","    </div>\n","\n","    <!-- Embedded application data -->\n","    <script>\n","        const appConfig = {\n","            apiEndpoint: \"/api/products\",\n","            updateInterval: 300,\n","            features: {\n","                realTimePricing: true,\n","                inventoryTracking: true\n","            }\n","        };\n","\n","        const initialState = {\n","            products: [\n","                {\n","                    id: \"SHH-001\",\n","                    name: \"Smart Home Hub\",\n","                    price: 199.99,\n","                    stock: 45\n","                },\n","                {\n","                    id: \"SSB-002\",\n","                    name: \"Smart Security Bundle\",\n","                    price: 299.99,\n","                    stock: 30\n","                }\n","            ],\n","            lastUpdate: \"2025-02-08T10:00:00Z\"\n","        };\n","    </script>\n","</body>\n","</html>\n","\"\"\".strip()\n","\n","    with open('rag_web_samples/embedded_json.html', 'w') as f:\n","        f.write(embedded_json_html)\n","    logger.info(\"Created embedded JSON sample\")\n","\n","# Create all our sample files\n","create_static_html_sample()\n","create_dynamic_html_sample()\n","create_embedded_json_sample()\n","\n","\"\"\"## Understanding Our Sample Content\n","\n","Let's examine the sample files we've created and understand their characteristics:\n","\n","1. static_article.html:\n","   - Demonstrates semantic HTML structure\n","   - Contains metadata in various forms\n","   - Includes tables and lists\n","   - Uses proper HTML5 sectioning\n","\n","2. dynamic_dashboard.html:\n","   - Shows client-side data loading\n","   - Includes interactive elements\n","   - Demonstrates state management\n","   - Uses asynchronous operations\n","\n","3. embedded_json.html:\n","   - Contains structured JSON-LD data\n","   - Demonstrates application configuration\n","   - Shows state management patterns\n","   - Includes multiple JSON formats\n","\n","These samples will help us explore different aspects of web content processing\n","in the following notebooks.\n","\n","## Verifying Our Samples\n","\n","Let's verify our sample files and examine their characteristics:\n","\"\"\"\n","\n","def analyze_samples():\n","    \"\"\"Analyze and display information about our sample files.\"\"\"\n","    sample_dir = Path('rag_web_samples')\n","\n","    print(\"Sample Files Analysis:\")\n","    print(\"-\" * 50)\n","\n","    for file_path in sample_dir.glob('*.html'):\n","        size = file_path.stat().st_size\n","\n","        with open(file_path, 'r') as f:\n","            content = f.read()\n","            soup = BeautifulSoup(content, 'html5lib')\n","\n","            # Analyze content\n","            scripts = len(soup.find_all('script'))\n","            json_ld = len(soup.find_all('script', {'type': 'application/ld+json'}))\n","\n","            print(f\"\\nFile: {file_path.name}\")\n","            print(f\"Size: {size} bytes\")\n","            print(f\"Scripts: {scripts}\")\n","            print(f\"JSON-LD blocks: {json_ld}\")\n","            print(f\"Main heading: {soup.find('h1').text if soup.find('h1') else 'None'}\")\n","\n","# Run the analysis\n","analyze_samples()\n","\n","\"\"\"## What's Next?\n","\n","In Part 2, we'll begin implementing processors for static HTML content, building\n","upon these samples to create robust content extraction capabilities. We'll focus on:\n","\n","1. HTML parsing and navigation\n","2. Content extraction strategies\n","3. Metadata handling\n","4. Structure preservation\n","\n","The sample files we've created here will serve as our test cases throughout\n","the series, helping us validate our implementations against real-world scenarios.\"\"\""]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Web Content Processing in RAG Systems - Part 2: Static HTML Processing\n","\n","This notebook focuses on processing static HTML content effectively in RAG systems.\n","We'll build a robust HTML processor that can extract meaningful content while\n","preserving important structural relationships and metadata.\n","\n","Make sure you've run Part 1 first to create the sample files we'll use here.\n","\"\"\"\n","\n","# First, let's ensure we have all necessary packages\n","!pip install beautifulsoup4 html5lib lxml pandas\n","\n","import os\n","import json\n","from bs4 import BeautifulSoup\n","from typing import List, Dict, Any, Optional\n","import logging\n","from pathlib import Path\n","from datetime import datetime\n","import pandas as pd\n","import re\n","\n","# Set up logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Understanding HTML Processing Challenges\n","\n","Before we dive into implementation, let's understand the key challenges in\n","processing HTML content for RAG systems:\n","\n","1. Structure Preservation: We need to maintain the hierarchical relationships\n","   between different parts of the content.\n","\n","2. Content Classification: Different parts of an HTML page serve different\n","   purposes (navigation, main content, sidebars, etc.).\n","\n","3. Metadata Extraction: HTML pages often contain rich metadata in various\n","   forms (meta tags, JSON-LD, Open Graph tags).\n","\n","4. Text Cleaning: HTML content often needs cleaning to remove boilerplate,\n","   advertisements, and irrelevant content.\n","\n","Our implementation will address each of these challenges systematically.\n","\"\"\"\n","\n","class HTMLProcessor:\n","    \"\"\"\n","    A comprehensive HTML processor designed for RAG systems.\n","    Extracts and structures content while preserving important relationships\n","    and metadata.\n","    \"\"\"\n","\n","    def __init__(self, html_content: str):\n","        \"\"\"\n","        Initialize the HTML processor with content.\n","\n","        Args:\n","            html_content: Raw HTML content to process\n","        \"\"\"\n","        # Parse with html5lib for maximum compatibility\n","        self.soup = BeautifulSoup(html_content, 'html5lib')\n","        self.metadata = {}\n","        self.content_blocks = []\n","\n","    def extract_metadata(self) -> Dict[str, Any]:\n","        \"\"\"\n","        Extract comprehensive metadata from the HTML document.\n","        Handles various metadata formats including meta tags, JSON-LD,\n","        and Open Graph tags.\n","\n","        Returns:\n","            Dictionary containing extracted metadata\n","        \"\"\"\n","        metadata = {\n","            'title': None,\n","            'description': None,\n","            'keywords': None,\n","            'author': None,\n","            'published_date': None,\n","            'modified_date': None,\n","            'structured_data': [],\n","            'open_graph': {}\n","        }\n","\n","        # Extract basic metadata\n","        title_tag = self.soup.find('title')\n","        metadata['title'] = title_tag.text.strip() if title_tag else None\n","\n","        # Process meta tags\n","        for meta in self.soup.find_all('meta'):\n","            name = meta.get('name', '').lower()\n","            property = meta.get('property', '').lower()\n","            content = meta.get('content', '')\n","\n","            if name == 'description':\n","                metadata['description'] = content\n","            elif name == 'keywords':\n","                metadata['keywords'] = [k.strip() for k in content.split(',')]\n","            elif name == 'author':\n","                metadata['author'] = content\n","            elif name in ['published_time', 'article:published_time']:\n","                metadata['published_date'] = content\n","            elif name in ['modified_time', 'article:modified_time']:\n","                metadata['modified_date'] = content\n","            elif property.startswith('og:'):  # Open Graph tags\n","                metadata['open_graph'][property[3:]] = content\n","\n","        # Extract JSON-LD structured data\n","        for script in self.soup.find_all('script', type='application/ld+json'):\n","            try:\n","                json_data = json.loads(script.string)\n","                metadata['structured_data'].append(json_data)\n","            except (json.JSONDecodeError, TypeError) as e:\n","                logger.warning(f\"Error parsing JSON-LD: {str(e)}\")\n","\n","        self.metadata = metadata\n","        return metadata\n","\n","    def extract_main_content(self) -> List[Dict[str, Any]]:\n","        \"\"\"\n","        Extract the main content from the HTML document.\n","        Uses heuristics to identify and extract meaningful content blocks.\n","\n","        Returns:\n","            List of dictionaries containing content blocks with their metadata\n","        \"\"\"\n","        content_blocks = []\n","\n","        # Find the main content area\n","        main_content = self.soup.find(['main', 'article']) or self.soup.find(\n","            ['div', 'section'],\n","            class_=re.compile(r'(content|article|post)'))\n","\n","        if not main_content:\n","            logger.warning(\"No main content area found, processing entire body\")\n","            main_content = self.soup.body\n","\n","        # Process content blocks\n","        for block in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n","                                          'ul', 'ol', 'table']):\n","            block_type = block.name\n","            block_content = {}\n","\n","            if block_type in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n","                block_content = {\n","                    'type': 'heading',\n","                    'level': int(block_type[1]),\n","                    'text': block.get_text(strip=True),\n","                    'id': block.get('id', ''),\n","                }\n","\n","            elif block_type in ['ul', 'ol']:\n","                items = [li.get_text(strip=True) for li in block.find_all('li')]\n","                block_content = {\n","                    'type': 'list',\n","                    'list_type': block_type,\n","                    'items': items\n","                }\n","\n","            elif block_type == 'table':\n","                # Convert table to DataFrame and then to dict\n","                table_data = []\n","                headers = []\n","\n","                # Extract headers\n","                header_row = block.find('tr')\n","                if header_row:\n","                    headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]\n","\n","                # Extract rows\n","                for row in block.find_all('tr')[1:]:  # Skip header row\n","                    row_data = [td.get_text(strip=True) for td in row.find_all('td')]\n","                    if row_data:  # Skip empty rows\n","                        table_data.append(row_data)\n","\n","                if headers and table_data:\n","                    df = pd.DataFrame(table_data, columns=headers)\n","                    block_content = {\n","                        'type': 'table',\n","                        'headers': headers,\n","                        'data': df.to_dict('records')\n","                    }\n","\n","            else:  # Paragraphs and other text blocks\n","                block_content = {\n","                    'type': 'text',\n","                    'text': block.get_text(strip=True)\n","                }\n","\n","            if block_content:  # Only add non-empty blocks\n","                content_blocks.append(block_content)\n","\n","        self.content_blocks = content_blocks\n","        return content_blocks\n","\n","    def clean_content(self):\n","        \"\"\"\n","        Clean and normalize extracted content.\n","        Removes boilerplate, normalizes whitespace, and handles special characters.\n","        \"\"\"\n","        for block in self.content_blocks:\n","            if 'text' in block:\n","                # Normalize whitespace\n","                block['text'] = re.sub(r'\\s+', ' ', block['text']).strip()\n","\n","                # Remove common boilerplate phrases\n","                boilerplate = [\n","                    'Share this article',\n","                    'Follow us',\n","                    'Advertisement',\n","                    'Subscribe to our newsletter'\n","                ]\n","                for phrase in boilerplate:\n","                    block['text'] = block['text'].replace(phrase, '')\n","\n","            elif block['type'] == 'list':\n","                block['items'] = [\n","                    re.sub(r'\\s+', ' ', item).strip()\n","                    for item in block['items']\n","                ]\n","\n","    def process(self) -> Dict[str, Any]:\n","        \"\"\"\n","        Process the HTML document completely.\n","        Extracts metadata and content, then returns the structured result.\n","\n","        Returns:\n","            Dictionary containing processed content and metadata\n","        \"\"\"\n","        try:\n","            metadata = self.extract_metadata()\n","            content_blocks = self.extract_main_content()\n","            self.clean_content()\n","\n","            return {\n","                'metadata': metadata,\n","                'content': content_blocks,\n","                'stats': {\n","                    'total_blocks': len(content_blocks),\n","                    'processed_at': datetime.now().isoformat()\n","                }\n","            }\n","\n","        except Exception as e:\n","            logger.error(f\"Error processing HTML: {str(e)}\")\n","            raise\n","\n","\"\"\"## Testing Our Implementation\n","\n","Let's test our HTML processor with the static article sample we created in Part 1.\n","This will demonstrate how it handles different types of content and metadata.\n","\"\"\"\n","\n","def test_html_processor():\n","    \"\"\"Test the HTML processor with our sample file.\"\"\"\n","    try:\n","        # Read the sample file\n","        with open('rag_web_samples/static_article.html', 'r') as f:\n","            html_content = f.read()\n","\n","        # Process the content\n","        processor = HTMLProcessor(html_content)\n","        result = processor.process()\n","\n","        # Display results\n","        print(\"Extracted Metadata:\")\n","        print(\"-\" * 50)\n","        print(json.dumps(result['metadata'], indent=2))\n","\n","        print(\"\\nContent Blocks:\")\n","        print(\"-\" * 50)\n","        for block in result['content']:\n","            print(f\"\\nType: {block['type']}\")\n","            if block['type'] == 'heading':\n","                print(f\"Level: {block['level']}\")\n","                print(f\"Text: {block['text']}\")\n","            elif block['type'] == 'table':\n","                print(\"Table Headers:\", block['headers'])\n","                print(\"First Row:\", block['data'][0])\n","            else:\n","                print(f\"Content: {block.get('text', block.get('items', []))}\")\n","\n","        print(\"\\nProcessing Statistics:\")\n","        print(\"-\" * 50)\n","        print(json.dumps(result['stats'], indent=2))\n","\n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","\n","# Run the test\n","test_html_processor()\n","\n","\"\"\"## Important Concepts to Note\n","\n","Our HTML processor demonstrates several important concepts for RAG systems:\n","\n","1. Hierarchical Processing: We maintain the document's structure by preserving\n","   heading levels and content relationships.\n","\n","2. Content Classification: We distinguish between different types of content\n","   (headings, paragraphs, lists, tables) and process each appropriately.\n","\n","3. Rich Metadata Extraction: We handle multiple metadata formats and preserve\n","   structured data that might be valuable for retrieval.\n","\n","4. Clean and Normalized Output: Our processor produces consistent, clean output\n","   suitable for further processing in a RAG pipeline.\n","\n","## What's Next?\n","\n","In Part 3, we'll explore handling dynamic web content, including:\n","- JavaScript-rendered content\n","- Interactive elements\n","- Real-time updates\n","- Single Page Applications (SPAs)\n","\n","We'll build upon our static HTML processing capabilities while adding support\n","for dynamic content extraction.\"\"\""],"metadata":{"id":"zzEKPHZm6xIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Web Content Processing in RAG Systems - Part 3: Dynamic Web Content\n","\n","This notebook focuses on processing dynamic web content in RAG systems. While static\n","HTML processing gives us a foundation, modern web applications often rely heavily\n","on JavaScript to render content, handle user interactions, and manage state. We'll\n","explore how to handle these dynamic elements effectively.\n","\n","Make sure you've completed Parts 1 and 2 before starting this section, as we'll\n","build upon concepts and code established there.\n","\"\"\"\n","\n","# First, let's install necessary packages\n","!pip install playwright pandas beautifulsoup4 asyncio\n","!playwright install chromium\n","\n","import asyncio\n","from playwright.async_api import async_playwright\n","from bs4 import BeautifulSoup\n","import json\n","import logging\n","from typing import List, Dict, Any, Optional\n","from datetime import datetime\n","import time\n","from pathlib import Path\n","\n","# Set up logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","\"\"\"## Understanding Dynamic Web Content\n","\n","Modern web applications present several challenges beyond static HTML processing:\n","\n","1. JavaScript Rendering: Content is often generated and modified by JavaScript\n","   after the initial page load.\n","\n","2. Asynchronous Loading: Data might be fetched from APIs and rendered gradually\n","   rather than being available immediately.\n","\n","3. State Management: The page's content can change based on user interactions\n","   and application state.\n","\n","4. Single Page Applications (SPAs): The entire application might run in the\n","   browser, with content updates happening without full page reloads.\n","\n","Let's build a processor that can handle these challenges.\n","\"\"\"\n","\n","class DynamicContentProcessor:\n","    \"\"\"\n","    Processes dynamic web content with support for JavaScript rendering,\n","    asynchronous loading, and state changes.\n","    \"\"\"\n","\n","    def __init__(self, wait_time: int = 5000):\n","        \"\"\"\n","        Initialize the dynamic content processor.\n","\n","        Args:\n","            wait_time: Time to wait for dynamic content to load (milliseconds)\n","        \"\"\"\n","        self.wait_time = wait_time\n","        self.page = None\n","        self.context = None\n","        self.browser = None\n","\n","    async def _initialize_browser(self):\n","        \"\"\"\n","        Initialize the browser with appropriate settings for content extraction.\n","        Configures browser behavior to handle modern web applications.\n","        \"\"\"\n","        playwright = await async_playwright().start()\n","        self.browser = await playwright.chromium.launch(\n","            headless=True,  # Run without visible browser window\n","        )\n","\n","        # Create a context with specific settings\n","        self.context = await self.browser.new_context(\n","            viewport={'width': 1920, 'height': 1080},\n","            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n","        )\n","\n","        # Create a page with event handlers\n","        self.page = await self.context.new_page()\n","\n","        # Monitor network activity\n","        self.page.on('request', lambda req: logger.debug(f'Request: {req.url}'))\n","        self.page.on('response', lambda res: logger.debug(f'Response: {res.url}'))\n","\n","    async def _wait_for_dynamic_content(self):\n","        \"\"\"\n","        Wait for dynamic content to load and stabilize.\n","        Uses multiple strategies to ensure content is fully rendered.\n","        \"\"\"\n","        # Wait for initial network activity to settle\n","        await self.page.wait_for_load_state('networkidle')\n","\n","        # Wait for any animations to complete\n","        await self.page.wait_for_timeout(self.wait_time)\n","\n","        # Scroll to load lazy content\n","        await self.page.evaluate(\"\"\"\n","            window.scrollTo({\n","                top: document.body.scrollHeight,\n","                behavior: 'smooth'\n","            });\n","        \"\"\")\n","\n","        # Wait for any new content to load\n","        await self.page.wait_for_timeout(1000)\n","\n","    async def _extract_dynamic_state(self) -> Dict[str, Any]:\n","        \"\"\"\n","        Extract application state and dynamic data.\n","        Captures both visible content and internal application state.\n","\n","        Returns:\n","            Dictionary containing extracted state and data\n","        \"\"\"\n","        # Get any global state variables\n","        state = await self.page.evaluate(\"\"\"() => {\n","            const state = {};\n","\n","            // Common state variable names\n","            const stateVars = [\n","                'window.__INITIAL_STATE__',\n","                'window.__PRELOADED_STATE__',\n","                'window.__APOLLO_STATE__',\n","                'window.__NUXT__'\n","            ];\n","\n","            // Try to capture known state variables\n","            for (const varName of stateVars) {\n","                try {\n","                    const value = eval(varName);\n","                    if (value) {\n","                        state[varName] = value;\n","                    }\n","                } catch (e) {\n","                    // Ignore errors for missing variables\n","                }\n","            }\n","\n","            return state;\n","        }\"\"\")\n","\n","        return state\n","\n","    async def _extract_network_data(self) -> List[Dict[str, Any]]:\n","        \"\"\"\n","        Extract data from network requests.\n","        Captures API responses and dynamically loaded content.\n","\n","        Returns:\n","            List of captured network responses\n","        \"\"\"\n","        # Enable network interception\n","        await self.page.route('**/*', lambda route: route.continue_())\n","\n","        responses = []\n","\n","        def handle_response(response):\n","            try:\n","                if response.status == 200:\n","                    content_type = response.headers.get('content-type', '')\n","                    if 'application/json' in content_type:\n","                        responses.append({\n","                            'url': response.url,\n","                            'type': 'json',\n","                            'content': response.json()\n","                        })\n","            except Exception as e:\n","                logger.warning(f\"Error processing response: {str(e)}\")\n","\n","        self.page.on('response', handle_response)\n","\n","        return responses\n","\n","    async def process_url(self, url: str) -> Dict[str, Any]:\n","        \"\"\"\n","        Process a URL containing dynamic content.\n","        Handles page loading, content extraction, and cleanup.\n","\n","        Args:\n","            url: URL to process\n","\n","        Returns:\n","            Dictionary containing processed content and metadata\n","        \"\"\"\n","        try:\n","            # Initialize browser if needed\n","            if not self.browser:\n","                await self._initialize_browser()\n","\n","            logger.info(f\"Processing URL: {url}\")\n","\n","            # Navigate to the page\n","            await self.page.goto(url, wait_until='networkidle')\n","\n","            # Wait for dynamic content\n","            await self._wait_for_dynamic_content()\n","\n","            # Extract dynamic state\n","            state = await self._extract_dynamic_state()\n","\n","            # Get the rendered HTML\n","            content = await self.page.content()\n","\n","            # Parse with BeautifulSoup for content extraction\n","            soup = BeautifulSoup(content, 'html5lib')\n","\n","            # Extract network data\n","            network_data = await self._extract_network_data()\n","\n","            # Combine all extracted information\n","            result = {\n","                'url': url,\n","                'timestamp': datetime.now().isoformat(),\n","                'rendered_content': {\n","                    'title': soup.title.text if soup.title else None,\n","                    'body': soup.body.get_text(strip=True) if soup.body else None,\n","                },\n","                'dynamic_state': state,\n","                'network_data': network_data,\n","                'metadata': {\n","                    'processing_time': time.time(),\n","                    'renderer': 'playwright',\n","                    'wait_time': self.wait_time\n","                }\n","            }\n","\n","            return result\n","\n","        except Exception as e:\n","            logger.error(f\"Error processing URL: {str(e)}\")\n","            raise\n","\n","        finally:\n","            if self.browser:\n","                await self.browser.close()\n","\n","    @staticmethod\n","    def clean_extracted_content(content: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"\n","        Clean and normalize extracted content.\n","        Removes unnecessary content and normalizes data structures.\n","\n","        Args:\n","            content: Raw extracted content\n","\n","        Returns:\n","            Cleaned and normalized content\n","        \"\"\"\n","        # Function to recursively clean dictionary values\n","        def clean_value(value):\n","            if isinstance(value, str):\n","                # Remove excessive whitespace\n","                return ' '.join(value.split())\n","            elif isinstance(value, list):\n","                return [clean_value(v) for v in value]\n","            elif isinstance(value, dict):\n","                return {k: clean_value(v) for k, v in value.items()}\n","            return value\n","\n","        # Clean all content recursively\n","        cleaned = {k: clean_value(v) for k, v in content.items()}\n","\n","        return cleaned\n","\n","\"\"\"## Testing Dynamic Content Processing\n","\n","Let's test our dynamic content processor with the sample dynamic dashboard\n","we created in Part 1. This will demonstrate how it handles JavaScript-rendered\n","content and state changes.\n","\"\"\"\n","\n","async def test_dynamic_processor():\n","    \"\"\"Test the dynamic content processor with our sample file.\"\"\"\n","    try:\n","        # Create a local HTTP server to serve our test file\n","        import http.server\n","        import socketserver\n","        import threading\n","\n","        PORT = 8000\n","        Handler = http.server.SimpleHTTPRequestHandler\n","\n","        def run_server():\n","            with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n","                print(f\"Serving at port {PORT}\")\n","                httpd.serve_forever()\n","\n","        # Start server in a separate thread\n","        server_thread = threading.Thread(target=run_server)\n","        server_thread.daemon = True\n","        server_thread.start()\n","\n","        # Process the dynamic content\n","        processor = DynamicContentProcessor(wait_time=5000)\n","        result = await processor.process_url(\n","            f\"http://localhost:{PORT}/rag_web_samples/dynamic_dashboard.html\"\n","        )\n","\n","        # Clean the results\n","        cleaned_result = processor.clean_extracted_content(result)\n","\n","        # Display results\n","        print(\"Processed Dynamic Content:\")\n","        print(\"-\" * 50)\n","        print(json.dumps(cleaned_result, indent=2))\n","\n","    except Exception as e:\n","        print(f\"Error during testing: {str(e)}\")\n","\n","# Run the test using asyncio\n","await test_dynamic_processor()\n","\n","\"\"\"## Handling Different Types of Dynamic Content\n","\n","Our processor demonstrates several important capabilities for handling\n","dynamic web content:\n","\n","1. JavaScript Rendering:\n","   - Uses Playwright to fully render JavaScript content\n","   - Waits for dynamic updates to complete\n","   - Handles modern web frameworks and libraries\n","\n","2. State Management:\n","   - Captures application state variables\n","   - Monitors network requests for data updates\n","   - Tracks dynamic content changes\n","\n","3. Content Extraction:\n","   - Processes both initially loaded and dynamically added content\n","   - Handles lazy-loaded and infinite-scroll content\n","   - Preserves relationships between dynamic elements\n","\n","4. Error Handling:\n","   - Manages timeouts and loading failures\n","   - Handles partial content loads\n","   - Provides detailed error information\n","\n","## What's Next?\n","\n","In Part 4, we'll explore working with web APIs and JSON data, including:\n","- RESTful API interaction\n","- GraphQL queries\n","- Streaming data handling\n","- Rate limiting and error handling\n","\n","We'll build upon our dynamic content processing capabilities while adding\n","sophisticated API interaction features.\"\"\""],"metadata":{"id":"-qvMUltI8FVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Web Content Processing in RAG Systems - Part 4: Web APIs and JSON Processing\n","\n","This notebook demonstrates API integration in RAG systems with robust error handling\n","and port management. We'll use dynamic port allocation to avoid conflicts.\n","\"\"\"\n","\n","# Let's modify our test to use a dynamic port and better cleanup\n","async def test_api_integration():\n","    \"\"\"\n","    Test the API client with a sample scenario using dynamic port allocation.\n","    Includes proper resource cleanup and error handling.\n","    \"\"\"\n","    from aiohttp import web\n","    import socket\n","\n","    def find_free_port():\n","        \"\"\"Find a free port on localhost by letting the OS assign one.\"\"\"\n","        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n","            s.bind(('', 0))\n","            s.listen(1)\n","            port = s.getsockname()[1]\n","            return port\n","\n","    # Sample data\n","    products = [\n","        {\"id\": 1, \"name\": \"Product A\", \"price\": 99.99},\n","        {\"id\": 2, \"name\": \"Product B\", \"price\": 149.99}\n","    ]\n","\n","    async def get_products(request):\n","        \"\"\"Mock endpoint that returns products.\"\"\"\n","        return web.json_response({\"data\": products})\n","\n","    port = find_free_port()\n","    logger.info(f\"Starting test server on port {port}\")\n","\n","    # Create the API server\n","    app = web.Application()\n","    app.router.add_get('/api/products', get_products)\n","\n","    runner = web.AppRunner(app)\n","    site = None\n","    client = None\n","\n","    try:\n","        # Set up the server\n","        await runner.setup()\n","        site = web.TCPSite(runner, 'localhost', port)\n","        await site.start()\n","\n","        # Initialize our API client\n","        base_url = f'http://localhost:{port}/api'\n","        client = APIClient(base_url)\n","\n","        # Document the endpoint\n","        client.document_endpoint(\n","            'products',\n","            description='Get all products',\n","            response_schema={\n","                'type': 'object',\n","                'properties': {\n","                    'data': {\n","                        'type': 'array',\n","                        'items': {\n","                            'type': 'object',\n","                            'properties': {\n","                                'id': {'type': 'integer'},\n","                                'name': {'type': 'string'},\n","                                'price': {'type': 'number'}\n","                            }\n","                        }\n","                    }\n","                }\n","            }\n","        )\n","\n","        # Create a data processor\n","        processor = DataProcessor()\n","\n","        def process_products(data: Dict) -> List[Dict]:\n","            \"\"\"Process product data for RAG system.\"\"\"\n","            products = data.get('data', [])\n","            return [{\n","                'product_id': p['id'],\n","                'name': p['name'],\n","                'price_usd': p['price'],\n","                'price_formatted': f\"${p['price']:.2f}\"\n","            } for p in products]\n","\n","        processor.register_processor('products', process_products)\n","\n","        # Make API requests\n","        logger.info(\"Making first request (no cache)...\")\n","        response1 = await client.request('GET', 'products')\n","        processed1 = processor.process_response(response1, 'products')\n","\n","        logger.info(\"Making second request (should use cache)...\")\n","        response2 = await client.request('GET', 'products')\n","        processed2 = processor.process_response(response2, 'products')\n","\n","        # Display results\n","        print(\"\\nFirst Request Results:\")\n","        print(\"-\" * 50)\n","        print(json.dumps(processed1, indent=2))\n","\n","        print(\"\\nCache Statistics:\")\n","        print(\"-\" * 50)\n","        print(f\"Cache size: {len(client.cache)}\")\n","        print(f\"Cache items: {list(client.cache.keys())}\")\n","\n","    except Exception as e:\n","        logger.error(f\"Test failed: {str(e)}\")\n","        raise\n","\n","    finally:\n","        logger.info(\"Cleaning up resources...\")\n","        if client:\n","            await client.close()\n","        if runner:\n","            await runner.cleanup()\n","\n","# Run the test with proper async handling\n","try:\n","    await test_api_integration()\n","except Exception as e:\n","    print(f\"Test failed with error: {str(e)}\")\n","    print(\"Please ensure no other tests are running and try again.\")\n","\"\"\"\n","\n","This fixed version includes:\n","1. Dynamic port allocation to avoid conflicts\n","2. Proper resource cleanup in the finally block\n","3. Better error handling and logging\n","4. Clear separation of setup, test, and cleanup phases\n","\n","To use this example:\n","1. Make sure you have the necessary imports from earlier in the notebook\n","2. Run this test after defining the APIClient and DataProcessor classes\n","\"\"\""],"metadata":{"id":"BQUOmAFKOpD6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","# Web Content Processing in RAG Systems - Part 5: Performance Monitoring Foundation\n","\n","This notebook begins our exploration of performance optimization and testing for web\n","content processing in RAG systems. We'll establish a robust foundation for\n","monitoring and measuring performance, which will guide our optimization efforts.\n","\n","Our performance monitoring system needs to track multiple aspects of processing:\n","- Execution time for different operations\n","- Memory usage patterns\n","- Resource utilization\n","- Error rates and types\n","- Processing throughput\n","\"\"\"\n","\n","# Install required packages for monitoring and profiling\n","!pip install memory_profiler psutil pytest-benchmark\n","\n","import time\n","import psutil\n","import logging\n","from typing import Dict, List, Any, Optional\n","from datetime import datetime\n","import json\n","from dataclasses import dataclass, asdict\n","import statistics\n","from contextlib import contextmanager\n","import gc\n","\n","# Set up logging with detailed formatting\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","logger = logging.getLogger(__name__)\n","\n","@dataclass\n","class OperationMetrics:\n","    \"\"\"\n","    Tracks detailed metrics for a single operation.\n","    This helps us understand the performance characteristics of each\n","    processing step.\n","    \"\"\"\n","    operation_name: str\n","    start_time: datetime\n","    end_time: Optional[datetime] = None\n","    memory_start: float = 0.0\n","    memory_peak: float = 0.0\n","    memory_end: float = 0.0\n","    cpu_percent: float = 0.0\n","    items_processed: int = 0\n","    bytes_processed: int = 0\n","    error_count: int = 0\n","\n","    @property\n","    def duration_seconds(self) -> float:\n","        \"\"\"Calculate operation duration in seconds.\"\"\"\n","        if not self.end_time:\n","            return 0\n","        return (self.end_time - self.start_time).total_seconds()\n","\n","    @property\n","    def processing_rate(self) -> float:\n","        \"\"\"Calculate items processed per second.\"\"\"\n","        if self.duration_seconds == 0:\n","            return 0\n","        return self.items_processed / self.duration_seconds\n","\n","    @property\n","    def memory_delta(self) -> float:\n","        \"\"\"Calculate memory usage change during operation.\"\"\"\n","        return self.memory_end - self.memory_start\n","\n","    def to_dict(self) -> Dict:\n","        \"\"\"Convert metrics to a dictionary for analysis and storage.\"\"\"\n","        base_dict = asdict(self)\n","        base_dict.update({\n","            'duration_seconds': self.duration_seconds,\n","            'processing_rate': self.processing_rate,\n","            'memory_delta': self.memory_delta\n","        })\n","        return base_dict\n","\n","class PerformanceMonitor:\n","    \"\"\"\n","    Comprehensive performance monitoring system for web content processing.\n","    Tracks detailed metrics across multiple operations and provides analysis tools.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the performance monitor.\"\"\"\n","        self.metrics_history: List[OperationMetrics] = []\n","        self.active_operations: Dict[str, OperationMetrics] = {}\n","        self.process = psutil.Process()\n","\n","    def _get_memory_usage(self) -> float:\n","        \"\"\"Get current memory usage in megabytes.\"\"\"\n","        return self.process.memory_info().rss / 1024 / 1024\n","\n","    @contextmanager\n","    def monitor_operation(self, operation_name: str, items_count: int = 0, bytes_count: int = 0):\n","        \"\"\"\n","        Context manager for monitoring an operation's performance.\n","\n","        Args:\n","            operation_name: Name of the operation to monitor\n","            items_count: Number of items being processed\n","            bytes_count: Size of data being processed in bytes\n","        \"\"\"\n","        # Force garbage collection before measuring\n","        gc.collect()\n","\n","        # Initialize metrics\n","        metrics = OperationMetrics(\n","            operation_name=operation_name,\n","            start_time=datetime.now(),\n","            memory_start=self._get_memory_usage(),\n","            items_processed=items_count,\n","            bytes_processed=bytes_count\n","        )\n","\n","        self.active_operations[operation_name] = metrics\n","\n","        try:\n","            yield metrics\n","\n","        finally:\n","            # Record final measurements\n","            metrics.end_time = datetime.now()\n","            metrics.memory_end = self._get_memory_usage()\n","            metrics.cpu_percent = self.process.cpu_percent()\n","\n","            # Store metrics\n","            self.metrics_history.append(metrics)\n","            del self.active_operations[operation_name]\n","\n","    def get_operation_statistics(self, operation_name: str) -> Dict[str, Any]:\n","        \"\"\"\n","        Calculate detailed statistics for an operation type.\n","\n","        Args:\n","            operation_name: Name of the operation to analyze\n","\n","        Returns:\n","            Dictionary containing comprehensive performance statistics\n","        \"\"\"\n","        relevant_metrics = [\n","            m for m in self.metrics_history\n","            if m.operation_name == operation_name\n","        ]\n","\n","        if not relevant_metrics:\n","            return {}\n","\n","        durations = [m.duration_seconds for m in relevant_metrics]\n","        memory_deltas = [m.memory_delta for m in relevant_metrics]\n","        processing_rates = [m.processing_rate for m in relevant_metrics]\n","\n","        return {\n","            'samples_count': len(relevant_metrics),\n","            'duration': {\n","                'mean': statistics.mean(durations),\n","                'median': statistics.median(durations),\n","                'min': min(durations),\n","                'max': max(durations),\n","                'std_dev': statistics.stdev(durations) if len(durations) > 1 else 0\n","            },\n","            'memory': {\n","                'mean_delta': statistics.mean(memory_deltas),\n","                'max_delta': max(memory_deltas),\n","                'peak_usage': max(m.memory_peak for m in relevant_metrics)\n","            },\n","            'processing_rate': {\n","                'mean': statistics.mean(processing_rates),\n","                'median': statistics.median(processing_rates)\n","            },\n","            'error_rate': sum(m.error_count for m in relevant_metrics) / len(relevant_metrics),\n","            'total_processed': {\n","                'items': sum(m.items_processed for m in relevant_metrics),\n","                'bytes': sum(m.bytes_processed for m in relevant_metrics)\n","            }\n","        }\n","\n","    def generate_report(self, operation_name: Optional[str] = None) -> Dict[str, Any]:\n","        \"\"\"\n","        Generate a comprehensive performance report.\n","\n","        Args:\n","            operation_name: Optional name to filter specific operations\n","\n","        Returns:\n","            Dictionary containing detailed performance analysis\n","        \"\"\"\n","        operations = (\n","            [operation_name] if operation_name\n","            else set(m.operation_name for m in self.metrics_history)\n","        )\n","\n","        report = {\n","            'timestamp': datetime.now().isoformat(),\n","            'total_operations': len(self.metrics_history),\n","            'operations': {}\n","        }\n","\n","        for op in operations:\n","            report['operations'][op] = self.get_operation_statistics(op)\n","\n","        return report\n","\n","\"\"\"## Testing Our Performance Monitoring\n","\n","Let's create some example operations to test our monitoring system and\n","demonstrate how it tracks different performance aspects.\n","\"\"\"\n","\n","def test_performance_monitor():\n","    \"\"\"Test the performance monitoring system with sample operations.\"\"\"\n","    monitor = PerformanceMonitor()\n","\n","    # Test 1: CPU-intensive operation\n","    def cpu_intensive_task():\n","        result = 0\n","        for i in range(1000000):\n","            result += i * i\n","        return result\n","\n","    with monitor.monitor_operation('cpu_task', items_count=1000000):\n","        result = cpu_intensive_task()\n","\n","    # Test 2: Memory-intensive operation\n","    def memory_intensive_task():\n","        large_list = list(range(1000000))\n","        return sum(large_list)\n","\n","    with monitor.monitor_operation('memory_task', items_count=1000000):\n","        result = memory_intensive_task()\n","\n","    # Generate and display report\n","    report = monitor.generate_report()\n","    print(\"\\nPerformance Report:\")\n","    print(\"-\" * 50)\n","    print(json.dumps(report, indent=2))\n","\n","# Run the test\n","test_performance_monitor()\n","\n","\"\"\"## Key Concepts in Performance Monitoring\n","\n","Our performance monitoring system demonstrates several important concepts:\n","\n","1. Comprehensive Metrics Collection:\n","   - Execution time measurement\n","   - Memory usage tracking\n","   - CPU utilization monitoring\n","   - Processing rate calculation\n","\n","2. Statistical Analysis:\n","   - Mean and median calculations\n","   - Standard deviation for variance analysis\n","   - Peak value tracking\n","   - Error rate monitoring\n","\n","3. Resource Management:\n","   - Proper cleanup with context managers\n","   - Garbage collection integration\n","   - Memory leak detection capabilities\n","\n","4. Flexible Reporting:\n","   - Detailed per-operation statistics\n","   - Aggregated performance metrics\n","   - Customizable report generation\n","\n","\"\"\""],"metadata":{"id":"MeSneMv5PcLw"},"execution_count":null,"outputs":[]}]}