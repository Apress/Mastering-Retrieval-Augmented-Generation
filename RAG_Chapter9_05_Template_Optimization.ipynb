{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhkKVR9723KOQaW/C3oqRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"QcpLdIomkti-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr1vyld9j5ZX"},"outputs":[],"source":["!pip install langchain langchain-openai tiktoken numpy scipy matplotlib pandas\n","\n","import os\n","import re\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","from typing import List, Dict, Any, Optional, Tuple, Callable\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","import tiktoken\n","from langchain.prompts import PromptTemplate\n","from langchain_core.documents import Document\n","from langchain_openai import OpenAI, ChatOpenAI"]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"_MgLtmaTkth0"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Create sample documents for testing\n","sample_docs = [\n","    Document(page_content=\"The Eiffel Tower is 330 meters (1,083 ft) tall and was the tallest man-made structure in the world from 1889 to 1930.\",\n","             metadata={\"source\": \"travel_guide\", \"page\": 25}),\n","    Document(page_content=\"The Eiffel Tower was built by Gustave Eiffel for the 1889 World's Fair in Paris.\",\n","             metadata={\"source\": \"history_book\", \"page\": 42}),\n","    Document(page_content=\"The Eiffel Tower is made of wrought iron and weighs approximately 10,100 tonnes.\",\n","             metadata={\"source\": \"engineering_text\", \"page\": 89}),\n","    Document(page_content=\"Tokyo Tower, completed in 1958, was inspired by the Eiffel Tower but is slightly taller at 333 meters.\",\n","             metadata={\"source\": \"global_landmarks\", \"page\": 118}),\n","]\n","\n","# Create evaluation set with questions and ground truth answers\n","eval_set = [\n","    {\n","        \"question\": \"How tall is the Eiffel Tower?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The Eiffel Tower is 330 meters (1,083 ft) tall.\",\n","        \"expected_source\": 0  # Index of the document containing the answer\n","    },\n","    {\n","        \"question\": \"Who built the Eiffel Tower?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The Eiffel Tower was built by Gustave Eiffel.\",\n","        \"expected_source\": 1\n","    },\n","    {\n","        \"question\": \"What material is the Eiffel Tower made of?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The Eiffel Tower is made of wrought iron.\",\n","        \"expected_source\": 2\n","    },\n","    {\n","        \"question\": \"When did the Eiffel Tower lose its status as the world's tallest structure?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The Eiffel Tower lost its status as the world's tallest structure in 1930.\",\n","        \"expected_source\": 0\n","    },\n","    {\n","        \"question\": \"How does the Eiffel Tower compare to Tokyo Tower in height?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The Eiffel Tower (330 meters) is slightly shorter than Tokyo Tower (333 meters).\",\n","        \"expected_source\": 3\n","    },\n","]\n"],"metadata":{"id":"I0dOt150lTr0","executionInfo":{"status":"ok","timestamp":1741266715094,"user_tz":-330,"elapsed":10,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: Evaluation Metrics for Prompt Template Performance**"],"metadata":{"id":"3sGGz0-QlYba"}},{"cell_type":"code","source":["print(\"Section 1: Evaluation Metrics for Prompt Template Performance\")\n","\n","def extract_facts(text: str) -> List[str]:\n","    \"\"\"Extract factual statements from text (simplified approach).\"\"\"\n","    # This is a simple sentence-based extraction\n","    # In practice, fact extraction is more complex\n","    sentences = re.split(r'(?<=[.!?])\\s+', text)\n","    facts = [s.strip() for s in sentences if len(s.strip()) > 10 and not s.strip().endswith('?')]\n","    return facts\n","\n","def evaluate_factual_accuracy(responses: List[str], ground_truths: List[str]) -> float:\n","    \"\"\"Evaluate factual accuracy of responses against ground truth (simplified).\"\"\"\n","    if not responses or not ground_truths:\n","        return 0.0\n","\n","    scores = []\n","    for response, truth in zip(responses, ground_truths):\n","        # Check if the key information from the ground truth appears in the response\n","        # This is a simplified approach - real evaluation is more nuanced\n","        key_facts = extract_facts(truth)\n","        matches = 0\n","\n","        for fact in key_facts:\n","            # Extract the core information (simplistic approach)\n","            core_info = ' '.join([w for w in fact.split() if len(w) > 3]).lower()\n","            if core_info in response.lower():\n","                matches += 1\n","\n","        accuracy = matches / len(key_facts) if key_facts else 0.0\n","        scores.append(accuracy)\n","\n","    return sum(scores) / len(scores)\n","\n","def evaluate_citation_quality(responses: List[str], expected_sources: List[int]) -> Dict[str, float]:\n","    \"\"\"Evaluate the quality of source citations in responses.\"\"\"\n","    if not responses or not expected_sources:\n","        return {\"citation_rate\": 0.0, \"citation_accuracy\": 0.0}\n","\n","    citation_counts = 0\n","    correct_citations = 0\n","\n","    for response, expected_source in zip(responses, expected_sources):\n","        # Check if response contains citations\n","        citations = re.findall(r'\\[(?:Document|Source|Doc) (\\d+)\\]', response)\n","        citation_counts += 1 if citations else 0\n","\n","        # Check if the expected source is cited\n","        expected_doc_num = expected_source + 1  # Convert 0-based index to 1-based doc number\n","        if str(expected_doc_num) in citations:\n","            correct_citations += 1\n","\n","    # Calculate metrics\n","    citation_rate = citation_counts / len(responses)\n","    citation_accuracy = correct_citations / len(responses)\n","\n","    return {\n","        \"citation_rate\": citation_rate,\n","        \"citation_accuracy\": citation_accuracy\n","    }\n","\n","def estimate_hallucination_rate(responses: List[str], contexts: List[str]) -> float:\n","    \"\"\"Estimate the rate of hallucinated content (simplified approach).\"\"\"\n","    if not responses or not contexts:\n","        return 0.0\n","\n","    hallucination_scores = []\n","\n","    for response, context in zip(responses, contexts):\n","        # Extract statements from response\n","        statements = extract_facts(response)\n","\n","        if not statements:\n","            hallucination_scores.append(0.0)\n","            continue\n","\n","        # Count potentially hallucinated statements\n","        hallucinated_count = 0\n","\n","        for statement in statements:\n","            # Simple approach: check if key terms from statement appear in context\n","            # This is highly simplified - real hallucination detection is more complex\n","            key_terms = [word for word in statement.lower().split()\n","                         if len(word) > 4 and word not in [\"about\", \"these\", \"there\", \"their\", \"which\", \"would\"]]\n","\n","            # If key terms are found in the context, it's less likely to be a hallucination\n","            term_found_count = sum(1 for term in key_terms if term in context.lower())\n","            if key_terms and term_found_count / len(key_terms) < 0.5:\n","                hallucinated_count += 1\n","\n","        hallucination_score = hallucinated_count / len(statements)\n","        hallucination_scores.append(hallucination_score)\n","\n","    return sum(hallucination_scores) / len(hallucination_scores)\n","\n","def assess_template(template: str, eval_set: List[Dict], llm=None) -> Dict[str, Any]:\n","    \"\"\"\n","    Assess a template against an evaluation set.\n","\n","    If LLM is not provided, will only calculate token usage.\n","    \"\"\"\n","    token_counts = []\n","    responses = []\n","\n","    for item in eval_set:\n","        # Format prompt\n","        formatted_prompt = template.format(context=item[\"context\"], question=item[\"question\"])\n","        token_counts.append(count_tokens(formatted_prompt))\n","\n","        # Generate response if LLM is provided\n","        if llm:\n","            try:\n","                response = llm.invoke(formatted_prompt).content\n","                responses.append(response)\n","            except Exception as e:\n","                print(f\"Error generating response: {e}\")\n","                responses.append(\"\")\n","\n","    # Basic stats about token usage\n","    token_stats = {\n","        \"avg_tokens\": sum(token_counts) / len(token_counts),\n","        \"max_tokens\": max(token_counts),\n","        \"min_tokens\": min(token_counts)\n","    }\n","\n","    # Return basic results if no LLM is provided\n","    if not llm or not responses:\n","        return {\n","            \"token_stats\": token_stats,\n","            \"responses\": None,\n","            \"metrics\": None\n","        }\n","\n","    # Calculate evaluation metrics\n","    ground_truths = [item[\"ground_truth\"] for item in eval_set]\n","    expected_sources = [item[\"expected_source\"] for item in eval_set]\n","    contexts = [item[\"context\"] for item in eval_set]\n","\n","    factual_accuracy = evaluate_factual_accuracy(responses, ground_truths)\n","    citation_metrics = evaluate_citation_quality(responses, expected_sources)\n","    hallucination_rate = estimate_hallucination_rate(responses, contexts)\n","\n","    metrics = {\n","        \"factual_accuracy\": factual_accuracy,\n","        \"citation_rate\": citation_metrics[\"citation_rate\"],\n","        \"citation_accuracy\": citation_metrics[\"citation_accuracy\"],\n","        \"hallucination_rate\": hallucination_rate,\n","        # Calculate a composite score (weighted sum of metrics)\n","        \"composite_score\": (\n","            factual_accuracy * 0.4 +\n","            citation_metrics[\"citation_accuracy\"] * 0.3 +\n","            (1 - hallucination_rate) * 0.3\n","        )\n","    }\n","\n","    return {\n","        \"token_stats\": token_stats,\n","        \"responses\": responses,\n","        \"metrics\": metrics\n","    }\n","\n","# Define templates to evaluate\n","templates = {\n","    \"basic\": \"\"\"\n","    Answer the question based on the context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"anti_hallucination\": \"\"\"\n","    Answer the question based ONLY on the context below.\n","    If the context doesn't contain the answer, say \"I don't have enough information.\"\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"citation\": \"\"\"\n","    Answer the question based on the context below.\n","    Cite your sources using [Document X] notation.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"comprehensive\": \"\"\"\n","    You are an assistant for question-answering tasks.\n","    Answer based EXCLUSIVELY on the provided context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    INSTRUCTIONS:\n","    1. If the answer is in the context, provide it clearly and concisely\n","    2. Cite specific documents using [Document X] notation\n","    3. If the answer isn't in the context, say \"I don't have enough information\"\n","    4. Only use information present in the context\n","\n","    ANSWER:\n","    \"\"\"\n","}\n","\n","# Evaluate templates (token usage only without API key)\n","print(\"Evaluating template token usage:\")\n","template_results = {}\n","\n","for name, template in templates.items():\n","    result = assess_template(template, eval_set)\n","    template_results[name] = result\n","    print(f\"{name}: Avg tokens = {result['token_stats']['avg_tokens']:.1f}, Max tokens = {result['token_stats']['max_tokens']}\")\n","\n","# Evaluate with LLM if API key is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        print(\"\\nEvaluating templates with LLM:\")\n","        llm = ChatOpenAI(temperature=0)\n","\n","        for name, template in templates.items():\n","            print(f\"Evaluating {name} template...\")\n","            result = assess_template(template, eval_set[:2], llm)  # Use just 2 examples to save API calls\n","            template_results[name] = result\n","\n","            if result[\"metrics\"]:\n","                print(f\"  Factual accuracy: {result['metrics']['factual_accuracy']:.2f}\")\n","                print(f\"  Citation rate: {result['metrics']['citation_rate']:.2f}\")\n","                print(f\"  Hallucination rate: {result['metrics']['hallucination_rate']:.2f}\")\n","                print(f\"  Composite score: {result['metrics']['composite_score']:.2f}\")\n","    except Exception as e:\n","        print(f\"Error evaluating with LLM: {e}\")\n","else:\n","    print(\"\\nOpenAI API key not set - skipping LLM evaluation\")\n","\n","print_separator()\n"],"metadata":{"id":"zhPXbdxUlYlI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: A/B Testing Different Template Structures**"],"metadata":{"id":"X1x_7IKklkwT"}},{"cell_type":"code","source":["print(\"Section 2: A/B Testing Different Template Structures\")\n","\n","def run_ab_test(template_a: str, template_b: str, eval_set: List[Dict], llm=None):\n","    \"\"\"Run an A/B test comparing two templates.\"\"\"\n","    # Evaluate both templates\n","    results_a = assess_template(template_a, eval_set, llm)\n","    results_b = assess_template(template_b, eval_set, llm)\n","\n","    # Return results for comparison\n","    return {\n","        \"template_a\": {\n","            \"template\": template_a,\n","            \"token_stats\": results_a[\"token_stats\"],\n","            \"metrics\": results_a[\"metrics\"],\n","            \"responses\": results_a[\"responses\"]\n","        },\n","        \"template_b\": {\n","            \"template\": template_b,\n","            \"token_stats\": results_b[\"token_stats\"],\n","            \"metrics\": results_b[\"metrics\"],\n","            \"responses\": results_b[\"responses\"]\n","        }\n","    }\n","\n","def present_ab_test_results(results: Dict):\n","    \"\"\"Present the results of an A/B test.\"\"\"\n","    # Token stats comparison\n","    print(\"Token Usage Comparison:\")\n","    print(f\"Template A: Avg = {results['template_a']['token_stats']['avg_tokens']:.1f}, Max = {results['template_a']['token_stats']['max_tokens']}\")\n","    print(f\"Template B: Avg = {results['template_b']['token_stats']['avg_tokens']:.1f}, Max = {results['template_b']['token_stats']['max_tokens']}\")\n","\n","    # Performance metrics comparison (if available)\n","    if results['template_a']['metrics'] and results['template_b']['metrics']:\n","        print(\"\\nPerformance Metrics Comparison:\")\n","        metrics = ['factual_accuracy', 'citation_rate', 'citation_accuracy', 'hallucination_rate', 'composite_score']\n","\n","        for metric in metrics:\n","            value_a = results['template_a']['metrics'][metric]\n","            value_b = results['template_b']['metrics'][metric]\n","            diff = value_b - value_a\n","            better = \"B\" if diff > 0 else \"A\" if diff < 0 else \"Neither\"\n","\n","            # For hallucination rate, lower is better\n","            if metric == 'hallucination_rate':\n","                better = \"B\" if diff < 0 else \"A\" if diff > 0 else \"Neither\"\n","\n","            print(f\"{metric}: A = {value_a:.2f}, B = {value_b:.2f}, Diff = {diff:.2f}, Better: {better}\")\n","\n","def check_statistical_significance(metrics_a: List[float], metrics_b: List[float], confidence_level=0.95):\n","    \"\"\"Check if the difference between metrics is statistically significant.\"\"\"\n","    try:\n","        import scipy.stats as stats\n","\n","        t_stat, p_value = stats.ttest_ind(metrics_a, metrics_b)\n","\n","        return {\n","            \"t_statistic\": t_stat,\n","            \"p_value\": p_value,\n","            \"significant\": p_value < (1 - confidence_level)\n","        }\n","    except Exception as e:\n","        print(f\"Error in statistical significance testing: {e}\")\n","        return {\"significant\": False}\n","\n","# Define template variants for testing\n","control_template = \"\"\"\n","Answer the question based on the context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","# Variant with just one change - adding citation instruction\n","variant_template = \"\"\"\n","Answer the question based on the context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Cite your sources using [Document X] notation.\n","\n","ANSWER:\n","\"\"\"\n","\n","# Run A/B test\n","print(\"Running A/B test between two template variants:\")\n","ab_results = run_ab_test(control_template, variant_template, eval_set)\n","present_ab_test_results(ab_results)\n","\n","# Controlled Variable Testing\n","print(\"\\nControlled Variable Testing:\")\n","print(\"We can isolate the impact of specific template elements by testing variants\")\n","print(\"that differ in only one specific aspect.\")\n","print(\"\\nExample variant pairs:\")\n","print(\"1. Basic vs. Citation instruction\")\n","print(\"2. No constraint vs. Anti-hallucination constraint\")\n","print(\"3. Single task vs. Step-by-step instructions\")\n","\n","# Demonstrate significance testing (with mock data)\n","print(\"\\nStatistical Significance Testing Example:\")\n","# Create mock per-question scores for demonstration\n","mock_scores_a = [0.7, 0.8, 0.75, 0.65, 0.72]\n","mock_scores_b = [0.82, 0.88, 0.85, 0.8, 0.84]\n","\n","sig_result = check_statistical_significance(mock_scores_a, mock_scores_b)\n","print(f\"t-statistic: {sig_result.get('t_statistic', 'N/A')}\")\n","print(f\"p-value: {sig_result.get('p_value', 'N/A')}\")\n","print(f\"Statistically significant: {sig_result.get('significant', False)}\")\n","\n","print_separator()"],"metadata":{"id":"QJlES_iDllTu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Iterative Refinement Based on Output Quality**"],"metadata":{"id":"spmqqMyQlusw"}},{"cell_type":"code","source":["print(\"Section 3: Iterative Refinement Based on Output Quality\")\n","\n","def analyze_errors(responses: List[str], contexts: List[str], questions: List[str]) -> Dict[str, int]:\n","    \"\"\"Analyze common error patterns in responses.\"\"\"\n","    error_types = {\n","        \"hallucination\": 0,\n","        \"missing_information\": 0,\n","        \"incorrect_citation\": 0,\n","        \"irrelevant_content\": 0,\n","        \"contradiction\": 0\n","    }\n","\n","    for response, context, question in zip(responses, contexts, questions):\n","        # Simplified error detection logic\n","\n","        # Check for potential hallucinations (text not in context)\n","        key_statements = extract_facts(response)\n","        for statement in key_statements:\n","            # Simplistic check - in practice would be more sophisticated\n","            key_terms = [word for word in statement.lower().split() if len(word) > 5]\n","            if key_terms and all(term not in context.lower() for term in key_terms):\n","                error_types[\"hallucination\"] += 1\n","                break\n","\n","        # Check for missing information\n","        if \"I don't have enough information\" in response and question.lower() in context.lower():\n","            error_types[\"missing_information\"] += 1\n","\n","        # Check for incorrect citations\n","        citations = re.findall(r'\\[(?:Document|Source|Doc) (\\d+)\\]', response)\n","        if citations and not all(f\"Document {c}\" in context for c in citations):\n","            error_types[\"incorrect_citation\"] += 1\n","\n","        # Very simplistic relevance check\n","        question_terms = [w.lower() for w in question.split() if len(w) > 3]\n","        if not any(term in response.lower() for term in question_terms):\n","            error_types[\"irrelevant_content\"] += 1\n","\n","    return error_types\n","\n","class TemplateVersion:\n","    def __init__(self, template, name, description, changes=None):\n","        self.template = template\n","        self.name = name\n","        self.description = description\n","        self.changes = changes or []\n","        self.metrics = {}\n","        self.timestamp = datetime.now()\n","\n","    def add_metric(self, name, value):\n","        self.metrics[name] = value\n","\n","    def summary(self):\n","        return {\n","            \"name\": self.name,\n","            \"description\": self.description,\n","            \"changes\": self.changes,\n","            \"metrics\": self.metrics,\n","            \"timestamp\": self.timestamp.isoformat()\n","        }\n","\n","# Create template versions for demonstration\n","template_history = []\n","\n","v1 = TemplateVersion(\n","    template=templates[\"basic\"],\n","    name=\"v1_basic\",\n","    description=\"Basic QA template\"\n",")\n","v1.add_metric(\"factual_accuracy\", 0.72)\n","v1.add_metric(\"hallucination_rate\", 0.18)\n","v1.add_metric(\"composite_score\", 0.65)\n","template_history.append(v1)\n","\n","v2 = TemplateVersion(\n","    template=templates[\"anti_hallucination\"],\n","    name=\"v2_anti_hallucination\",\n","    description=\"Added anti-hallucination instruction\",\n","    changes=[\"Added explicit instruction not to use external knowledge\"]\n",")\n","v2.add_metric(\"factual_accuracy\", 0.70)\n","v2.add_metric(\"hallucination_rate\", 0.08)\n","v2.add_metric(\"composite_score\", 0.71)\n","template_history.append(v2)\n","\n","v3 = TemplateVersion(\n","    template=templates[\"citation\"],\n","    name=\"v3_citation\",\n","    description=\"Added citation requirement\",\n","    changes=[\"Added instruction to cite sources using Document notation\"]\n",")\n","v3.add_metric(\"factual_accuracy\", 0.75)\n","v3.add_metric(\"hallucination_rate\", 0.12)\n","v3.add_metric(\"citation_rate\", 0.85)\n","v3.add_metric(\"composite_score\", 0.76)\n","template_history.append(v3)\n","\n","v4 = TemplateVersion(\n","    template=templates[\"comprehensive\"],\n","    name=\"v4_comprehensive\",\n","    description=\"Comprehensive template with multiple improvements\",\n","    changes=[\n","        \"Added system role context\",\n","        \"Added structured instructions\",\n","        \"Combined anti-hallucination and citation requirements\",\n","        \"Added explicit steps for answering\"\n","    ]\n",")\n","v4.add_metric(\"factual_accuracy\", 0.82)\n","v4.add_metric(\"hallucination_rate\", 0.05)\n","v4.add_metric(\"citation_rate\", 0.92)\n","v4.add_metric(\"composite_score\", 0.85)\n","template_history.append(v4)\n","\n","# Visualize the template improvement history\n","print(\"Template Improvement History:\")\n","for i, version in enumerate(template_history):\n","    print(f\"{version.name}: {version.description}\")\n","    print(f\"  Changes: {', '.join(version.changes) if version.changes else 'None'}\")\n","    print(f\"  Metrics: {', '.join([f'{k}={v:.2f}' for k, v in version.metrics.items()])}\")\n","    print()\n","\n","# Plot improvement over versions\n","try:\n","    plt.figure(figsize=(10, 6))\n","\n","    # Extract version names and metrics\n","    names = [v.name for v in template_history]\n","    factual_scores = [v.metrics.get('factual_accuracy', 0) for v in template_history]\n","    hallucination_rates = [v.metrics.get('hallucination_rate', 0) for v in template_history]\n","    composite_scores = [v.metrics.get('composite_score', 0) for v in template_history]\n","\n","    # Create plot\n","    plt.plot(names, factual_scores, 'o-', label='Factual Accuracy')\n","    plt.plot(names, [1-rate for rate in hallucination_rates], 'o-', label='Anti-Hallucination')\n","    plt.plot(names, composite_scores, 'o-', label='Composite Score')\n","\n","    plt.title('Template Performance Improvement Over Versions')\n","    plt.xlabel('Template Version')\n","    plt.ylabel('Score (higher is better)')\n","    plt.ylim(0, 1)\n","    plt.grid(True, linestyle='--', alpha=0.7)\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","except Exception as e:\n","    print(f\"Error creating plot: {e}\")\n","    print(\"To see the visualization, run this notebook in an environment that supports matplotlib.\")\n","\n","print_separator()"],"metadata":{"id":"B0ZAip5Dlu1O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Automated Prompt Optimization Techniques**"],"metadata":{"id":"ui_Z8tSDl5ab"}},{"cell_type":"code","source":["print(\"Section 4: Automated Prompt Optimization Techniques\")\n","\n","def generate_template_variants(base_template, components):\n","    \"\"\"Generate multiple template variants by combining components.\"\"\"\n","    variants = []\n","\n","    # Create templates with different combinations of components\n","    import itertools\n","    for r in range(1, len(components) + 1):\n","        for combo in itertools.combinations(components.items(), r):\n","            # Start with the base template\n","            variant_template = base_template\n","            description = \"Base template with: \"\n","            changes = []\n","\n","            # Add each selected component\n","            for name, text in combo:\n","                variant_template += text\n","                description += name + \", \"\n","                changes.append(f\"Added {name}\")\n","\n","            # Create a version object for this variant\n","            variant = TemplateVersion(\n","                template=variant_template,\n","                name=f\"variant_{'_'.join([name for name, _ in combo])}\",\n","                description=description.rstrip(\", \"),\n","                changes=changes\n","            )\n","\n","            variants.append(variant)\n","\n","    return variants\n","\n","# Define base template and optional components\n","base_template = \"\"\"\n","Answer the question based on the context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","\"\"\"\n","\n","optional_components = {\n","    \"anti_hallucination\": \"Only use information from the context. If the answer isn't in the context, say so.\\n\\n\",\n","    \"citation\": \"Cite your sources using [Document X] notation.\\n\\n\",\n","    \"structured_response\": \"Structure your answer with: (1) Direct answer (2) Supporting details (3) Source information.\\n\\n\",\n","    \"confidence\": \"Indicate your confidence in the answer (High/Medium/Low) based on how clearly it's stated in the context.\\n\\n\"\n","}\n","\n","# Generate template variants\n","print(\"Programmatic Template Generation:\")\n","variants = generate_template_variants(base_template, optional_components)\n","print(f\"Generated {len(variants)} template variants from {len(optional_components)} optional components\")\n","\n","for i, variant in enumerate(variants[:3]):  # Show first 3 as examples\n","    print(f\"\\nVariant {i+1}: {variant.name}\")\n","    print(f\"Description: {variant.description}\")\n","    print(f\"Changes: {', '.join(variant.changes)}\")\n","    print(\"Template preview:\")\n","    print(variant.template)\n","\n","def optimize_token_allocation(base_allocation, eval_function, steps=5, learning_rate=0.05):\n","    \"\"\"Optimize token allocation between different prompt components.\"\"\"\n","    print(\"\\nToken Allocation Optimization:\")\n","    print(f\"Starting allocation: {base_allocation}\")\n","\n","    current_allocation = base_allocation.copy()\n","    best_allocation = base_allocation.copy()\n","    best_score = eval_function(best_allocation)\n","    print(f\"Initial score: {best_score:.4f}\")\n","\n","    improvements = []\n","\n","    for step in range(steps):\n","        print(f\"\\nStep {step+1}:\")\n","        improved = False\n","\n","        # Try adjusting each component\n","        for component in current_allocation:\n","            # Try increasing this component's allocation\n","            test_allocation = current_allocation.copy()\n","            test_allocation[component] += 50  # Add 50 tokens\n","\n","            # Normalize to maintain total token count\n","            total = sum(test_allocation.values())\n","            target_total = sum(current_allocation.values())\n","            scaling_factor = target_total / total\n","            test_allocation = {k: int(v * scaling_factor) for k, v in test_allocation.items()}\n","\n","            # Evaluate\n","            score = eval_function(test_allocation)\n","            print(f\"  Testing more tokens for {component}: score = {score:.4f}\")\n","\n","            if score > best_score:\n","                best_allocation = test_allocation\n","                best_score = score\n","                improved = True\n","                improvements.append((step, component, \"increase\", best_score))\n","\n","        if not improved:\n","            print(\"  No improvement found in this step\")\n","            break\n","\n","        # Update current allocation\n","        current_allocation = best_allocation.copy()\n","        print(f\"  New best allocation: {current_allocation}\")\n","        print(f\"  New best score: {best_score:.4f}\")\n","\n","    return best_allocation, best_score, improvements\n","\n","# Define a simple evaluation function for token allocation\n","def evaluate_token_allocation(allocation):\n","    \"\"\"\n","    Evaluate a token allocation strategy.\n","    This is a mock function - in practice would use actual performance metrics.\n","    \"\"\"\n","    # This function simulates performance with different token allocations\n","    # The formula represents a hypothetical relationship between token allocation and performance\n","    # - More tokens for context is good up to a point, then diminishing returns\n","    # - Instructions benefit from having enough tokens but not too many\n","    # - Question needs just enough tokens\n","    # - System prompt has minimal impact\n","\n","    context_score = min(1.0, allocation[\"context\"] / 1000) * 0.6\n","    instruction_score = min(1.0, allocation[\"instructions\"] / 300) * 0.3\n","    question_score = min(1.0, allocation[\"question\"] / 100) * 0.1\n","    system_score = min(1.0, allocation[\"system\"] / 200) * 0.05\n","\n","    # Penalize if context gets too little\n","    if allocation[\"context\"] < 500:\n","        context_score *= 0.5\n","\n","    # Penalize if instructions get too verbose\n","    if allocation[\"instructions\"] > 500:\n","        instruction_score *= 0.8\n","\n","    # Calculate total score\n","    return context_score + instruction_score + question_score + system_score\n","\n","# Initial token allocation\n","token_allocation = {\n","    \"system\": 200,\n","    \"context\": 800,\n","    \"instructions\": 200,\n","    \"question\": 100\n","}\n","\n","# Run optimization\n","optimized_allocation, optimized_score, improvement_history = optimize_token_allocation(\n","    token_allocation, evaluate_token_allocation\n",")\n","\n","print(\"\\nOptimization Results:\")\n","print(f\"Initial allocation: {token_allocation}\")\n","print(f\"Optimized allocation: {optimized_allocation}\")\n","print(f\"Improvement: {optimized_score - evaluate_token_allocation(token_allocation):.4f}\")\n","\n","# Evolution-based approach (conceptual overview)\n","print(\"\\nEvolution-Based Optimization (conceptual overview):\")\n","print(\"1. Create an initial population of templates with different instructions/structures\")\n","print(\"2. Evaluate each template's performance on a test set\")\n","print(\"3. Select the best-performing templates to 'reproduce'\")\n","print(\"4. Create 'offspring' by combining and mutating successful templates\")\n","print(\"5. Repeat for multiple generations\")\n","print(\"6. The highest-performing templates emerge through evolutionary pressure\")\n","\n","def create_initial_population(size=10):\n","    \"\"\"Create initial population of templates with random components.\"\"\"\n","    population = []\n","\n","    for i in range(size):\n","        # Start with base template\n","        template = \"Answer the question based on the context.\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\n\"\n","\n","        # Randomly add components\n","        if random.random() > 0.5:\n","            template += \"Use ONLY information from the context.\\n\"\n","        if random.random() > 0.5:\n","            template += \"Cite sources using [Document X] notation.\\n\"\n","        if random.random() > 0.7:\n","            template += \"Structure your answer in a clear, concise manner.\\n\"\n","        if random.random() > 0.8:\n","            template += \"If information is missing from the context, say so clearly.\\n\"\n","\n","        template += \"\\nANSWER:\"\n","        population.append(template)\n","\n","    return population\n","\n","def mutate_template(template, mutation_rate=0.3):\n","    \"\"\"Apply random mutations to a template.\"\"\"\n","    # Possible mutations\n","    mutations = [\n","        (\"Add anti-hallucination\", \"Only use information from the context. Do not use external knowledge.\\n\"),\n","        (\"Add citation\", \"Cite your sources using [Document X] notation.\\n\"),\n","        (\"Add structure\", \"Structure your answer with: (1) Direct answer (2) Supporting details.\\n\"),\n","        (\"Add confidence\", \"Indicate your confidence level in the answer.\\n\")\n","    ]\n","\n","    # Apply random mutations\n","    for name, text in mutations:\n","        if random.random() < mutation_rate and text not in template:\n","            # Find a position before \"ANSWER:\" to insert the mutation\n","            answer_pos = template.find(\"\\nANSWER:\")\n","            if answer_pos == -1:\n","                template += text\n","            else:\n","                template = template[:answer_pos] + \"\\n\" + text + template[answer_pos:]\n","\n","    return template\n","\n","def evolve_templates(population, fitness_function, generations=3):\n","    \"\"\"Evolve a population of templates using a genetic algorithm approach.\"\"\"\n","    print(f\"\\nEvolving templates across {generations} generations:\")\n","\n","    history = []\n","\n","    for generation in range(generations):\n","        print(f\"\\nGeneration {generation+1}:\")\n","\n","        # Evaluate fitness\n","        fitness_scores = []\n","        for i, template in enumerate(population):\n","            # In a real system, this would use the actual evaluation metrics\n","            # Here we'll use a random score for demonstration\n","            fitness = fitness_function(template)\n","            fitness_scores.append((i, fitness))\n","            print(f\"  Template {i+1}: fitness = {fitness:.4f}\")\n","\n","        # Sort by fitness\n","        fitness_scores.sort(key=lambda x: x[1], reverse=True)\n","\n","        # Keep track of best template in this generation\n","        best_idx, best_fitness = fitness_scores[0]\n","        history.append((generation, population[best_idx], best_fitness))\n","\n","        # Create next generation\n","        next_gen = []\n","\n","        # Elitism: keep the best templates\n","        elite_count = len(population) // 4\n","        for i in range(elite_count):\n","            if i < len(fitness_scores):\n","                idx = fitness_scores[i][0]\n","                next_gen.append(population[idx])\n","\n","        # Create offspring until we fill the population\n","        while len(next_gen) < len(population):\n","            # Tournament selection\n","            parent_indices = random.sample(range(len(population)), 2)\n","            if fitness_scores[parent_indices[0]][1] > fitness_scores[parent_indices[1]][1]:\n","                parent1 = population[parent_indices[0]]\n","            else:\n","                parent1 = population[parent_indices[1]]\n","\n","            parent_indices = random.sample(range(len(population)), 2)\n","            if fitness_scores[parent_indices[0]][1] > fitness_scores[parent_indices[1]][1]:\n","                parent2 = population[parent_indices[0]]\n","            else:\n","                parent2 = population[parent_indices[1]]\n","\n","            # Crossover (simplified for text templates)\n","            # In practice, would use more sophisticated template representation\n","            # Here we'll just take the first half from parent1 and second half from parent2\n","            split_point = len(parent1) // 2\n","            child = parent1[:split_point] + parent2[split_point:]\n","\n","            # Mutation\n","            child = mutate_template(child)\n","\n","            next_gen.append(child)\n","\n","        # Update population for next generation\n","        population = next_gen\n","\n","    # Return best template from the final generation\n","    final_fitness_scores = [(i, fitness_function(template)) for i, template in enumerate(population)]\n","    final_fitness_scores.sort(key=lambda x: x[1], reverse=True)\n","    best_idx = final_fitness_scores[0][0]\n","\n","    return population[best_idx], history\n","\n","# Mock fitness function for template evolution\n","def mock_fitness_function(template):\n","    \"\"\"\n","    Mock fitness function for template evaluation.\n","    This simulates evaluating a template's performance.\n","    \"\"\"\n","    # In a real system, this would run the template against a test set\n","    # and calculate actual performance metrics\n","\n","    # Here we'll use a simplified scoring approach based on desirable features\n","    score = 0.5  # Base score\n","\n","    # Check for desirable features\n","    if \"ONLY\" in template or \"only\" in template:\n","        score += 0.1  # Anti-hallucination is good\n","    if \"[Document\" in template:\n","        score += 0.15  # Citation is good\n","    if \"structure\" in template.lower():\n","        score += 0.05  # Structure guidance is good\n","    if \"confidence\" in template.lower():\n","        score += 0.05  # Confidence indication is good\n","    if \"missing\" in template.lower() and \"information\" in template.lower():\n","        score += 0.1  # Handling missing information is good\n","\n","    # Add some randomness to simulate real-world variation\n","    score += random.uniform(-0.05, 0.05)\n","\n","    # Ensure score is between 0 and 1\n","    return max(0, min(1, score))\n","\n","# Try the evolution approach\n","initial_population = create_initial_population(size=6)\n","best_template, evolution_history = evolve_templates(\n","    initial_population,\n","    mock_fitness_function,\n","    generations=3\n",")\n","\n","print(\"\\nBest Template After Evolution:\")\n","print(best_template)\n","print(f\"Final fitness score: {mock_fitness_function(best_template):.4f}\")\n","\n","print(\"\\nEvolution History:\")\n","for generation, template, fitness in evolution_history:\n","    print(f\"Generation {generation+1}: fitness = {fitness:.4f}\")\n","    print(f\"Template preview: {template[:100]}...\")\n","\n","print_separator()"],"metadata":{"id":"1TeaOfDCl8Mi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Case Study - Optimizing a RAG Template**"],"metadata":{"id":"_Vhwm4Osl9AB"}},{"cell_type":"code","source":["print(\"Section 5: Case Study - Optimizing a RAG Template\")\n","\n","# Initial template\n","initial_rag_template = \"\"\"\n","Answer the question based on the context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","# Problem statement\n","print(\"Problem: Users report that our RAG system sometimes provides incorrect information\")\n","print(\"or fails to cite sources, making it difficult to verify answers.\")\n","print(\"\\nGoal: Optimize the template to improve accuracy, reduce hallucinations,\")\n","print(\"and increase source attribution.\")\n","\n","# Iteration 1: Add anti-hallucination instruction\n","iter1_template = \"\"\"\n","Answer the question based ONLY on the information in the context.\n","If the context doesn't contain the answer, say \"I don't have enough information.\"\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","print(\"\\nIteration 1: Added anti-hallucination instruction\")\n","print(\"Results: Hallucination rate decreased from 18% to 9%,\")\n","print(\"but factual accuracy also decreased from 75% to 72%\")\n","\n","# Iteration 2: Add citation requirement\n","iter2_template = \"\"\"\n","Answer the question based ONLY on the information in the context.\n","If the context doesn't contain the answer, say \"I don't have enough information.\"\n","Cite your sources using [Document X] notation.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","print(\"\\nIteration 2: Added citation requirement\")\n","print(\"Results: Citation rate increased from 15% to 78%,\")\n","print(\"factual accuracy increased to 74%,\")\n","print(\"hallucination rate remained at 9%\")\n","\n","# Iteration 3: Add structured instructions\n","iter3_template = \"\"\"\n","You are an assistant for question-answering tasks.\n","Answer based EXCLUSIVELY on the provided context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","1. If the answer is in the context, provide it clearly and concisely\n","2. Cite specific documents using [Document X] notation\n","3. If the answer isn't in the context, say \"I don't have enough information\"\n","4. Only use information present in the context\n","\n","ANSWER:\n","\"\"\"\n","\n","print(\"\\nIteration 3: Added structured instructions and system context\")\n","print(\"Results: Factual accuracy increased to 83%,\")\n","print(\"hallucination rate decreased to 4%,\")\n","print(\"citation rate increased to 92%\")\n","\n","# Iteration 4: Add response format guidance\n","final_template = \"\"\"\n","You are an assistant for question-answering tasks.\n","Answer based EXCLUSIVELY on the provided context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","1. If the answer is in the context, provide it clearly and concisely\n","2. Cite specific documents using [Document X] notation\n","3. If the answer isn't in the context, say \"I don't have enough information\"\n","4. Only use information present in the context\n","\n","FORMAT YOUR RESPONSE AS FOLLOWS:\n","- Start with a direct answer to the question\n","- Provide supporting details from the context\n","- Include relevant citations for each fact\n","- If different documents contain conflicting information, acknowledge this\n","\n","ANSWER:\n","\"\"\"\n","\n","print(\"\\nIteration 4: Added response format guidance\")\n","print(\"Results: Factual accuracy increased to 85%,\")\n","print(\"hallucination rate decreased to 3%,\")\n","print(\"citation rate remained at 92%,\")\n","print(\"response coherence improved by 23%\")\n","\n","# Summary of improvements\n","print(\"\\nSummary of template optimization process:\")\n","templates = [initial_rag_template, iter1_template, iter2_template, iter3_template, final_template]\n","names = [\"Initial\", \"Anti-hallucination\", \"Citation\", \"Structured\", \"Final\"]\n","\n","# Map of mock metrics across iterations\n","metrics = {\n","    \"factual_accuracy\": [0.75, 0.72, 0.74, 0.83, 0.85],\n","    \"hallucination_rate\": [0.18, 0.09, 0.09, 0.04, 0.03],\n","    \"citation_rate\": [0.15, 0.15, 0.78, 0.92, 0.92],\n","    \"coherence\": [0.65, 0.67, 0.70, 0.75, 0.80]\n","}\n","\n","# Create a pandas DataFrame for better visualization\n","try:\n","    metrics_df = pd.DataFrame(metrics, index=names)\n","    print(\"\\nMetrics across iterations:\")\n","    print(metrics_df)\n","\n","    # Create visualization\n","    plt.figure(figsize=(12, 8))\n","\n","    for metric in metrics:\n","        if metric == \"hallucination_rate\":\n","            # For hallucination, lower is better so we'll plot 1 - rate\n","            plt.plot(names, [1-v for v in metrics[metric]], 'o-', label=f\"Anti-{metric}\")\n","        else:\n","            plt.plot(names, metrics[metric], 'o-', label=metric)\n","\n","    plt.title('RAG Template Optimization Progress')\n","    plt.xlabel('Template Version')\n","    plt.ylabel('Score (higher is better)')\n","    plt.ylim(0, 1)\n","    plt.grid(True, linestyle='--', alpha=0.7)\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","except Exception as e:\n","    print(f\"Error creating DataFrame or plot: {e}\")\n","\n","    # Fallback text display\n","    print(\"\\nMetrics across iterations:\")\n","    for metric in metrics:\n","        print(f\"{metric}: {', '.join([str(round(v, 2)) for v in metrics[metric]])}\")\n","\n","print(\"\\nKey learnings from the optimization process:\")\n","print(\"1. Anti-hallucination instructions are effective but can reduce coverage\")\n","print(\"2. Citation requirements dramatically improve verification but need clear formatting\")\n","print(\"3. Structured instructions improve both accuracy and citation quality\")\n","print(\"4. Response format guidance improves coherence without sacrificing accuracy\")\n","print(\"5. Each improvement should be evaluated independently before combining\")\n","\n","print_separator()"],"metadata":{"id":"YfTHlS1bl5kP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Best Practices for Template Testing and Optimization**"],"metadata":{"id":"gk-sg5cDmgWA"}},{"cell_type":"code","source":["print(\"Section 6: Best Practices for Template Testing and Optimization\")\n","\n","print(\"1. Start with a Diverse Test Set\")\n","print(\"   - Include multiple question types (factual, comparative, explanatory)\")\n","print(\"   - Cover edge cases (ambiguous questions, missing information)\")\n","print(\"   - Include examples with contradictory or partial information\")\n","print(\"   - Test with varying context lengths and complexities\")\n","\n","print(\"\\n2. Use Clear Evaluation Metrics\")\n","print(\"   - Define objective metrics that align with application goals\")\n","print(\"   - Balance between factual accuracy, citation quality, and coherence\")\n","print(\"   - Consider human evaluation for subjective aspects\")\n","print(\"   - Track multiple metrics to avoid optimization tradeoffs\")\n","\n","print(\"\\n3. Isolate Changes for Proper Attribution\")\n","print(\"   - Test one change at a time to understand its impact\")\n","print(\"   - Create controlled A/B tests with sufficient sample size\")\n","print(\"   - Use statistical significance testing for reliable conclusions\")\n","print(\"   - Document all changes and their measured effects\")\n","\n","print(\"\\n4. Follow an Iterative Cycle\")\n","print(\"   - Start with a simple baseline template\")\n","print(\"   - Analyze error patterns in detail before making changes\")\n","print(\"   - Implement targeted improvements based on error analysis\")\n","print(\"   - Measure impact of each change before proceeding\")\n","print(\"   - Be willing to revert changes that don't improve performance\")\n","\n","print(\"\\n5. Consider Computational and Token Efficiency\")\n","print(\"   - Monitor token usage as template complexity increases\")\n","print(\"   - Balance instruction detail with token efficiency\")\n","print(\"   - Optimize allocation of tokens between components\")\n","print(\"   - Consider the impact on inference time and cost\")\n","\n","print(\"\\n6. Test in Real-World Conditions\")\n","print(\"   - Move beyond synthetic test sets to real user queries\")\n","print(\"   - Consider different document types and quality levels\")\n","print(\"   - Test with various retrieval qualities (perfect vs. noisy)\")\n","print(\"   - Evaluate performance degradation with larger context windows\")\n","\n","print(\"\\n7. Use Template Versioning\")\n","print(\"   - Maintain a history of template versions and their performance\")\n","print(\"   - Document the rationale behind each change\")\n","print(\"   - Implement a systematic version naming convention\")\n","print(\"   - Enable rollback to previous versions if needed\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")\n","\n","# Key Takeaways:\n","# 1. Systematic template evaluation requires clear metrics tailored to application goals\n","# 2. A/B testing with controlled variables helps identify effective template components\n","# 3. Iterative refinement based on error analysis leads to continuous improvement\n","# 4. Automated optimization techniques can efficiently explore the template design space\n","# 5. Balancing multiple performance metrics is essential for optimal template design"],"metadata":{"id":"qPx4o5SQmgim"},"execution_count":null,"outputs":[]}]}