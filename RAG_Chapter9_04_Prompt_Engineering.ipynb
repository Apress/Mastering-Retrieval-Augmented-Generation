{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/e3Jrq3Hx0kwvWzK0upAZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"w6xA40PYeOXw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qAGIIgEeLei"},"outputs":[],"source":["!pip install langchain langchain-openai tiktoken openai\n","\n","import os\n","import re\n","import json\n","from typing import List, Dict, Any, Optional\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","import tiktoken\n","from langchain.prompts import PromptTemplate\n","from langchain_core.documents import Document\n","from langchain_openai import OpenAI, ChatOpenAI"]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"SgaE5SJmflu8"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Create sample documents for testing\n","sample_docs = [\n","    Document(page_content=\"The Eiffel Tower is 330 meters (1,083 ft) tall, about the same height as an 81-story building, and was the tallest man-made structure in the world from its completion in 1889 until 1930.\",\n","             metadata={\"source\": \"travel_guide\", \"page\": 25}),\n","    Document(page_content=\"The Tower was built by Gustave Eiffel for the 1889 World's Fair. Initially criticized by some of France's leading artists and intellectuals for its design, it has since become a global cultural icon of France.\",\n","             metadata={\"source\": \"history_book\", \"page\": 42}),\n","    Document(page_content=\"The Eiffel Tower is made of wrought iron and weighs approximately 10,100 tonnes. It has three levels for visitors, with restaurants on the first and second levels.\",\n","             metadata={\"source\": \"engineering_text\", \"page\": 89}),\n","    Document(page_content=\"Tokyo Tower was completed in 1958 and is modeled after the Eiffel Tower, although it is painted white and orange to comply with air safety regulations. At 333 meters, it is slightly taller than the Eiffel Tower.\",\n","             metadata={\"source\": \"global_landmarks\", \"page\": 118}),\n","]\n","\n","# Create contradictory documents for testing\n","contradictory_docs = [\n","    Document(page_content=\"Studies suggest drinking coffee can increase the risk of heart disease due to its caffeine content raising blood pressure temporarily.\",\n","             metadata={\"source\": \"health_study_2005\", \"page\": 45}),\n","    Document(page_content=\"Recent research from 2022 indicates that moderate coffee consumption (3-4 cups daily) may actually decrease heart disease risk by improving arterial function.\",\n","             metadata={\"source\": \"medical_journal_2022\", \"page\": 112}),\n","    Document(page_content=\"A meta-analysis found no conclusive evidence linking coffee consumption to heart disease in healthy individuals.\",\n","             metadata={\"source\": \"nutrition_science\", \"page\": 78}),\n","    Document(page_content=\"Excessive coffee intake (over 6 cups daily) has been associated with increased heart palpitations and potential strain on the cardiovascular system.\",\n","             metadata={\"source\": \"cardiology_review\", \"page\": 23}),\n","]"],"metadata":{"id":"iBORIcOBfl3Z","executionInfo":{"status":"ok","timestamp":1741265245411,"user_tz":-330,"elapsed":67,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: Strategies for Reducing Hallucinations**"],"metadata":{"id":"d6KdLryyfx1Q"}},{"cell_type":"code","source":["print(\"Section 1: Strategies for Reducing Hallucinations\")\n","\n","# Explicit Constraints and Boundaries\n","anti_hallucination_template = \"\"\"\n","Answer the question based EXCLUSIVELY on the provided context.\n","If the context doesn't contain enough information to answer completely,\n","say \"I don't have enough information\" rather than guessing.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","# Knowledge Boundary Acknowledgment\n","boundary_template = \"\"\"\n","Use ONLY the facts present in the context below to answer the question.\n","Your knowledge is limited to this context - do not introduce external information.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Begin your response with a statement of what you know based on the context,\n","then provide your answer based strictly on that information.\n","\n","ANSWER:\n","\"\"\"\n","\n","# Source Attribution Requirements\n","attribution_template = \"\"\"\n","Answer based solely on the provided documents.\n","\n","DOCUMENTS:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","1. Every factual statement must cite its source document using [Doc X] notation\n","2. If information from multiple documents is used, cite each source\n","3. If the question cannot be answered from the documents, state this clearly\n","\n","ANSWER:\n","\"\"\"\n","\n","# Test hallucination reduction templates\n","context = \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)])\n","answerable_question = \"What is the height of the Eiffel Tower?\"\n","unanswerable_question = \"When was the Eiffel Tower renovated last?\"\n","\n","def format_and_analyze_prompt(template, context, question):\n","    \"\"\"Format a prompt and analyze its token usage.\"\"\"\n","    formatted = template.format(context=context, question=question)\n","    tokens = count_tokens(formatted)\n","    print(f\"Tokens: {tokens}\")\n","    return formatted\n","\n","print(\"\\nExplicit Constraints Template with answerable question:\")\n","formatted_anti_hallucination = format_and_analyze_prompt(\n","    anti_hallucination_template, context, answerable_question\n",")\n","print(formatted_anti_hallucination)\n","\n","print(\"\\nBoundary Template with unanswerable question:\")\n","formatted_boundary = format_and_analyze_prompt(\n","    boundary_template, context, unanswerable_question\n",")\n","print(formatted_boundary)\n","\n","print(\"\\nAttribution Template with answerable question:\")\n","formatted_attribution = format_and_analyze_prompt(\n","    attribution_template, context, answerable_question\n",")\n","print(formatted_attribution)\n","\n","# Evaluate with LLM if API key is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        llm = ChatOpenAI(temperature=0)\n","\n","        print(\"\\nTesting anti-hallucination techniques with LLM:\")\n","\n","        print(\"\\nStandard template (control):\")\n","        standard_template = \"\"\"\n","        Answer the question based on the provided context.\n","\n","        CONTEXT:\n","        {context}\n","\n","        QUESTION:\n","        {question}\n","        \"\"\"\n","\n","        standard_response = llm.invoke(standard_template.format(\n","            context=context,\n","            question=unanswerable_question\n","        ))\n","        print(f\"Response: {standard_response.content}\")\n","\n","        print(\"\\nAnti-hallucination template:\")\n","        anti_hall_response = llm.invoke(anti_hallucination_template.format(\n","            context=context,\n","            question=unanswerable_question\n","        ))\n","        print(f\"Response: {anti_hall_response.content}\")\n","    except Exception as e:\n","        print(f\"Error testing with LLM: {e}\")\n","else:\n","    print(\"\\nOpenAI API key not set - skipping LLM evaluation\")\n","\n","print_separator()"],"metadata":{"id":"eQNOmVSqfx-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: Techniques to Improve Relevance and Coherence**"],"metadata":{"id":"dQ81O6J3f-e5"}},{"cell_type":"code","source":["print(\"Section 2: Techniques to Improve Relevance and Coherence\")\n","\n","# Contextual Priming\n","priming_template = \"\"\"\n","You are answering a question about {domain}.\n","When working with {domain} information, it's important to consider {key_considerations}.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Using the context provided, give a well-structured answer addressing the question.\n","\n","ANSWER:\n","\"\"\"\n","\n","# Response Structuring\n","structured_response_template = \"\"\"\n","Answer the question using the provided context.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Please structure your answer as follows:\n","1. Direct answer to the question (1-2 sentences)\n","2. Supporting evidence from the context\n","3. Any important nuances or limitations to consider\n","\n","ANSWER:\n","\"\"\"\n","\n","# Query-Focused Reasoning\n","reasoning_template = \"\"\"\n","Answer the question using the provided information.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","REASONING PROCESS:\n","1. First, identify what the question is specifically asking for\n","2. Then, locate the relevant information in the context\n","3. Consider whether the context fully answers the question\n","4. Formulate your answer based on this analysis\n","\n","ANSWER:\n","\"\"\"\n","\n","# Test relevance and coherence templates\n","print(\"\\nContextual Priming Template:\")\n","domain = \"historical architecture\"\n","key_considerations = \"historical context, architectural significance, and cultural impact\"\n","formatted_priming = priming_template.format(\n","    domain=domain,\n","    key_considerations=key_considerations,\n","    context=context,\n","    question=\"Why is the Eiffel Tower significant?\"\n",")\n","print(formatted_priming)\n","print(f\"Tokens: {count_tokens(formatted_priming)}\")\n","\n","print(\"\\nStructured Response Template:\")\n","formatted_structured = structured_response_template.format(\n","    context=context,\n","    question=\"How tall is the Eiffel Tower compared to Tokyo Tower?\"\n",")\n","print(formatted_structured)\n","print(f\"Tokens: {count_tokens(formatted_structured)}\")\n","\n","print(\"\\nReasoning Template:\")\n","formatted_reasoning = reasoning_template.format(\n","    context=context,\n","    question=\"What materials were used to build the Eiffel Tower and why are they significant?\"\n",")\n","print(formatted_reasoning)\n","print(f\"Tokens: {count_tokens(formatted_reasoning)}\")\n","\n","print_separator()"],"metadata":{"id":"xI7KyVnnf-n-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Methods for Handling Contradictory Information**"],"metadata":{"id":"FRAKSWedgGxU"}},{"cell_type":"code","source":["print(\"Section 3: Methods for Handling Contradictory Information\")\n","\n","# Contradiction Acknowledgment\n","contradiction_template = \"\"\"\n","Answer based on the following information, which may contain contradictions.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","- If you notice contradictory information, explicitly identify the contradiction\n","- Present both perspectives with their respective sources\n","- If possible, explain potential reasons for the contradiction\n","- Indicate which answer has stronger support, if applicable\n","\n","ANSWER:\n","\"\"\"\n","\n","# Information Quality Assessment\n","quality_assessment_template = \"\"\"\n","Analyze the following information and answer the question.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","When answering:\n","1. Assess the reliability of each piece of information (recency, source credibility)\n","2. Prioritize information from more authoritative sources\n","3. Consider the consistency across multiple sources\n","4. Indicate confidence level in your final answer\n","\n","ANSWER:\n","\"\"\"\n","\n","# Multi-Perspective Synthesis\n","multi_perspective_template = \"\"\"\n","The following sources may present different perspectives on the question.\n","\n","SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Present a balanced answer that:\n","- Synthesizes the different perspectives\n","- Acknowledges areas of consensus and disagreement\n","- Avoids favoring one viewpoint without justification\n","- Helps the reader understand the full picture\n","\n","ANSWER:\n","\"\"\"\n","\n","# Test contradiction handling templates\n","contradiction_context = \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(contradictory_docs)])\n","contradiction_question = \"Does coffee increase the risk of heart disease?\"\n","\n","print(\"\\nContradiction Template:\")\n","formatted_contradiction = format_and_analyze_prompt(\n","    contradiction_template, contradiction_context, contradiction_question\n",")\n","print(formatted_contradiction)\n","\n","print(\"\\nQuality Assessment Template:\")\n","formatted_quality = format_and_analyze_prompt(\n","    quality_assessment_template, contradiction_context, contradiction_question\n",")\n","print(formatted_quality)\n","\n","print(\"\\nMulti-Perspective Template:\")\n","formatted_multi = format_and_analyze_prompt(\n","    multi_perspective_template, contradiction_context, contradiction_question\n",")\n","print(formatted_multi)\n","\n","# Test with LLM if API key is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        llm = ChatOpenAI(temperature=0)\n","\n","        print(\"\\nTesting contradiction handling with LLM:\")\n","        contradiction_response = llm.invoke(contradiction_template.format(\n","            context=contradiction_context,\n","            question=contradiction_question\n","        ))\n","        print(f\"Response: {contradiction_response.content}\")\n","    except Exception as e:\n","        print(f\"Error testing with LLM: {e}\")\n","else:\n","    print(\"\\nOpenAI API key not set - skipping LLM evaluation\")\n","\n","print_separator()"],"metadata":{"id":"Hm8v0vY8gHBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Approaches for Source Attribution and Citation**"],"metadata":{"id":"bK4sCe9hgRJB"}},{"cell_type":"code","source":["print(\"Section 4: Approaches for Source Attribution and Citation\")\n","\n","# Inline Citation Format\n","inline_citation_template = \"\"\"\n","Answer the question using the provided sources.\n","\n","SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Cite your sources using the format [Source X] immediately after each piece\n","of information that comes from that source. Every factual statement should\n","have a citation.\n","\n","ANSWER:\n","\"\"\"\n","\n","# Evidence Grading\n","evidence_grading_template = \"\"\"\n","Answer the question based on the provided information.\n","\n","INFORMATION:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","For each key point in your answer:\n","1. Cite the relevant source(s) [Source X]\n","2. Indicate evidence strength (Strong, Moderate, Limited)\n","3. Note if critical information is missing\n","\n","ANSWER:\n","\"\"\"\n","\n","# Source Qualification\n","source_qualification_template = \"\"\"\n","Answer using the following information sources.\n","\n","SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","When citing sources, include relevant qualifiers:\n","- Recency: Note publication date when relevant [Source X, 2022]\n","- Type: Identify source type [Source X, Research Paper]\n","- Agreement: Mention if multiple sources confirm the information [Sources X, Y]\n","\n","ANSWER:\n","\"\"\"\n","\n","# Test citation templates\n","print(\"\\nInline Citation Template:\")\n","formatted_inline = format_and_analyze_prompt(\n","    inline_citation_template, context, \"What is the design and history of the Eiffel Tower?\"\n",")\n","print(formatted_inline)\n","\n","print(\"\\nEvidence Grading Template:\")\n","formatted_evidence = format_and_analyze_prompt(\n","    evidence_grading_template, contradiction_context, \"What are the health effects of coffee consumption?\"\n",")\n","print(formatted_evidence)\n","\n","print(\"\\nSource Qualification Template:\")\n","formatted_qualification = format_and_analyze_prompt(\n","    source_qualification_template, contradiction_context, \"How does coffee affect heart health?\"\n",")\n","print(formatted_qualification)\n","\n","print_separator()"],"metadata":{"id":"smVoXCuKgRQD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Combining Techniques for Complex RAG Scenarios**"],"metadata":{"id":"Y0HY3ht9gZ-l"}},{"cell_type":"code","source":["print(\"Section 5: Combining Techniques for Complex RAG Scenarios\")\n","\n","# Comprehensive RAG template combining multiple techniques\n","comprehensive_template = \"\"\"\n","You are a knowledgeable assistant answering questions based solely on the provided information.\n","\n","SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","APPROACH:\n","1. First, identify if the sources contain sufficient information to answer the question\n","2. If you notice contradictions, explicitly acknowledge them\n","3. Consider the reliability and recency of each source\n","4. Organize your response in a clear, structured manner\n","\n","RESPONSE REQUIREMENTS:\n","- Cite sources for every factual claim using [Source X] notation\n","- Indicate confidence level for claims (High, Medium, Low)\n","- Clearly state when information is missing or incomplete\n","- Present a balanced view when multiple perspectives exist\n","- Begin with a direct answer, followed by supporting details\n","\n","ANSWER:\n","\"\"\"\n","\n","# Test comprehensive template\n","print(\"\\nComprehensive RAG Template:\")\n","formatted_comprehensive = format_and_analyze_prompt(\n","    comprehensive_template, contradiction_context, \"What is the relationship between coffee consumption and heart health?\"\n",")\n","print(formatted_comprehensive)\n","\n","# Test with LLM if API key is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        llm = ChatOpenAI(temperature=0)\n","\n","        print(\"\\nTesting comprehensive template with LLM:\")\n","        comprehensive_response = llm.invoke(comprehensive_template.format(\n","            context=contradiction_context,\n","            question=\"What is the relationship between coffee consumption and heart health?\"\n","        ))\n","        print(f\"Response: {comprehensive_response.content}\")\n","    except Exception as e:\n","        print(f\"Error testing with LLM: {e}\")\n","else:\n","    print(\"\\nOpenAI API key not set - skipping LLM evaluation\")\n","\n","print_separator()"],"metadata":{"id":"ciinMSscgaGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Evaluating Prompt Engineering Techniques**"],"metadata":{"id":"umKjZO8ggiex"}},{"cell_type":"code","source":["print(\"Section 6: Evaluating Prompt Engineering Techniques\")\n","\n","def evaluate_template(template_name, template, context, question, expected_elements=[]):\n","    \"\"\"\n","    Evaluate a template against specific criteria.\n","\n","    Args:\n","        template_name: Name of the template\n","        template: The template text\n","        context: Context to use in formatting\n","        question: Question to use in formatting\n","        expected_elements: List of elements that should be in a good response\n","\n","    Returns:\n","        Dictionary with evaluation metrics\n","    \"\"\"\n","    formatted = template.format(context=context, question=question)\n","    token_count = count_tokens(formatted)\n","\n","    # Calculate clarity score (simple heuristic)\n","    clarity_score = 0\n","    if \"CONTEXT:\" in template or \"SOURCES:\" in template:\n","        clarity_score += 1\n","    if \"QUESTION:\" in template:\n","        clarity_score += 1\n","    if \"INSTRUCTIONS:\" in template or \"APPROACH:\" in template:\n","        clarity_score += 1\n","    if \"ANSWER:\" in template:\n","        clarity_score += 1\n","\n","    # Calculate constraint score (simple heuristic)\n","    constraint_score = 0\n","    constraint_phrases = [\n","        \"only\", \"exclusively\", \"solely\", \"strictly\",\n","        \"do not\", \"avoid\", \"must\", \"should\", \"limit\"\n","    ]\n","    for phrase in constraint_phrases:\n","        if phrase in template.lower():\n","            constraint_score += 0.5\n","    constraint_score = min(constraint_score, 3)  # Cap at 3\n","\n","    # Calculate guidance score (simple heuristic)\n","    guidance_score = 0\n","    if \"steps\" in template.lower() or \"process\" in template.lower():\n","        guidance_score += 1\n","    if \"1.\" in template and \"2.\" in template:\n","        guidance_score += 1\n","    if \"structure\" in template.lower():\n","        guidance_score += 1\n","\n","    # Overall score\n","    overall_score = (clarity_score + constraint_score + guidance_score) / 3\n","\n","    return {\n","        \"name\": template_name,\n","        \"tokens\": token_count,\n","        \"clarity\": clarity_score,\n","        \"constraints\": constraint_score,\n","        \"guidance\": guidance_score,\n","        \"overall\": overall_score\n","    }\n","\n","# Templates to evaluate\n","templates_to_evaluate = {\n","    \"Standard\": \"\"\"\n","    Answer the question based on the context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"Anti-Hallucination\": anti_hallucination_template,\n","    \"Attribution\": attribution_template,\n","    \"Reasoning\": reasoning_template,\n","    \"Contradiction\": contradiction_template,\n","    \"Comprehensive\": comprehensive_template\n","}\n","\n","# Evaluate all templates\n","evaluation_results = []\n","for name, template in templates_to_evaluate.items():\n","    result = evaluate_template(name, template, contradiction_context, contradiction_question)\n","    evaluation_results.append(result)\n","\n","# Print evaluation results\n","print(\"Template Evaluation Results:\")\n","print(f\"{'Template':<20} | {'Tokens':<7} | {'Clarity':<7} | {'Constraints':<11} | {'Guidance':<8} | {'Overall':<7}\")\n","print(\"-\" * 70)\n","for result in evaluation_results:\n","    print(f\"{result['name']:<20} | {result['tokens']:<7} | {result['clarity']:<7.1f} | {result['constraints']:<11.1f} | {result['guidance']:<8.1f} | {result['overall']:<7.1f}\")\n","\n","print(\"\\nNote: This is a simplified evaluation. For real-world use, templates should be evaluated\")\n","print(\"based on actual LLM outputs and compared against ground truth answers.\")\n","\n","print_separator()"],"metadata":{"id":"J5Fnj-YRgimI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 7: Real-World Examples and Case Studies**"],"metadata":{"id":"KQzjqEVtgrB3"}},{"cell_type":"code","source":["print(\"Section 7: Real-World Examples and Case Studies\")\n","\n","# Financial analysis example\n","financial_docs = [\n","    Document(page_content=\"Apple Inc. reported Q1 2023 revenue of $97.3 billion, beating analyst expectations of $93.9 billion. The company's services segment grew by 17% year-over-year, reaching an all-time high.\",\n","             metadata={\"source\": \"earnings_report\", \"date\": \"2023-04-28\"}),\n","    Document(page_content=\"Apple's gross margin for Q1 2023 was 43.3%, down slightly from 43.7% in the previous quarter but up from 42.5% year-over-year.\",\n","             metadata={\"source\": \"financial_analysis\", \"date\": \"2023-04-29\"}),\n","    Document(page_content=\"The company announced a $90 billion share repurchase program, maintaining its position as the largest dividend payer in the world.\",\n","             metadata={\"source\": \"investor_call_transcript\", \"date\": \"2023-04-28\"}),\n","    Document(page_content=\"Analysts at Morgan Stanley maintained their 'overweight' rating on Apple stock following the earnings report, with a price target of $180.\",\n","             metadata={\"source\": \"analyst_report\", \"date\": \"2023-05-01\"}),\n","]\n","\n","financial_template = \"\"\"\n","You are a financial analyst providing insights based on the latest earnings information.\n","\n","SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANALYSIS FRAMEWORK:\n","1. Begin with the key metrics directly addressing the question\n","2. Provide context and comparisons (quarter-over-quarter, year-over-year)\n","3. Note any divergence from analyst expectations\n","4. Include relevant forward-looking statements\n","\n","When presenting financial information:\n","- Cite the specific source for each data point [Source X]\n","- Note the date of each source for temporal context\n","- Distinguish between reported figures and projections/estimates\n","- Indicate confidence level when interpreting significance\n","\n","ANALYSIS:\n","\"\"\"\n","\n","financial_context = \"\\n\".join([\n","    f\"[Document {i+1}, {doc.metadata.get('source')}, {doc.metadata.get('date')}] {doc.page_content}\"\n","    for i, doc in enumerate(financial_docs)\n","])\n","financial_question = \"How did Apple perform in Q1 2023 and what are the key takeaways for investors?\"\n","\n","print(\"Financial Analysis Case Study:\")\n","formatted_financial = format_and_analyze_prompt(\n","    financial_template, financial_context, financial_question\n",")\n","print(formatted_financial)\n","\n","# Medical information example\n","medical_docs = [\n","    Document(page_content=\"A 2021 meta-analysis of 15 clinical trials found that statin therapy reduced major cardiovascular events by 24% for each 1 mmol/L reduction in LDL cholesterol.\",\n","             metadata={\"source\": \"journal_cardiology\", \"date\": \"2021-03-15\", \"type\": \"meta-analysis\"}),\n","    Document(page_content=\"Common side effects of statins include muscle pain (reported in 5-10% of patients), liver enzyme elevations (0.5-3%), and slightly increased risk of type 2 diabetes.\",\n","             metadata={\"source\": \"medical_guidelines\", \"date\": \"2022-01-10\", \"type\": \"clinical guidelines\"}),\n","    Document(page_content=\"For patients with LDL cholesterol above 190 mg/dL, statin therapy is recommended regardless of calculated 10-year ASCVD risk.\",\n","             metadata={\"source\": \"treatment_handbook\", \"date\": \"2022-07-22\", \"type\": \"clinical recommendation\"}),\n","    Document(page_content=\"Some studies suggest that CoQ10 supplementation may reduce statin-related muscle pain, though evidence remains inconclusive and larger trials are needed.\",\n","             metadata={\"source\": \"research_review\", \"date\": \"2020-11-05\", \"type\": \"literature review\"}),\n","]\n","\n","medical_template = \"\"\"\n","You are a medical information provider answering questions based strictly on the provided medical literature.\n","\n","MEDICAL SOURCES:\n","{context}\n","\n","PATIENT QUESTION:\n","{question}\n","\n","RESPONSE GUIDELINES:\n","1. Present information accurately based ONLY on the provided sources\n","2. Cite each source with its type and date [Source X, Type, Date]\n","3. Clearly distinguish between established medical consensus and areas of ongoing research\n","4. Use precise medical terminology but explain it clearly\n","5. Include standard medical disclaimer about consulting healthcare providers\n","\n","INFORMATION RESPONSE:\n","\"\"\"\n","\n","medical_context = \"\\n\".join([\n","    f\"[Document {i+1}, {doc.metadata.get('source')}, {doc.metadata.get('type')}, {doc.metadata.get('date')}] {doc.page_content}\"\n","    for i, doc in enumerate(medical_docs)\n","])\n","medical_question = \"What are the benefits and risks of statin medications, and are there ways to reduce side effects?\"\n","\n","print(\"\\nMedical Information Case Study:\")\n","formatted_medical = format_and_analyze_prompt(\n","    medical_template, medical_context, medical_question\n",")\n","print(formatted_medical)\n","\n","print_separator()"],"metadata":{"id":"ahDscTHOgrKN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 8: Practical Techniques for Common RAG Challenges**"],"metadata":{"id":"ogZU3xzpgzIY"}},{"cell_type":"code","source":["print(\"Section 8: Practical Techniques for Common RAG Challenges\")\n","\n","# Handling long documents\n","long_document_template = \"\"\"\n","Answer the question using the provided lengthy document extracts.\n","\n","DOCUMENT EXTRACTS:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","APPROACH:\n","1. Scan the extracts to locate the most relevant sections\n","2. Focus on passages that directly address the question\n","3. Look for headings, topic sentences, and key terms related to the question\n","4. Synthesize information from multiple relevant sections if needed\n","\n","When answering:\n","- Cite the specific parts of the document you're drawing from\n","- Maintain the original meaning without oversimplification\n","- If information appears to be missing or incomplete, acknowledge this\n","\n","ANSWER:\n","\"\"\"\n","\n","# Handling multiple languages\n","multilingual_template = \"\"\"\n","Answer the question based on documents in multiple languages.\n","\n","MULTILINGUAL SOURCES:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","- Identify information relevant to the question regardless of source language\n","- Synthesize information across language barriers\n","- Maintain the accuracy of facts and concepts during translation\n","- Note any cultural or linguistic nuances that affect interpretation\n","- Provide your answer in {response_language}\n","\n","ANSWER:\n","\"\"\"\n","\n","# Handling ambiguous queries\n","ambiguous_query_template = \"\"\"\n","The following question may have multiple interpretations. Answer based on the provided context.\n","\n","CONTEXT:\n","{context}\n","\n","AMBIGUOUS QUESTION:\n","{question}\n","\n","APPROACH:\n","1. Identify the possible interpretations of the question\n","2. For each plausible interpretation:\n","   a. Note the interpretation\n","   b. Provide the relevant answer based on the context\n","   c. Cite supporting information from the context\n","3. If the context doesn't address certain interpretations, acknowledge this\n","\n","ANSWER:\n","\"\"\"\n","\n","# Practical examples\n","print(\"Long Document Handling Template:\")\n","long_doc_question = \"What are the environmental impacts of renewable energy technologies?\"\n","print(long_document_template.format(context=\"[Long document extracts would be here...]\", question=long_doc_question))\n","print(f\"Tokens (without actual content): {count_tokens(long_document_template.format(context='', question=long_doc_question))}\")\n","\n","print(\"\\nMultilingual Template:\")\n","multilingual_question = \"What are the key differences between Western and Eastern approaches to sustainable development?\"\n","print(multilingual_template.format(context=\"[Multilingual sources would be here...]\", question=multilingual_question, response_language=\"English\"))\n","print(f\"Tokens (without actual content): {count_tokens(multilingual_template.format(context='', question=multilingual_question, response_language='English'))}\")\n","\n","print(\"\\nAmbiguous Query Template:\")\n","ambiguous_question = \"Why did the company change its policy?\"\n","print(ambiguous_query_template.format(context=\"[Context would be here...]\", question=ambiguous_question))\n","print(f\"Tokens (without actual content): {count_tokens(ambiguous_query_template.format(context='', question=ambiguous_question))}\")\n","\n","print_separator()"],"metadata":{"id":"W5yvIHmqgzQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 9: Putting It All Together - A Prompt Engineering Framework**"],"metadata":{"id":"vGmfajZRg8Cp"}},{"cell_type":"code","source":["print(\"Section 9: Putting It All Together - A Prompt Engineering Framework\")\n","\n","class RAGPromptEngineer:\n","    \"\"\"Framework for systematically designing and optimizing RAG prompts.\"\"\"\n","\n","    def __init__(self):\n","        self.templates = {\n","            \"standard\": \"\"\"\n","                Answer the question based on the context.\n","\n","                CONTEXT:\n","                {context}\n","\n","                QUESTION:\n","                {question}\n","\n","                ANSWER:\n","            \"\"\",\n","            \"anti_hallucination\": anti_hallucination_template,\n","            \"attribution\": attribution_template,\n","            \"reasoning\": reasoning_template,\n","            \"contradiction\": contradiction_template,\n","            \"comprehensive\": comprehensive_template\n","        }\n","\n","    def analyze_query(self, query):\n","        \"\"\"Analyze the query to determine appropriate template features.\"\"\"\n","        features = {\n","            \"needs_reasoning\": False,\n","            \"may_have_contradictions\": False,\n","            \"needs_attribution\": False,\n","            \"is_complex\": False,\n","            \"is_sensitive\": False\n","        }\n","\n","        # Simple heuristics for query analysis\n","        reasoning_terms = [\"why\", \"how\", \"explain\", \"reason\", \"analyze\"]\n","        if any(term in query.lower() for term in reasoning_terms):\n","            features[\"needs_reasoning\"] = True\n","            features[\"is_complex\"] = True\n","\n","        comparison_terms = [\"compare\", \"contrast\", \"difference\", \"versus\", \"vs\"]\n","        if any(term in query.lower() for term in comparison_terms):\n","            features[\"may_have_contradictions\"] = True\n","            features[\"needs_attribution\"] = True\n","\n","        sensitive_domains = [\"medical\", \"health\", \"legal\", \"financial\", \"political\"]\n","        if any(domain in query.lower() for domain in sensitive_domains):\n","            features[\"needs_attribution\"] = True\n","            features[\"is_sensitive\"] = True\n","\n","        # Check if the query is complex\n","        if len(query.split()) > 15 or \"and\" in query.lower() or \"or\" in query.lower():\n","            features[\"is_complex\"] = True\n","\n","        return features\n","\n","    def analyze_context(self, context):\n","        \"\"\"Analyze the context to determine appropriate template features.\"\"\"\n","        features = {\n","            \"has_multiple_sources\": False,\n","            \"has_potential_contradictions\": False,\n","            \"has_technical_content\": False,\n","            \"has_numerical_data\": False,\n","            \"has_temporal_aspects\": False\n","        }\n","\n","        # Check for multiple sources\n","        if context.count(\"[Document\") > 1:\n","            features[\"has_multiple_sources\"] = True\n","\n","        # Check for potential contradictions (very simplistic approach)\n","        if \"however\" in context.lower() or \"contrary\" in context.lower() or \"whereas\" in context.lower():\n","            features[\"has_potential_contradictions\"] = True\n","\n","        # Check for technical content\n","        technical_terms = [\"algorithm\", \"methodology\", \"procedure\", \"technical\", \"mechanism\"]\n","        if any(term in context.lower() for term in technical_terms):\n","            features[\"has_technical_content\"] = True\n","\n","        # Check for numerical data\n","        if re.search(r'\\d+(?:\\.\\d+)?%|\\$\\d+|\\d+\\s(?:million|billion)', context):\n","            features[\"has_numerical_data\"] = True\n","\n","        # Check for temporal aspects\n","        time_terms = [\"year\", \"month\", \"century\", \"period\", \"era\", \"decade\"]\n","        if any(term in context.lower() for term in time_terms):\n","            features[\"has_temporal_aspects\"] = True\n","\n","        return features\n","\n","    def select_template_components(self, query_features, context_features):\n","        \"\"\"Select appropriate template components based on features.\"\"\"\n","        components = {\n","            \"anti_hallucination\": False,\n","            \"source_attribution\": False,\n","            \"reasoning_process\": False,\n","            \"contradiction_handling\": False,\n","            \"evidence_grading\": False,\n","            \"response_structuring\": False\n","        }\n","\n","        # Anti-hallucination for most cases\n","        components[\"anti_hallucination\"] = True\n","\n","        # Source attribution\n","        if query_features[\"needs_attribution\"] or context_features[\"has_multiple_sources\"]:\n","            components[\"source_attribution\"] = True\n","\n","        # Reasoning process\n","        if query_features[\"needs_reasoning\"] or query_features[\"is_complex\"]:\n","            components[\"reasoning_process\"] = True\n","\n","        # Contradiction handling\n","        if query_features[\"may_have_contradictions\"] or context_features[\"has_potential_contradictions\"]:\n","            components[\"contradiction_handling\"] = True\n","\n","        # Evidence grading\n","        if context_features[\"has_multiple_sources\"] or query_features[\"is_sensitive\"]:\n","            components[\"evidence_grading\"] = True\n","\n","        # Response structuring\n","        if query_features[\"is_complex\"] or context_features[\"has_technical_content\"] or context_features[\"has_numerical_data\"]:\n","            components[\"response_structuring\"] = True\n","\n","        return components\n","\n","    def build_custom_prompt(self, query, context):\n","        \"\"\"Build a custom prompt template based on query and context analysis.\"\"\"\n","        query_features = self.analyze_query(query)\n","        context_features = self.analyze_context(context)\n","        components = self.select_template_components(query_features, context_features)\n","\n","        # Start with base template\n","        prompt = \"You are an assistant answering questions based on provided information.\\n\\n\"\n","\n","        # Add context section\n","        prompt += \"SOURCES:\\n{context}\\n\\n\"\n","\n","        # Add question section\n","        prompt += \"QUESTION:\\n{question}\\n\\n\"\n","\n","        # Add instructions based on selected components\n","        prompt += \"INSTRUCTIONS:\\n\"\n","\n","        if components[\"anti_hallucination\"]:\n","            prompt += \"- Answer based EXCLUSIVELY on the provided sources. If the sources don't contain sufficient information, say so clearly.\\n\"\n","\n","        if components[\"source_attribution\"]:\n","            prompt += \"- Cite sources for each piece of information using [Source X] notation.\\n\"\n","\n","        if components[\"reasoning_process\"]:\n","            prompt += \"- Explain your reasoning step by step before providing the final answer.\\n\"\n","\n","        if components[\"contradiction_handling\"]:\n","            prompt += \"- If you find contradictory information in the sources, acknowledge the contradictions and present multiple perspectives.\\n\"\n","\n","        if components[\"evidence_grading\"]:\n","            prompt += \"- Indicate the strength of evidence (Strong, Moderate, Limited) for key claims.\\n\"\n","\n","        if components[\"response_structuring\"]:\n","            prompt += \"- Structure your response with: (1) Direct answer, (2) Supporting evidence, (3) Nuance or limitations.\\n\"\n","\n","        # Add answer section\n","        prompt += \"\\nANSWER:\\n\"\n","\n","        return prompt\n","\n","    def generate_prompt(self, query, context, template_type=\"auto\"):\n","        \"\"\"Generate a formatted prompt based on query, context, and template type.\"\"\"\n","        if template_type == \"auto\":\n","            template = self.build_custom_prompt(query, context)\n","        else:\n","            template = self.templates.get(template_type, self.templates[\"standard\"])\n","\n","        formatted_prompt = template.format(context=context, question=query)\n","        tokens = count_tokens(formatted_prompt)\n","\n","        return {\n","            \"prompt\": formatted_prompt,\n","            \"tokens\": tokens,\n","            \"template_type\": template_type if template_type != \"auto\" else \"custom\"\n","        }\n","\n","# Test the RAG Prompt Engineer\n","prompt_engineer = RAGPromptEngineer()\n","\n","# Financial query example\n","financial_query = \"What were Apple's Q1 2023 financial results and how did they compare to analyst expectations?\"\n","result_financial = prompt_engineer.generate_prompt(financial_query, financial_context)\n","\n","print(\"Automatically Generated Prompt for Financial Query:\")\n","print(result_financial[\"prompt\"])\n","print(f\"Tokens: {result_financial['tokens']}\")\n","print(f\"Template Type: {result_financial['template_type']}\")\n","\n","# Medical query example\n","medical_query = \"What are the evidence-based benefits and risks of statins?\"\n","result_medical = prompt_engineer.generate_prompt(medical_query, medical_context)\n","\n","print(\"\\nAutomatically Generated Prompt for Medical Query:\")\n","print(result_medical[\"prompt\"])\n","print(f\"Tokens: {result_medical['tokens']}\")\n","print(f\"Template Type: {result_medical['template_type']}\")\n","\n","# Try with specific template\n","result_contradiction = prompt_engineer.generate_prompt(\n","    \"Does coffee increase the risk of heart disease?\",\n","    contradiction_context,\n","    template_type=\"contradiction\"\n",")\n","\n","print(\"\\nUsing Specific Contradiction Template:\")\n","print(result_contradiction[\"prompt\"])\n","print(f\"Tokens: {result_contradiction['tokens']}\")\n","print(f\"Template Type: {result_contradiction['template_type']}\")\n","\n","print_separator()\n","\n"],"metadata":{"id":"Ak2Ekjo_g8J-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 10: Testing with LLMs (If API Key Available)**"],"metadata":{"id":"-n_mTtoHhZUd"}},{"cell_type":"code","source":["print(\"Section 10: Testing with LLMs (If API Key Available)\")\n","\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        llm = ChatOpenAI(temperature=0)\n","\n","        print(\"Testing various templates with OpenAI API:\")\n","\n","        test_question = \"Does coffee increase the risk of heart disease?\"\n","\n","        # Test with standard template\n","        print(\"\\nStandard Template Response:\")\n","        standard_result = llm.invoke(\n","            templates_to_evaluate[\"Standard\"].format(\n","                context=contradiction_context,\n","                question=test_question\n","            )\n","        )\n","        print(standard_result.content)\n","\n","        # Test with anti-hallucination template\n","        print(\"\\nAnti-Hallucination Template Response:\")\n","        anti_hallucination_result = llm.invoke(\n","            templates_to_evaluate[\"Anti-Hallucination\"].format(\n","                context=contradiction_context,\n","                question=test_question\n","            )\n","        )\n","        print(anti_hallucination_result.content)\n","\n","        # Test with comprehensive template\n","        print(\"\\nComprehensive Template Response:\")\n","        comprehensive_result = llm.invoke(\n","            templates_to_evaluate[\"Comprehensive\"].format(\n","                context=contradiction_context,\n","                question=test_question\n","            )\n","        )\n","        print(comprehensive_result.content)\n","\n","        # Test with auto-generated template\n","        print(\"\\nAuto-Generated Template Response:\")\n","        auto_template = prompt_engineer.generate_prompt(\n","            test_question,\n","            contradiction_context\n","        )[\"prompt\"]\n","\n","        auto_result = llm.invoke(auto_template)\n","        print(auto_result.content)\n","\n","    except Exception as e:\n","        print(f\"Error testing with LLM: {e}\")\n","else:\n","    print(\"OpenAI API key not set - skipping LLM testing\")\n","    print(\"\\nTo test with an LLM:\")\n","    print(\"1. Get an API key from OpenAI\")\n","    print(\"2. Set it as an environment variable: export OPENAI_API_KEY='your-api-key'\")\n","    print(\"3. Or add it to this notebook: os.environ['OPENAI_API_KEY'] = 'your-api-key'\")\n","    print(\"4. Re-run this section to see how different prompting techniques affect the responses\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")\n","\n","# Key Takeaways:\n","# 1. Effective prompt engineering can significantly reduce hallucinations in RAG systems\n","# 2. Different types of queries and contexts benefit from specialized prompt templates\n","# 3. Combining multiple techniques creates robust prompts for complex scenarios\n","# 4. Systematic analysis of queries and contexts can guide template selection\n","# 5. Prompt engineering should be evaluated based on output quality, not just intuition"],"metadata":{"id":"NDgk2t4OhZcx"},"execution_count":null,"outputs":[]}]}