{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTDgiCXByqnAjloGBG9Q6G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"rr3IEMvX2yaY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFJlhYAV2jBj"},"outputs":[],"source":["!pip install langchain langchain-openai tiktoken pandas matplotlib seaborn\n","\n","import os\n","import re\n","import json\n","import time\n","from datetime import datetime\n","from typing import List, Dict, Any, Optional, Union, Tuple\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n","\n","import tiktoken\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from langchain.prompts import PromptTemplate\n","from langchain_core.documents import Document\n","from langchain_openai import OpenAI, ChatOpenAI"]},{"cell_type":"markdown","source":["**Basic Utility Functions**"],"metadata":{"id":"9EIHWz693KoF"}},{"cell_type":"code","source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n","    \"\"\"Count the number of tokens in a text string.\"\"\"\n","    encoder = tiktoken.encoding_for_model(model)\n","    return len(encoder.encode(text))\n","\n","def print_separator():\n","    \"\"\"Print a visual separator.\"\"\"\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Create sample documents for testing\n","sample_docs = [\n","    Document(page_content=\"Renewable energy sources like solar and wind power have seen significant cost decreases over the past decade. Solar photovoltaic costs dropped by 85% between 2010 and 2020, while onshore wind costs fell by 56% during the same period.\",\n","             metadata={\"source\": \"energy_report_2021\", \"page\": 42}),\n","    Document(page_content=\"Energy storage remains a challenge for renewable energy integration. Lithium-ion battery costs have decreased by 89% from 2010 to 2020, but grid-scale storage deployment still lags behind renewable energy installation. Pumped hydro storage accounts for over 90% of global energy storage capacity.\",\n","             metadata={\"source\": \"grid_storage_analysis\", \"page\": 128}),\n","    Document(page_content=\"The global transition to renewable energy requires significant infrastructure investment. Estimates suggest $4.4 trillion in annual investment is needed by 2030 to achieve net-zero emissions by 2050. This includes grid modernization, energy storage, and renewable generation capacity.\",\n","             metadata={\"source\": \"climate_finance_report\", \"page\": 75}),\n","    Document(page_content=\"Community solar projects provide renewable energy access to those unable to install their own systems. These shared solar facilities are now available in 39 states and serve over 700,000 households. The average subscriber saves approximately 10% on their electricity costs.\",\n","             metadata={\"source\": \"community_energy_initiative\", \"page\": 12}),\n","]\n","\n","# Create evaluation set with questions and ground truth answers\n","eval_set = [\n","    {\n","        \"question\": \"How much have solar photovoltaic costs decreased between 2010 and 2020?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"Solar photovoltaic costs dropped by 85% between 2010 and 2020.\",\n","        \"key_elements\": [\"85%\", \"between 2010 and 2020\", \"solar photovoltaic\"],\n","        \"expected_source\": 0  # Index of the document containing the answer\n","    },\n","    {\n","        \"question\": \"What percentage of global energy storage capacity comes from pumped hydro storage?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"Pumped hydro storage accounts for over 90% of global energy storage capacity.\",\n","        \"key_elements\": [\"over 90%\", \"pumped hydro\", \"global energy storage capacity\"],\n","        \"expected_source\": 1\n","    },\n","    {\n","        \"question\": \"How much annual investment is needed by 2030 to achieve net-zero emissions by 2050?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"$4.4 trillion in annual investment is needed by 2030 to achieve net-zero emissions by 2050.\",\n","        \"key_elements\": [\"$4.4 trillion\", \"annual investment\", \"by 2030\", \"net-zero emissions by 2050\"],\n","        \"expected_source\": 2\n","    },\n","    {\n","        \"question\": \"What is the approximate percentage that community solar subscribers save on electricity costs?\",\n","        \"context\": \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)]),\n","        \"ground_truth\": \"The average subscriber saves approximately 10% on their electricity costs.\",\n","        \"key_elements\": [\"10%\", \"average subscriber\", \"electricity costs\"],\n","        \"expected_source\": 3\n","    }\n","]\n"],"metadata":{"id":"QZ-2lrVO3KyN","executionInfo":{"status":"ok","timestamp":1741271405310,"user_tz":-330,"elapsed":18,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["**Section 1: Template Evaluation Framework**"],"metadata":{"id":"LyDE1Ni-3Rql"}},{"cell_type":"code","source":["print(\"Section 1: Template Evaluation Framework\")\n","\n","class TemplateEvaluator:\n","    \"\"\"Framework for evaluating prompt templates against best practices.\"\"\"\n","\n","    def __init__(self):\n","        # Define evaluation criteria\n","        self.criteria = {\n","            \"clarity\": self._check_clarity,\n","            \"structure\": self._check_structure,\n","            \"constraints\": self._check_constraints,\n","            \"citation\": self._check_citation,\n","            \"token_efficiency\": self._check_token_efficiency,\n","            \"task_guidance\": self._check_task_guidance,\n","            \"error_handling\": self._check_error_handling,\n","            \"identity\": self._check_identity\n","        }\n","\n","    def evaluate_template(self, template: str) -> Dict[str, Any]:\n","        \"\"\"Evaluate a template against all criteria.\"\"\"\n","        results = {}\n","        total_score = 0\n","        max_score = 0\n","\n","        for criterion, check_function in self.criteria.items():\n","            score, feedback = check_function(template)\n","            results[criterion] = {\n","                \"score\": score,\n","                \"feedback\": feedback\n","            }\n","            total_score += score\n","            max_score += 5  # Assuming all criteria use a 0-5 scale\n","\n","        # Calculate overall score as percentage\n","        overall_score = (total_score / max_score) * 100 if max_score > 0 else 0\n","\n","        return {\n","            \"criteria_scores\": results,\n","            \"overall_score\": overall_score,\n","            \"token_count\": count_tokens(template)\n","        }\n","\n","    def _check_clarity(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template has clear, unambiguous instructions.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for clear section delineation\n","        if re.search(r\"CONTEXT|QUESTION|ANSWER\", template):\n","            score += 2\n","        else:\n","            feedback.append(\"Missing clear section headers (CONTEXT, QUESTION, ANSWER, etc.)\")\n","\n","        # Check for concise language\n","        avg_sentence_length = len(template) / max(1, len(re.findall(r'[.!?]', template)))\n","        if avg_sentence_length > 25:\n","            feedback.append(\"Sentences are overly long, averaging {:.1f} characters\".format(avg_sentence_length))\n","        else:\n","            score += 1\n","\n","        # Check for imperative verbs\n","        imperative_verbs = [\"use\", \"answer\", \"provide\", \"cite\", \"include\", \"consider\", \"avoid\"]\n","        if any(verb in template.lower() for verb in imperative_verbs):\n","            score += 1\n","        else:\n","            feedback.append(\"Missing clear directive verbs (use, answer, provide, etc.)\")\n","\n","        # Check for ambiguous language\n","        ambiguous_terms = [\"maybe\", \"might\", \"possibly\", \"potentially\", \"could\", \"try to\"]\n","        if any(term in template.lower() for term in ambiguous_terms):\n","            feedback.append(\"Contains ambiguous terms: \" + \", \".join(term for term in ambiguous_terms if term in template.lower()))\n","        else:\n","            score += 1\n","\n","        if not feedback:\n","            feedback = [\"Template has clear, unambiguous instructions\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_structure(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template has a clear, logical structure.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for clear sections\n","        sections = [\"context\", \"question\", \"answer\", \"instruction\"]\n","        found_sections = [s for s in sections if s.lower() in template.lower()]\n","\n","        if len(found_sections) >= 3:\n","            score += 2\n","        elif len(found_sections) >= 2:\n","            score += 1\n","            feedback.append(\"Missing some key sections\")\n","        else:\n","            feedback.append(\"No clear structural elements found\")\n","\n","        # Check for visual separation\n","        if re.search(r'[-=_]{3,}|\\n{2,}', template):\n","            score += 1\n","        else:\n","            feedback.append(\"Lacks visual separation between sections\")\n","\n","        # Check for placeholder formatting\n","        if re.search(r'\\{[a-z_]+\\}', template):\n","            score += 1\n","        else:\n","            feedback.append(\"No properly formatted placeholders found\")\n","\n","        # Check for logical ordering\n","        logical_order = (\n","            template.lower().find(\"context\") < template.lower().find(\"question\") < template.lower().find(\"answer\")\n","            if all(s in template.lower() for s in [\"context\", \"question\", \"answer\"])\n","            else False\n","        )\n","\n","        if logical_order:\n","            score += 1\n","        else:\n","            feedback.append(\"Sections may not be in logical order\")\n","\n","        if not feedback:\n","            feedback = [\"Template has clear, logical structure\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_constraints(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template includes appropriate constraints.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for information source constraints\n","        source_constraints = [\"only use\", \"based on\", \"from the context\", \"in the provided\", \"don't use external\"]\n","        found_constraints = [c for c in source_constraints if c in template.lower()]\n","\n","        if found_constraints:\n","            score += 2\n","        else:\n","            feedback.append(\"Missing explicit constraints on information sources\")\n","\n","        # Check for hallucination prevention\n","        hallucination_prevention = [\"don't make up\", \"don't guess\", \"don't use external\", \"don't assume\", \"insufficient info\"]\n","        found_prevention = [p for p in hallucination_prevention if p in template.lower()]\n","\n","        if found_prevention:\n","            score += 2\n","        else:\n","            feedback.append(\"Missing explicit hallucination prevention\")\n","\n","        # Check for explicit scope limits\n","        scope_limits = [\"only\", \"specifically\", \"limit\", \"focus on\", \"restrict\"]\n","        found_limits = [l for l in scope_limits if l in template.lower()]\n","\n","        if found_limits:\n","            score += 1\n","        else:\n","            feedback.append(\"Missing explicit scope limitations\")\n","\n","        if not feedback:\n","            feedback = [\"Template includes appropriate constraints\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_citation(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template includes citation requirements.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for citation terms\n","        citation_terms = [\"cite\", \"reference\", \"source\", \"document\", \"attribution\"]\n","        found_terms = [t for t in citation_terms if t in template.lower()]\n","\n","        if found_terms:\n","            score += 2\n","        else:\n","            feedback.append(\"Missing citation requirements\")\n","\n","        # Check for citation format specification\n","        citation_formats = [\"[doc\", \"document\", \"source\", \"[\", \"]\"]\n","        found_formats = [f for f in citation_formats if f in template.lower()]\n","\n","        if found_formats:\n","            score += 2\n","        else:\n","            feedback.append(\"Missing citation format specification\")\n","\n","        # Check for contextual citation guidance\n","        if \"when\" in template.lower() and any(t in template.lower() for t in citation_terms):\n","            score += 1\n","        else:\n","            feedback.append(\"Missing contextual citation guidance\")\n","\n","        if not feedback:\n","            feedback = [\"Template includes clear citation requirements\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_token_efficiency(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template is token-efficient.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check total token count\n","        token_count = count_tokens(template)\n","\n","        if token_count < 100:\n","            score += 2\n","            feedback.append(f\"Excellent token efficiency ({token_count} tokens)\")\n","        elif token_count < 200:\n","            score += 1\n","            feedback.append(f\"Good token efficiency ({token_count} tokens)\")\n","        else:\n","            feedback.append(f\"Template is token-heavy ({token_count} tokens)\")\n","\n","        # Check for redundancy\n","        sentences = re.split(r'[.!?]', template)\n","        unique_sentences = set(s.strip().lower() for s in sentences if s.strip())\n","        redundancy_ratio = 1 - (len(unique_sentences) / max(1, len(sentences)))\n","\n","        if redundancy_ratio < 0.1:\n","            score += 2\n","        elif redundancy_ratio < 0.2:\n","            score += 1\n","            feedback.append(f\"Some redundancy detected ({redundancy_ratio:.2f} redundancy ratio)\")\n","        else:\n","            feedback.append(f\"High redundancy detected ({redundancy_ratio:.2f} redundancy ratio)\")\n","\n","        # Check for verbose phrases\n","        verbose_phrases = [\"in order to\", \"for the purpose of\", \"in the event that\", \"at this point in time\",\n","                          \"due to the fact that\", \"with regard to\", \"it is important to note that\"]\n","        found_phrases = [p for p in verbose_phrases if p in template.lower()]\n","\n","        if not found_phrases:\n","            score += 1\n","        else:\n","            feedback.append(\"Contains verbose phrases: \" + \", \".join(found_phrases))\n","\n","        if not feedback:\n","            feedback = [\"Template is token-efficient\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_task_guidance(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template includes task-specific guidance.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for specific task indicators\n","        task_indicators = {\n","            \"question answering\": [\"answer the question\", \"respond to the question\"],\n","            \"summarization\": [\"summarize\", \"summary\", \"key points\"],\n","            \"comparison\": [\"compare\", \"contrast\", \"differences\", \"similarities\"],\n","            \"analysis\": [\"analyze\", \"analysis\", \"evaluate\", \"assessment\"]\n","        }\n","\n","        found_tasks = []\n","        for task, indicators in task_indicators.items():\n","            if any(i in template.lower() for i in indicators):\n","                found_tasks.append(task)\n","\n","        if found_tasks:\n","            score += 2\n","            feedback.append(f\"Template specifies task type: {', '.join(found_tasks)}\")\n","        else:\n","            feedback.append(\"No specific task type identified\")\n","\n","        # Check for output format guidance\n","        format_guidance = [\"format\", \"structure\", \"organize\", \"bullet\", \"paragraph\", \"list\"]\n","        found_format = [f for f in format_guidance if f in template.lower()]\n","\n","        if found_format:\n","            score += 2\n","            feedback.append(f\"Includes output format guidance\")\n","        else:\n","            feedback.append(\"Missing output format guidance\")\n","\n","        # Check for quality criteria\n","        quality_criteria = [\"concise\", \"comprehensive\", \"accurate\", \"balanced\", \"objective\"]\n","        found_criteria = [c for c in quality_criteria if c in template.lower()]\n","\n","        if found_criteria:\n","            score += 1\n","            feedback.append(f\"Specifies quality criteria: {', '.join(found_criteria)}\")\n","        else:\n","            feedback.append(\"Missing quality criteria\")\n","\n","        if len(feedback) == 1 and \"Template specifies\" in feedback[0]:\n","            feedback = [\"Template includes comprehensive task-specific guidance\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_error_handling(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template includes guidance for handling errors or edge cases.\"\"\"\n","        score = 0\n","        feedback = []\n","\n","        # Check for insufficient information handling\n","        insufficient_info = [\"insufficient\", \"not enough\", \"doesn't contain\", \"unable to answer\", \"missing\"]\n","        found_insufficient = [i for i in insufficient_info if i in template.lower()]\n","\n","        if found_insufficient:\n","            score += 2\n","            feedback.append(\"Includes guidance for insufficient information\")\n","        else:\n","            feedback.append(\"Missing guidance for insufficient information\")\n","\n","        # Check for conflicting information handling\n","        conflicting_info = [\"conflict\", \"contradict\", \"disagree\", \"inconsistent\", \"different perspective\"]\n","        found_conflicting = [c for c in conflicting_info if c in template.lower()]\n","\n","        if found_conflicting:\n","            score += 2\n","            feedback.append(\"Includes guidance for conflicting information\")\n","        else:\n","            feedback.append(\"Missing guidance for conflicting information\")\n","\n","        # Check for uncertainty communication\n","        uncertainty = [\"uncertain\", \"confidence\", \"likely\", \"probability\", \"unclear\"]\n","        found_uncertainty = [u for u in uncertainty if u in template.lower()]\n","\n","        if found_uncertainty:\n","            score += 1\n","            feedback.append(\"Includes guidance for communicating uncertainty\")\n","        else:\n","            feedback.append(\"Missing guidance for communicating uncertainty\")\n","\n","        if feedback[0].startswith(\"Includes\"):\n","            feedback = [\"Template provides comprehensive error handling guidance\"]\n","\n","        return min(score, 5), \"; \".join(feedback)\n","\n","    def _check_identity(self, template: str) -> Tuple[int, str]:\n","        \"\"\"Check if the template avoids identity confusion.\"\"\"\n","        score = 5  # Start with perfect score and deduct\n","        feedback = []\n","\n","        # Check for problematic identity terms\n","        identity_terms = [\"you are\", \"as an expert\", \"as a professional\", \"you know\", \"your knowledge\", \"your expertise\"]\n","        found_terms = [t for t in identity_terms if t in template.lower()]\n","\n","        if found_terms:\n","            score -= 3\n","            feedback.append(f\"Contains potentially confusing identity framing: {', '.join(found_terms)}\")\n","\n","        # Check for knowledge claims\n","        knowledge_claims = [\"you have knowledge\", \"you are familiar with\", \"you understand\", \"you have expertise\"]\n","        found_claims = [c for c in knowledge_claims if c in template.lower()]\n","\n","        if found_claims:\n","            score -= 2\n","            feedback.append(f\"Makes knowledge claims about the model: {', '.join(found_claims)}\")\n","\n","        if not feedback:\n","            feedback = [\"Template avoids identity confusion\"]\n","\n","        return max(score, 0), \"; \".join(feedback)\n","\n","# Define templates for evaluation\n","templates_to_evaluate = {\n","    \"basic\": \"\"\"\n","    Answer the question based on the context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"detailed\": \"\"\"\n","    You are an assistant for question-answering tasks. Use the following pieces of context to answer the question at the end.\n","    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","    Use three sentences maximum and keep the answer concise.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    ANSWER:\n","    \"\"\",\n","\n","    \"optimized\": \"\"\"\n","    Answer based ONLY on the provided context. If the information isn't available, say \"I don't have enough information.\"\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    Cite sources as [Doc X]. Be concise.\n","    \"\"\",\n","\n","    \"best_practice\": \"\"\"\n","    Answer the question using ONLY the provided context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    INSTRUCTIONS:\n","    - If the answer is in the context, provide it clearly and concisely\n","    - Cite specific documents using [Document X] notation\n","    - If the answer isn't in the context, say \"I don't have enough information\"\n","    - If information is conflicting, acknowledge the contradiction\n","\n","    ANSWER:\n","    \"\"\"\n","}\n","\n","# Create evaluator and evaluate templates\n","evaluator = TemplateEvaluator()\n","evaluation_results = {}\n","\n","for name, template in templates_to_evaluate.items():\n","    evaluation_results[name] = evaluator.evaluate_template(template)\n","    print(f\"Evaluating template: {name}\")\n","    print(f\"Overall score: {evaluation_results[name]['overall_score']:.1f}%\")\n","    print(f\"Token count: {evaluation_results[name]['token_count']}\")\n","    print(\"Criteria scores:\")\n","    for criterion, result in evaluation_results[name]['criteria_scores'].items():\n","        print(f\"  {criterion}: {result['score']}/5 - {result['feedback']}\")\n","    print()\n","\n","# Visualize evaluation results\n","try:\n","    criteria = list(evaluation_results[next(iter(evaluation_results))]['criteria_scores'].keys())\n","    template_names = list(evaluation_results.keys())\n","\n","    scores_data = []\n","    for template_name in template_names:\n","        template_scores = []\n","        for criterion in criteria:\n","            template_scores.append(evaluation_results[template_name]['criteria_scores'][criterion]['score'])\n","        scores_data.append(template_scores)\n","\n","    # Create a radar chart\n","    angles = np.linspace(0, 2*np.pi, len(criteria), endpoint=False).tolist()\n","    angles += angles[:1]  # Close the polygon\n","\n","    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n","\n","    for i, template_name in enumerate(template_names):\n","        values = scores_data[i].copy()\n","        values += values[:1]  # Close the polygon\n","        ax.plot(angles, values, linewidth=2, label=template_name)\n","        ax.fill(angles, values, alpha=0.1)\n","\n","    # Add labels\n","    ax.set_xticks(angles[:-1])\n","    ax.set_xticklabels(criteria)\n","    ax.set_yticks([1, 2, 3, 4, 5])\n","    ax.set_title(\"Template Evaluation Scores\", fontsize=14, pad=20)\n","    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Create a bar chart for overall scores\n","    overall_scores = [evaluation_results[name]['overall_score'] for name in template_names]\n","\n","    plt.figure(figsize=(10, 6))\n","    bars = plt.bar(template_names, overall_scores)\n","\n","    # Add score labels on top of bars\n","    for bar, score in zip(bars, overall_scores):\n","        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n","                 f\"{score:.1f}%\", ha='center', fontsize=12)\n","\n","    plt.title(\"Overall Template Evaluation Scores\", fontsize=14)\n","    plt.ylabel(\"Score (%)\")\n","    plt.ylim(0, 105)  # Leave room for labels\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","    plt.tight_layout()\n","    plt.show()\n","except Exception as e:\n","    print(f\"Error creating visualization: {e}\")\n","    print(\"To view the visualization, run this notebook in an environment that supports matplotlib.\")\n","\n","print_separator()"],"metadata":{"id":"LOoeQOPz3R0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 2: Before and After Template Optimization**"],"metadata":{"id":"sLt-CyZI3eEV"}},{"cell_type":"code","source":["print(\"Section 2: Before and After Template Optimization\")\n","\n","before_template = \"\"\"\n","You are an expert assistant with deep knowledge about many topics. Your task is to utilize your extensive knowledge and the provided context to answer the user's question in a helpful manner. Try to be as informative as possible while keeping your answer accurate and relevant to what the user is asking about. If possible, include additional interesting facts that might be relevant to the question. If you're unsure about something, you can make an educated guess based on your knowledge.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","ANSWER:\n","\"\"\"\n","\n","after_template = \"\"\"\n","Answer the question using ONLY information from the context below.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","- If the information is in the context, provide a clear and concise answer\n","- Cite your sources using [Document X] notation\n","- If the answer isn't in the context, state \"I don't have enough information\"\n","- Do not introduce external knowledge\n","\n","ANSWER:\n","\"\"\"\n","\n","print(\"BEFORE OPTIMIZATION:\")\n","print(before_template)\n","print(f\"Token count: {count_tokens(before_template)}\")\n","\n","before_eval = evaluator.evaluate_template(before_template)\n","print(f\"Overall score: {before_eval['overall_score']:.1f}%\")\n","print(\"Key issues:\")\n","problem_criteria = [c for c in before_eval['criteria_scores'] if before_eval['criteria_scores'][c]['score'] <= 2]\n","for criterion in problem_criteria:\n","    print(f\"- {criterion}: {before_eval['criteria_scores'][criterion]['feedback']}\")\n","\n","print(\"\\nAFTER OPTIMIZATION:\")\n","print(after_template)\n","print(f\"Token count: {count_tokens(after_template)}\")\n","\n","after_eval = evaluator.evaluate_template(after_template)\n","print(f\"Overall score: {after_eval['overall_score']:.1f}%\")\n","print(\"Improvements:\")\n","for criterion in before_eval['criteria_scores']:\n","    before_score = before_eval['criteria_scores'][criterion]['score']\n","    after_score = after_eval['criteria_scores'][criterion]['score']\n","    if after_score > before_score:\n","        print(f\"- {criterion}: {before_score} â†’ {after_score} ({after_eval['criteria_scores'][criterion]['feedback']})\")\n","\n","print(\"\\nOptimization Results:\")\n","token_reduction = count_tokens(before_template) - count_tokens(after_template)\n","score_improvement = after_eval['overall_score'] - before_eval['overall_score']\n","print(f\"- Token reduction: {token_reduction} tokens ({token_reduction/count_tokens(before_template):.1%})\")\n","print(f\"- Score improvement: {score_improvement:.1f} percentage points\")\n","\n","print_separator()"],"metadata":{"id":"y7yLxhZ63eN6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 3: Template Management System**"],"metadata":{"id":"DMdTiQCZ3m_3"}},{"cell_type":"code","source":["print(\"Section 3: Template Management System\")\n","\n","class TemplateManager:\n","    \"\"\"System for managing, versioning, and optimizing prompt templates.\"\"\"\n","\n","    def __init__(self):\n","        self.templates = {}\n","        self.version_history = {}\n","        self.evaluator = TemplateEvaluator()\n","        self.default_template = None\n","\n","    def add_template(self, name, template, description=None, version=\"1.0.0\", set_as_default=False):\n","        \"\"\"Add a new template or a new version of an existing template.\"\"\"\n","        # Evaluate the template\n","        evaluation = self.evaluator.evaluate_template(template)\n","\n","        # Create template metadata\n","        metadata = {\n","            \"description\": description or f\"Template: {name}\",\n","            \"version\": version,\n","            \"created_at\": datetime.now().isoformat(),\n","            \"token_count\": count_tokens(template),\n","            \"evaluation\": evaluation,\n","            \"usage_count\": 0\n","        }\n","\n","        # Add to templates dictionary\n","        self.templates[name] = {\n","            \"template\": template,\n","            \"metadata\": metadata\n","        }\n","\n","        # Add to version history\n","        if name not in self.version_history:\n","            self.version_history[name] = {}\n","\n","        self.version_history[name][version] = {\n","            \"template\": template,\n","            \"metadata\": metadata\n","        }\n","\n","        # Set as default if requested or if it's the first template\n","        if set_as_default or self.default_template is None:\n","            self.default_template = name\n","\n","        return metadata\n","\n","    def get_template(self, name=None, version=None):\n","        \"\"\"Get a template by name and optional version.\"\"\"\n","        # Use default if no name specified\n","        if name is None:\n","            name = self.default_template\n","            if name is None:\n","                raise ValueError(\"No template name specified and no default template set\")\n","\n","        # Check if template exists\n","        if name not in self.templates:\n","            raise ValueError(f\"Template '{name}' not found\")\n","\n","        # Return specific version if requested\n","        if version is not None:\n","            if version not in self.version_history[name]:\n","                raise ValueError(f\"Version '{version}' not found for template '{name}'\")\n","\n","            return self.version_history[name][version][\"template\"]\n","\n","        # Return current version\n","        return self.templates[name][\"template\"]\n","\n","    def list_templates(self):\n","        \"\"\"List all available templates with metadata.\"\"\"\n","        return {name: template[\"metadata\"] for name, template in self.templates.items()}\n","\n","    def list_versions(self, name):\n","        \"\"\"List all versions of a specific template.\"\"\"\n","        if name not in self.version_history:\n","            raise ValueError(f\"Template '{name}' not found\")\n","\n","        return {version: info[\"metadata\"] for version, info in self.version_history[name].items()}\n","\n","    def format_prompt(self, template_name=None, version=None, **kwargs):\n","        \"\"\"Format a prompt using the specified template and variables.\"\"\"\n","        template = self.get_template(template_name, version)\n","\n","        # Update usage count\n","        if template_name is None:\n","            template_name = self.default_template\n","\n","        self.templates[template_name][\"metadata\"][\"usage_count\"] += 1\n","\n","        # Format and return the prompt\n","        return template.format(**kwargs)\n","\n","    def optimize_template(self, name, target_criteria=None):\n","        \"\"\"Generate an optimized version of a template based on evaluation results.\"\"\"\n","        if name not in self.templates:\n","            raise ValueError(f\"Template '{name}' not found\")\n","\n","        original_template = self.templates[name][\"template\"]\n","        original_eval = self.evaluator.evaluate_template(original_template)\n","\n","        # Focus on criteria with the lowest scores if not specified\n","        if target_criteria is None:\n","            scores = original_eval[\"criteria_scores\"]\n","            target_criteria = sorted(scores.keys(), key=lambda k: scores[k][\"score\"])[:3]\n","\n","        print(f\"Optimizing template '{name}' targeting: {', '.join(target_criteria)}\")\n","\n","        # Apply optimization strategies based on target criteria\n","        optimized_template = original_template\n","\n","        if \"token_efficiency\" in target_criteria:\n","            optimized_template = self._optimize_token_efficiency(optimized_template)\n","\n","        if \"constraints\" in target_criteria:\n","            optimized_template = self._add_constraints(optimized_template)\n","\n","        if \"citation\" in target_criteria:\n","            optimized_template = self._add_citation_requirements(optimized_template)\n","\n","        if \"structure\" in target_criteria:\n","            optimized_template = self._improve_structure(optimized_template)\n","\n","        if \"clarity\" in target_criteria:\n","            optimized_template = self._improve_clarity(optimized_template)\n","\n","        if \"identity\" in target_criteria:\n","            optimized_template = self._fix_identity_issues(optimized_template)\n","\n","        if \"error_handling\" in target_criteria:\n","            optimized_template = self._add_error_handling(optimized_template)\n","\n","        if \"task_guidance\" in target_criteria:\n","            optimized_template = self._add_task_guidance(optimized_template)\n","\n","        # Evaluate the optimized template\n","        optimized_eval = self.evaluator.evaluate_template(optimized_template)\n","\n","        # Create a new version\n","        current_version = self.templates[name][\"metadata\"][\"version\"]\n","        version_parts = current_version.split('.')\n","        new_version = f\"{version_parts[0]}.{version_parts[1]}.{int(version_parts[2]) + 1}\"\n","\n","        # Add the optimized template\n","        optimization_description = f\"Optimized for: {', '.join(target_criteria)}\"\n","        self.add_template(name, optimized_template, description=optimization_description, version=new_version)\n","\n","        return {\n","            \"original\": original_template,\n","            \"optimized\": optimized_template,\n","            \"original_eval\": original_eval,\n","            \"optimized_eval\": optimized_eval,\n","            \"new_version\": new_version\n","        }\n","\n","    def _optimize_token_efficiency(self, template):\n","        \"\"\"Optimize template for token efficiency.\"\"\"\n","        # Replace verbose phrases\n","        verbose_replacements = {\n","            \"in order to\": \"to\",\n","            \"for the purpose of\": \"for\",\n","            \"in the event that\": \"if\",\n","            \"at this point in time\": \"now\",\n","            \"due to the fact that\": \"because\",\n","            \"with regard to\": \"regarding\",\n","            \"it is important to note that\": \"\"\n","        }\n","\n","        optimized = template\n","        for verbose, concise in verbose_replacements.items():\n","            optimized = optimized.replace(verbose, concise)\n","\n","        # Remove redundant instructions\n","        if \"based on the context\" in optimized.lower() and \"using the provided context\" in optimized.lower():\n","            optimized = optimized.replace(\"using the provided context\", \"\")\n","\n","        # Consolidate instructions\n","        lines = optimized.split('\\n')\n","        consolidated_lines = []\n","\n","        for i, line in enumerate(lines):\n","            if i > 0 and line.strip() and lines[i-1].strip() and line.strip().startswith('-') and lines[i-1].strip().startswith('-'):\n","                # Check if this bullet point is similar to the previous one\n","                similarity = self._calculate_similarity(line, lines[i-1])\n","                if similarity > 0.5:  # Skip if too similar\n","                    continue\n","            consolidated_lines.append(line)\n","\n","        optimized = '\\n'.join(consolidated_lines)\n","\n","        return optimized\n","\n","    def _add_constraints(self, template):\n","        \"\"\"Add explicit constraints to the template.\"\"\"\n","        # Check if constraints already exist\n","        if \"only\" in template.lower() and \"context\" in template.lower():\n","            # Already has basic constraints\n","            pass\n","        else:\n","            # Add constraint before ANSWER section\n","            answer_pos = template.find(\"ANSWER:\")\n","            if answer_pos != -1:\n","                constraint = \"\\nUse ONLY information from the provided context. Do not introduce external knowledge.\\n\\n\"\n","                optimized = template[:answer_pos] + constraint + template[answer_pos:]\n","            else:\n","                # Add at the beginning\n","                constraint = \"Use ONLY information from the provided context. Do not introduce external knowledge.\\n\\n\"\n","                optimized = constraint + template\n","\n","        # Add instruction for insufficient information\n","        if \"don't know\" not in template.lower() and \"insufficient\" not in template.lower():\n","            answer_pos = template.find(\"ANSWER:\")\n","            if answer_pos != -1:\n","                instruction = \"If the context doesn't contain the answer, say \\\"I don't have enough information.\\\"\\n\"\n","                optimized = template[:answer_pos] + instruction + template[answer_pos:]\n","            else:\n","                optimized = template + \"\\nIf the context doesn't contain the answer, say \\\"I don't have enough information.\\\"\\n\"\n","        else:\n","            optimized = template\n","\n","        return optimized\n","\n","    def _add_citation_requirements(self, template):\n","        \"\"\"Add citation requirements to the template.\"\"\"\n","        # Check if citation requirements already exist\n","        if \"cite\" in template.lower() or \"document\" in template.lower() and \"[\" in template:\n","            # Already has citation requirements\n","            optimized = template\n","        else:\n","            # Add citation instruction before ANSWER section\n","            answer_pos = template.find(\"ANSWER:\")\n","            if answer_pos != -1:\n","                citation = \"Cite your sources using [Document X] notation.\\n\"\n","                optimized = template[:answer_pos] + citation + template[answer_pos:]\n","            else:\n","                # Add at the end\n","                citation = \"\\nCite your sources using [Document X] notation.\"\n","                optimized = template + citation\n","\n","        return optimized\n","\n","    def _improve_structure(self, template):\n","        \"\"\"Improve the template structure.\"\"\"\n","        # Check if basic sections exist\n","        has_context = \"CONTEXT:\" in template\n","        has_question = \"QUESTION:\" in template\n","        has_answer = \"ANSWER:\" in template\n","\n","        if not has_context or not has_question or not has_answer:\n","            # Create a basic structure\n","            sections = []\n","\n","            if not has_context:\n","                sections.append(\"CONTEXT:\\n{context}\\n\")\n","            else:\n","                sections.append(self._extract_section(template, \"CONTEXT:\"))\n","\n","            if not has_question:\n","                sections.append(\"QUESTION:\\n{question}\\n\")\n","            else:\n","                sections.append(self._extract_section(template, \"QUESTION:\"))\n","\n","            if \"INSTRUCTION\" not in template and \"instruction\" not in template.lower():\n","                sections.append(\"INSTRUCTIONS:\\n- Answer based only on the provided context\\n- Cite sources using [Document X] notation\\n- If the answer isn't in the context, say so\\n\")\n","\n","            if not has_answer:\n","                sections.append(\"ANSWER:\")\n","            else:\n","                sections.append(self._extract_section(template, \"ANSWER:\"))\n","\n","            optimized = \"\\n\\n\".join(sections)\n","        else:\n","            optimized = template\n","\n","        return optimized\n","\n","    def _improve_clarity(self, template):\n","        \"\"\"Improve the clarity of the template.\"\"\"\n","        # Replace ambiguous terms\n","        ambiguous_replacements = {\n","            \"try to\": \"\",\n","            \"if possible\": \"\",\n","            \"you might want to\": \"\",\n","            \"perhaps\": \"\",\n","            \"maybe\": \"\"\n","        }\n","\n","        optimized = template\n","        for ambiguous, replacement in ambiguous_replacements.items():\n","            optimized = optimized.replace(ambiguous, replacement)\n","\n","        # Add clear section headers if missing\n","        if \"CONTEXT:\" not in optimized and \"context:\" not in optimized.lower():\n","            optimized = optimized.replace(\"{context}\", \"CONTEXT:\\n{context}\")\n","\n","        if \"QUESTION:\" not in optimized and \"question:\" not in optimized.lower():\n","            optimized = optimized.replace(\"{question}\", \"QUESTION:\\n{question}\")\n","\n","        return optimized\n","\n","    def _fix_identity_issues(self, template):\n","        \"\"\"Fix identity-related issues in the template.\"\"\"\n","        # Replace problematic phrases\n","        identity_replacements = {\n","            \"you are an expert\": \"Answer based on the context\",\n","            \"as an AI assistant\": \"\",\n","            \"using your knowledge\": \"using the provided context\",\n","            \"you know\": \"the context contains\",\n","            \"you have expertise\": \"based on the context\",\n","            \"as a helpful assistant\": \"\"\n","        }\n","\n","        optimized = template\n","        for phrase, replacement in identity_replacements.items():\n","            optimized = optimized.replace(phrase, replacement)\n","\n","        return optimized\n","\n","    def _add_error_handling(self, template):\n","        \"\"\"Add error handling instructions to the template.\"\"\"\n","        # Check if error handling already exists\n","        has_insufficient = \"don't have enough\" in template.lower() or \"insufficient\" in template.lower()\n","        has_conflicting = \"conflict\" in template.lower() or \"contradict\" in template.lower()\n","\n","        optimized = template\n","\n","        # Add instruction section if it doesn't exist\n","        if \"INSTRUCTION\" not in optimized and \"instruction\" not in optimized.lower():\n","            answer_pos = optimized.find(\"ANSWER:\")\n","            if answer_pos != -1:\n","                instructions = \"\\nINSTRUCTIONS:\\n\"\n","                if not has_insufficient:\n","                    instructions += \"- If the answer isn't in the context, say \\\"I don't have enough information\\\"\\n\"\n","                if not has_conflicting:\n","                    instructions += \"- If information is conflicting, acknowledge the contradiction\\n\"\n","\n","                optimized = optimized[:answer_pos] + instructions + optimized[answer_pos:]\n","        else:\n","            # Add to existing instructions\n","            instruction_pos = optimized.lower().find(\"instruction\")\n","            next_section_pos = optimized.find(\"\\n\\n\", instruction_pos)\n","            if next_section_pos == -1:\n","                next_section_pos = len(optimized)\n","\n","            additional_instructions = \"\"\n","            if not has_insufficient:\n","                additional_instructions += \"\\n- If the answer isn't in the context, say \\\"I don't have enough information\\\"\"\n","            if not has_conflicting:\n","                additional_instructions += \"\\n- If information is conflicting, acknowledge the contradiction\"\n","\n","            optimized = optimized[:next_section_pos] + additional_instructions + optimized[next_section_pos:]\n","\n","        return optimized\n","\n","    def _add_task_guidance(self, template):\n","        \"\"\"Add task-specific guidance to the template.\"\"\"\n","        # Determine if this is a QA template (most common)\n","        is_qa = \"question\" in template.lower() and \"answer\" in template.lower()\n","\n","        if is_qa:\n","            # Add QA-specific guidance if not already present\n","            quality_guidance = [\"concise\", \"direct\", \"clear\"]\n","            format_guidance = [\"format\", \"structure\", \"organize\"]\n","\n","            has_quality = any(term in template.lower() for term in quality_guidance)\n","            has_format = any(term in template.lower() for term in format_guidance)\n","\n","            if not has_quality or not has_format:\n","                answer_pos = template.find(\"ANSWER:\")\n","                if answer_pos != -1:\n","                    guidance = \"\"\n","                    if not has_quality:\n","                        guidance += \"Provide a clear, concise answer that directly addresses the question.\\n\"\n","                    if not has_format:\n","                        guidance += \"Structure complex answers with main points first, followed by supporting details.\\n\"\n","\n","                    optimized = template[:answer_pos] + guidance + template[answer_pos:]\n","                else:\n","                    guidance = \"\\nProvide a clear, concise answer that directly addresses the question.\\n\"\n","                    optimized = template + guidance\n","            else:\n","                optimized = template\n","        else:\n","            # Generic task guidance\n","            if \"task\" not in template.lower() and \"format\" not in template.lower():\n","                optimized = template + \"\\n\\nFormat your response appropriately for the task, maintaining clarity and conciseness.\"\n","            else:\n","                optimized = template\n","\n","        return optimized\n","\n","    def _extract_section(self, template, section_header):\n","        \"\"\"Extract a section from the template.\"\"\"\n","        start_pos = template.find(section_header)\n","        if start_pos == -1:\n","            return \"\"\n","\n","        start_pos += len(section_header)\n","        end_pos = template.find(\"\\n\\n\", start_pos)\n","        if end_pos == -1:\n","            return template[start_pos:]\n","\n","        return template[start_pos:end_pos]\n","\n","    def _calculate_similarity(self, str1, str2):\n","        \"\"\"Calculate string similarity (simple version).\"\"\"\n","        # Convert to lowercase and remove punctuation\n","        s1 = ''.join(c.lower() for c in str1 if c.isalnum() or c.isspace())\n","        s2 = ''.join(c.lower() for c in str2 if c.isalnum() or c.isspace())\n","\n","        # Split into words\n","        words1 = set(s1.split())\n","        words2 = set(s2.split())\n","\n","        # Calculate Jaccard similarity\n","        intersection = len(words1.intersection(words2))\n","        union = len(words1.union(words2))\n","\n","        return intersection / union if union > 0 else 0\n","\n","# Create a template manager and add templates\n","manager = TemplateManager()\n","\n","# Add templates from our evaluation\n","for name, template in templates_to_evaluate.items():\n","    manager.add_template(name, template, description=f\"{name} template\")\n","\n","# Set the default template\n","manager.add_template(\n","    \"production\",\n","    \"\"\"\n","    Answer the question using ONLY the provided context.\n","\n","    CONTEXT:\n","    {context}\n","\n","    QUESTION:\n","    {question}\n","\n","    INSTRUCTIONS:\n","    - If the answer is in the context, provide it clearly and concisely\n","    - Cite specific documents using [Document X] notation\n","    - If the answer isn't in the context, say \"I don't have enough information\"\n","    - If information is conflicting, acknowledge the contradiction\n","\n","    ANSWER:\n","    \"\"\",\n","    description=\"Production template with best practices\",\n","    set_as_default=True\n",")\n","\n","print(\"Template Manager initialized with templates:\")\n","for name, metadata in manager.list_templates().items():\n","    print(f\"- {name}: {metadata['description']} (v{metadata['version']}, score: {metadata['evaluation']['overall_score']:.1f}%)\")\n","\n","# Try optimizing a template\n","print(\"\\nOptimizing the 'basic' template:\")\n","optimization_result = manager.optimize_template(\"basic\")\n","\n","print(\"\\nOriginal Template:\")\n","print(optimization_result[\"original\"])\n","print(f\"Token count: {count_tokens(optimization_result['original'])}\")\n","print(f\"Overall score: {optimization_result['original_eval']['overall_score']:.1f}%\")\n","\n","print(\"\\nOptimized Template (v{version}):\".format(version=optimization_result[\"new_version\"]))\n","print(optimization_result[\"optimized\"])\n","print(f\"Token count: {count_tokens(optimization_result['optimized'])}\")\n","print(f\"Overall score: {optimization_result['optimized_eval']['overall_score']:.1f}%\")\n","\n","# Show the version history\n","print(\"\\nVersion history for 'basic' template:\")\n","versions = manager.list_versions(\"basic\")\n","for version, metadata in versions.items():\n","    print(f\"- v{version}: {metadata['description']} (score: {metadata['evaluation']['overall_score']:.1f}%)\")\n","\n","# Generate a formatted prompt\n","query = \"How much have solar photovoltaic costs decreased between 2010 and 2020?\"\n","context = \"\\n\".join([f\"[Document {i+1}] {doc.page_content}\" for i, doc in enumerate(sample_docs)])\n","\n","formatted_prompt = manager.format_prompt(context=context, question=query)\n","print(\"\\nFormatted Prompt (using default template):\")\n","print(formatted_prompt)\n","\n","# Generate a response if LLM is available\n","if os.environ.get(\"OPENAI_API_KEY\"):\n","    try:\n","        print(\"\\nGenerating response with LLM:\")\n","        llm = ChatOpenAI(temperature=0)\n","        response = llm.invoke(formatted_prompt)\n","        print(\"Response:\", response.content)\n","    except Exception as e:\n","        print(f\"Error generating response: {e}\")\n","else:\n","    print(\"\\nOpenAI API key not set - skipping LLM response generation\")\n","\n","print_separator()"],"metadata":{"id":"ykdaO4B63nKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 4: Implementation of Best Practices Checklist**"],"metadata":{"id":"mLSLdwNQ39jh"}},{"cell_type":"code","source":["print(\"Section 4: Implementation of Best Practices Checklist\")\n","\n","class TemplateChecklist:\n","    \"\"\"Checklist for template design, testing, and optimization.\"\"\"\n","\n","    def __init__(self):\n","        # Define checklist categories and items\n","        self.checklist = {\n","            \"design\": [\n","                \"Clear sections for context, query, and response\",\n","                \"Explicit constraints on using only retrieved information\",\n","                \"Guidance for handling insufficient information\",\n","                \"Citation requirements and format\",\n","                \"Token-efficient instructions\",\n","                \"Task-specific guidance\",\n","                \"Error handling for edge cases\",\n","                \"Appropriate identity framing\"\n","            ],\n","            \"testing\": [\n","                \"Tested with diverse query types\",\n","                \"Tested with different document qualities\",\n","                \"Factual accuracy measured\",\n","                \"Citation quality assessed\",\n","                \"Token usage analyzed\",\n","                \"User feedback collected\",\n","                \"Edge cases evaluated\"\n","            ],\n","            \"optimization\": [\n","                \"Instruction text optimized for token efficiency\",\n","                \"Context allocation maximized\",\n","                \"Template variants compared\",\n","                \"Error patterns addressed\",\n","                \"Performance metrics improved\",\n","                \"Versioning implemented\"\n","            ],\n","            \"deployment\": [\n","                \"Template versioning established\",\n","                \"Default templates set\",\n","                \"Documentation created\",\n","                \"Monitoring plan established\",\n","                \"Feedback collection mechanism\",\n","                \"Update schedule defined\"\n","            ]\n","        }\n","\n","        # Track completion status\n","        self.status = {}\n","        for category, items in self.checklist.items():\n","            self.status[category] = {item: {\"completed\": False, \"notes\": \"\"} for item in items}\n","\n","    def mark_completed(self, category, item, notes=\"\"):\n","        \"\"\"Mark a checklist item as completed.\"\"\"\n","        if category not in self.checklist:\n","            raise ValueError(f\"Category '{category}' not found\")\n","\n","        if item not in self.checklist[category]:\n","            raise ValueError(f\"Item '{item}' not found in category '{category}'\")\n","\n","        self.status[category][item] = {\"completed\": True, \"notes\": notes}\n","\n","    def add_notes(self, category, item, notes):\n","        \"\"\"Add notes to a checklist item.\"\"\"\n","        if category not in self.checklist:\n","            raise ValueError(f\"Category '{category}' not found\")\n","\n","        if item not in self.checklist[category]:\n","            raise ValueError(f\"Item '{item}' not found in category '{category}'\")\n","\n","        self.status[category][item][\"notes\"] = notes\n","\n","    def get_completion_status(self):\n","        \"\"\"Get the completion status of the checklist.\"\"\"\n","        status = {}\n","        for category, items in self.checklist.items():\n","            completed = sum(1 for item in items if self.status[category][item][\"completed\"])\n","            total = len(items)\n","            status[category] = {\n","                \"completed\": completed,\n","                \"total\": total,\n","                \"percentage\": (completed / total) * 100 if total > 0 else 0\n","            }\n","\n","        # Calculate overall completion\n","        total_completed = sum(status[category][\"completed\"] for category in status)\n","        total_items = sum(status[category][\"total\"] for category in status)\n","        overall = (total_completed / total_items) * 100 if total_items > 0 else 0\n","\n","        status[\"overall\"] = {\n","            \"completed\": total_completed,\n","            \"total\": total_items,\n","            \"percentage\": overall\n","        }\n","\n","        return status\n","\n","    def print_checklist(self, show_notes=True):\n","        \"\"\"Print the checklist with completion status.\"\"\"\n","        for category, items in self.checklist.items():\n","            print(f\"\\n{category.upper()} PHASE:\")\n","            for i, item in enumerate(items, 1):\n","                status = \"âœ…\" if self.status[category][item][\"completed\"] else \"â¬œ\"\n","                print(f\"{status} {i}. {item}\")\n","                if show_notes and self.status[category][item][\"notes\"]:\n","                    print(f\"   Notes: {self.status[category][item]['notes']}\")\n","\n","        # Print overall status\n","        completion = self.get_completion_status()\n","        print(\"\\nCOMPLETION STATUS:\")\n","        for category, status in completion.items():\n","            if category != \"overall\":\n","                print(f\"{category}: {status['completed']}/{status['total']} ({status['percentage']:.1f}%)\")\n","        print(f\"Overall: {completion['overall']['completed']}/{completion['overall']['total']} ({completion['overall']['percentage']:.1f}%)\")\n","\n","    def generate_progress_report(self):\n","        \"\"\"Generate a progress report for the template development process.\"\"\"\n","        completion = self.get_completion_status()\n","\n","        report = \"# Template Development Progress Report\\n\\n\"\n","        report += f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n","\n","        report += \"## Overall Progress\\n\\n\"\n","        report += f\"* **Overall completion**: {completion['overall']['completed']}/{completion['overall']['total']} ({completion['overall']['percentage']:.1f}%)\\n\\n\"\n","\n","        for category, status in completion.items():\n","            if category != \"overall\":\n","                report += f\"* **{category.capitalize()}**: {status['completed']}/{status['total']} ({status['percentage']:.1f}%)\\n\"\n","\n","        report += \"\\n## Detailed Status\\n\\n\"\n","\n","        for category, items in self.checklist.items():\n","            report += f\"### {category.capitalize()} Phase\\n\\n\"\n","\n","            completed_items = [item for item in items if self.status[category][item][\"completed\"]]\n","            pending_items = [item for item in items if not self.status[category][item][\"completed\"]]\n","\n","            if completed_items:\n","                report += \"#### Completed Items\\n\\n\"\n","                for item in completed_items:\n","                    report += f\"* âœ… {item}\\n\"\n","                    if self.status[category][item][\"notes\"]:\n","                        report += f\"  * Notes: {self.status[category][item]['notes']}\\n\"\n","                report += \"\\n\"\n","\n","            if pending_items:\n","                report += \"#### Pending Items\\n\\n\"\n","                for item in pending_items:\n","                    report += f\"* â¬œ {item}\\n\"\n","                    if self.status[category][item][\"notes\"]:\n","                        report += f\"  * Notes: {self.status[category][item]['notes']}\\n\"\n","                report += \"\\n\"\n","\n","        report += \"## Next Steps\\n\\n\"\n","\n","        # Identify the category with the lowest completion percentage\n","        lowest_category = min(\n","            [c for c in completion if c != \"overall\"],\n","            key=lambda c: completion[c][\"percentage\"]\n","        )\n","\n","        report += f\"Focus on completing items in the **{lowest_category}** phase, which has the lowest completion rate.\\n\\n\"\n","\n","        # List a few specific next steps\n","        next_steps = [\n","            item for item in self.checklist[lowest_category]\n","            if not self.status[lowest_category][item][\"completed\"]\n","        ][:3]\n","\n","        if next_steps:\n","            report += \"Specific next steps:\\n\\n\"\n","            for step in next_steps:\n","                report += f\"1. Complete **{step}**\\n\"\n","\n","        return report\n","\n","# Create a template checklist\n","checklist = TemplateChecklist()\n","\n","# Mark some items as completed for our \"production\" template\n","checklist.mark_completed(\"design\", \"Clear sections for context, query, and response\",\n","                        \"Template uses clear CONTEXT, QUESTION, and ANSWER sections\")\n","checklist.mark_completed(\"design\", \"Explicit constraints on using only retrieved information\",\n","                        \"Template explicitly instructs to use ONLY the provided context\")\n","checklist.mark_completed(\"design\", \"Guidance for handling insufficient information\",\n","                        \"Template includes instruction to say 'I don't have enough information'\")\n","checklist.mark_completed(\"design\", \"Citation requirements and format\",\n","                        \"Template requires citing sources using [Document X] notation\")\n","checklist.mark_completed(\"design\", \"Error handling for edge cases\",\n","                        \"Template addresses conflicting information\")\n","\n","checklist.mark_completed(\"testing\", \"Tested with diverse query types\",\n","                        \"Template tested with factual, comparative, and analytical queries\")\n","checklist.mark_completed(\"testing\", \"Token usage analyzed\",\n","                        \"Template uses approximately 150 tokens, leaving ample room for context\")\n","\n","checklist.mark_completed(\"optimization\", \"Instruction text optimized for token efficiency\",\n","                        \"Instructions are concise and avoid redundancy\")\n","checklist.mark_completed(\"optimization\", \"Template variants compared\",\n","                        \"Compared against basic, detailed, and optimized variants\")\n","\n","checklist.mark_completed(\"deployment\", \"Template versioning established\",\n","                        \"Using semantic versioning (major.minor.patch)\")\n","checklist.mark_completed(\"deployment\", \"Default templates set\",\n","                        \"Production template set as default\")\n","\n","# Add notes to some pending items\n","checklist.add_notes(\"testing\", \"User feedback collected\",\n","                   \"Plan to collect feedback from 5 test users next week\")\n","checklist.add_notes(\"deployment\", \"Monitoring plan established\",\n","                   \"Will track accuracy, relevance, and user satisfaction metrics\")\n","\n","# Print the checklist\n","print(\"Template Development Checklist:\")\n","checklist.print_checklist()\n","\n","# Generate and print progress report\n","print(\"\\nTemplate Development Progress Report:\")\n","report = checklist.generate_progress_report()\n","print(report)\n","\n","# Visualize checklist completion\n","try:\n","    completion = checklist.get_completion_status()\n","\n","    categories = [cat for cat in completion if cat != \"overall\"]\n","    percentages = [completion[cat][\"percentage\"] for cat in categories]\n","\n","    plt.figure(figsize=(10, 6))\n","    bars = plt.bar(categories, percentages)\n","\n","    # Add percentage labels on top of bars\n","    for bar, percentage in zip(bars, percentages):\n","        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n","                 f\"{percentage:.1f}%\", ha='center', fontsize=12)\n","\n","    plt.title(\"Template Development Progress by Phase\", fontsize=14)\n","    plt.ylabel(\"Completion Percentage\")\n","    plt.ylim(0, 105)  # Leave room for labels\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","    plt.tight_layout()\n","    plt.show()\n","except Exception as e:\n","    print(f\"Error creating visualization: {e}\")\n","    print(\"To view the visualization, run this notebook in an environment that supports matplotlib.\")\n","\n","print_separator()\n"],"metadata":{"id":"1g41Cidn39rt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 5: Performance Comparison Across Template Designs**"],"metadata":{"id":"DpFv-ZDi4H7O"}},{"cell_type":"code","source":["print(\"Section 5: Performance Comparison Across Template Designs\")\n","\n","# Define metrics for performance comparison\n","metrics = {\n","    \"factual_accuracy\": {\n","        \"description\": \"Percentage of factual claims that match the source documents\",\n","        \"importance\": \"Critical - Determines fundamental reliability\"\n","    },\n","    \"citation_rate\": {\n","        \"description\": \"Percentage of claims that include proper source citations\",\n","        \"importance\": \"High - Enables verification of information\"\n","    },\n","    \"hallucination_rate\": {\n","        \"description\": \"Percentage of claims not supported by source documents\",\n","        \"importance\": \"Critical - Measures factual reliability\"\n","    },\n","    \"relevance\": {\n","        \"description\": \"Degree to which the response addresses the specific query\",\n","        \"importance\": \"High - Determines usefulness to the user\"\n","    },\n","    \"token_efficiency\": {\n","        \"description\": \"Ratio of information content to total tokens used\",\n","        \"importance\": \"Medium - Affects cost and context usage\"\n","    },\n","    \"response_coherence\": {\n","        \"description\": \"Quality of organization and logical flow in the response\",\n","        \"importance\": \"Medium - Impacts readability and comprehension\"\n","    }\n","}\n","\n","# Mock performance data for different template designs\n","# In a real scenario, this would come from systematic testing with a large evaluation set\n","performance_data = {\n","    \"basic\": {\n","        \"factual_accuracy\": 75.3,\n","        \"citation_rate\": 12.1,\n","        \"hallucination_rate\": 18.7,\n","        \"relevance\": 83.2,\n","        \"token_efficiency\": 68.5,\n","        \"response_coherence\": 72.4\n","    },\n","    \"detailed\": {\n","        \"factual_accuracy\": 82.6,\n","        \"citation_rate\": 54.3,\n","        \"hallucination_rate\": 13.2,\n","        \"relevance\": 86.7,\n","        \"token_efficiency\": 62.1,\n","        \"response_coherence\": 78.9\n","    },\n","    \"optimized\": {\n","        \"factual_accuracy\": 85.1,\n","        \"citation_rate\": 87.3,\n","        \"hallucination_rate\": 8.4,\n","        \"relevance\": 87.4,\n","        \"token_efficiency\": 79.2,\n","        \"response_coherence\": 80.3\n","    },\n","    \"best_practice\": {\n","        \"factual_accuracy\": 91.5,\n","        \"citation_rate\": 94.7,\n","        \"hallucination_rate\": 4.3,\n","        \"relevance\": 89.6,\n","        \"token_efficiency\": 83.8,\n","        \"response_coherence\": 86.2\n","    }\n","}\n","\n","# Create a DataFrame for easier analysis\n","performance_df = pd.DataFrame(performance_data)\n","\n","# Add metric importance as a column\n","importance_values = {\"Critical\": 3, \"High\": 2, \"Medium\": 1}\n","importance = [importance_values[metrics[m][\"importance\"].split(\" \")[0]] for m in performance_df.index]\n","performance_df[\"importance\"] = importance\n","\n","# Calculate weighted scores\n","template_scores = {}\n","for template in performance_data:\n","    # Calculate scores with special handling for hallucination_rate (lower is better)\n","    weighted_scores = []\n","    for metric in performance_df.index:\n","        if metric == \"hallucination_rate\":\n","            # For hallucination, lower is better, so we invert it\n","            weighted_scores.append((100 - performance_df.loc[metric, template]) * performance_df.loc[metric, \"importance\"])\n","        else:\n","            weighted_scores.append(performance_df.loc[metric, template] * performance_df.loc[metric, \"importance\"])\n","\n","    # Calculate total weighted score\n","    total_weights = sum(performance_df[\"importance\"])\n","    weighted_average = sum(weighted_scores) / total_weights\n","    template_scores[template] = weighted_average\n","\n","# Print performance comparison\n","print(\"Template Performance Comparison:\")\n","print(\"\\nRaw Metrics:\")\n","print(performance_df[list(performance_data.keys())].round(1))\n","\n","print(\"\\nMetric Descriptions:\")\n","for metric, info in metrics.items():\n","    print(f\"- {metric}: {info['description']} ({info['importance']})\")\n","\n","print(\"\\nWeighted Scores:\")\n","for template, score in template_scores.items():\n","    print(f\"- {template}: {score:.1f}\")\n","\n","# Visualize the performance comparison\n","try:\n","    # Create a heatmap of performance metrics\n","    plt.figure(figsize=(12, 8))\n","    sns.heatmap(performance_df[list(performance_data.keys())], annot=True, cmap=\"YlGnBu\", fmt=\".1f\",\n","                linewidths=.5, cbar_kws={'label': 'Score'})\n","    plt.title(\"Template Performance Comparison\", fontsize=16, pad=20)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Create a bar chart of weighted scores\n","    plt.figure(figsize=(10, 6))\n","    bars = plt.bar(template_scores.keys(), template_scores.values())\n","\n","    # Add score labels on top of bars\n","    for bar, score in zip(bars, template_scores.values()):\n","        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n","                 f\"{score:.1f}\", ha='center', fontsize=12)\n","\n","    plt.title(\"Weighted Template Performance Scores\", fontsize=14)\n","    plt.ylabel(\"Score\")\n","    plt.ylim(0, max(template_scores.values()) * 1.1)  # Leave room for labels\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Create a radar chart for multi-dimensional comparison\n","    metrics_list = [m for m in performance_df.index if m != \"importance\"]\n","    template_names = list(performance_data.keys())\n","\n","    # Create a radar chart\n","    angles = np.linspace(0, 2*np.pi, len(metrics_list), endpoint=False).tolist()\n","    angles += angles[:1]  # Close the polygon\n","\n","    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n","\n","    for template_name in template_names:\n","        # For hallucination rate, we invert it since lower is better\n","        values = []\n","        for metric in metrics_list:\n","            if metric == \"hallucination_rate\":\n","                values.append(100 - performance_data[template_name][metric])\n","            else:\n","                values.append(performance_data[template_name][metric])\n","\n","        values += values[:1]  # Close the polygon\n","        ax.plot(angles, values, linewidth=2, label=template_name)\n","        ax.fill(angles, values, alpha=0.1)\n","\n","    # Add labels\n","    ax.set_xticks(angles[:-1])\n","    ax.set_xticklabels(metrics_list)\n","    ax.set_yticks([20, 40, 60, 80, 100])\n","    ax.set_title(\"Template Performance Across Metrics\", fontsize=14, pad=20)\n","    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n","\n","    plt.tight_layout()\n","    plt.show()\n","except Exception as e:\n","    print(f\"Error creating visualization: {e}\")\n","    print(\"To view the visualization, run this notebook in an environment that supports matplotlib.\")\n","\n","print_separator()"],"metadata":{"id":"nzJOrLqp4IJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 6: Case Studies: Template Evolution in Production**"],"metadata":{"id":"cngrDLNB4Vcy"}},{"cell_type":"code","source":["print(\"Section 6: Case Studies: Template Evolution in Production\")\n","\n","print(\"Case Study 1: Scientific Research Assistant\")\n","print(\"\\nInitial Template:\")\n","scientific_v1 = \"\"\"\n","Answer the scientific question based on the research papers provided.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Provide a comprehensive answer.\n","\"\"\"\n","print(scientific_v1)\n","\n","print(\"\\nFirst Iteration (Problem: High hallucination rate):\")\n","scientific_v2 = \"\"\"\n","Answer the scientific question using ONLY the information from the provided research papers.\n","Do not introduce external knowledge or make claims not supported by the papers.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","If the information isn't in the papers, acknowledge the limitations.\n","\"\"\"\n","print(scientific_v2)\n","\n","print(\"\\nSecond Iteration (Problem: Poor attribution):\")\n","scientific_v3 = \"\"\"\n","Answer the scientific question using ONLY the information from the provided research papers.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","- Cite specific papers using [Paper X] notation\n","- If information isn't in the papers, say \"The provided papers don't address this question\"\n","- Acknowledge limitations and uncertainty where appropriate\n","\"\"\"\n","print(scientific_v3)\n","\n","print(\"\\nFinal Version (Problem: Lack of confidence indicators):\")\n","scientific_v4 = \"\"\"\n","Answer the scientific question using ONLY the information from the provided research papers.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","INSTRUCTIONS:\n","- Cite specific papers using [Paper X] notation\n","- If information isn't in the papers, say \"The provided papers don't address this question\"\n","- Indicate confidence level for each claim (Strong evidence, Moderate evidence, Limited evidence)\n","- Note contradictions or disagreements between papers\n","- Maintain scientific precision in terminology and claims\n","\n","ANSWER FORMAT:\n","- First paragraph: Direct answer with primary findings\n","- Additional paragraphs: Supporting evidence with citations\n","- Final paragraph: Limitations and areas of uncertainty\n","\"\"\"\n","print(scientific_v4)\n","\n","print(\"\\nPerformance Evolution:\")\n","scientific_metrics = {\n","    \"v1\": {\"hallucination_rate\": 32.5, \"citation_rate\": 10.2, \"factual_accuracy\": 68.7},\n","    \"v2\": {\"hallucination_rate\": 12.3, \"citation_rate\": 15.6, \"factual_accuracy\": 83.2},\n","    \"v3\": {\"hallucination_rate\": 8.7, \"citation_rate\": 87.3, \"factual_accuracy\": 86.5},\n","    \"v4\": {\"hallucination_rate\": 4.2, \"citation_rate\": 94.8, \"factual_accuracy\": 92.1}\n","}\n","\n","for version, metrics in scientific_metrics.items():\n","    print(f\"- {version}: Hallucination: {metrics['hallucination_rate']}%, Citation: {metrics['citation_rate']}%, Accuracy: {metrics['factual_accuracy']}%\")\n","\n","print(\"\\nKey Learnings:\")\n","print(\"1. Explicit constraints dramatically reduce hallucinations\")\n","print(\"2. Specific citation formats improve attribution\")\n","print(\"3. Structured response formats enhance clarity\")\n","print(\"4. Confidence indicators help users gauge reliability\")\n","print(\"5. Domain-specific terminology guidance improves precision\")\n","\n","print(\"\\n\" + \"-\"*50 + \"\\n\")\n","\n","print(\"Case Study 2: Legal Research Assistant\")\n","print(\"\\nInitial Template:\")\n","legal_v1 = \"\"\"\n","Answer the legal question based on the provided materials.\n","\n","CONTEXT:\n","{context}\n","\n","QUESTION:\n","{question}\n","\n","Provide legal analysis and guidance.\n","\"\"\"\n","print(legal_v1)\n","\n","print(\"\\nFinal Template (After Several Iterations):\")\n","legal_v4 = \"\"\"\n","Analyze the legal question based EXCLUSIVELY on the provided legal materials.\n","Do not introduce legal principles or precedents not found in these materials.\n","\n","LEGAL MATERIALS:\n","{context}\n","\n","LEGAL QUESTION:\n","{question}\n","\n","ANALYSIS STRUCTURE:\n","1. Identify the relevant legal principles/rules from the materials\n","2. Apply these principles to the specific question\n","3. Consider potential counterarguments or alternative interpretations\n","4. Formulate a reasoned conclusion\n","\n","IMPORTANT GUIDANCE:\n","- Cite specific materials using standard legal citation [Case X] or [Statute Y]\n","- Use precise legal terminology from the provided materials\n","- Acknowledge where the materials may be inconclusive or ambiguous\n","- Clearly separate established legal principles from interpretive analysis\n","- Do NOT provide definitive legal advice, instead frame as \"based on these materials...\"\n","\n","LEGAL ANALYSIS:\n","\"\"\"\n","print(legal_v4)\n","\n","print(\"\\nKey Improvements:\")\n","print(\"1. Domain-specific citation format aligned with legal standards\")\n","print(\"2. Clear analytical structure matching legal reasoning patterns\")\n","print(\"3. Explicit separation of principles from interpretation\")\n","print(\"4. Strong disclaimers about providing definitive legal advice\")\n","print(\"5. Terminology guidance specific to legal domain\")\n","\n","print(\"\\nResults After Implementation:\")\n","print(\"- Hallucination rate decreased from 28.7% to 3.5%\")\n","print(\"- Citation accuracy increased from 45.2% to 96.8%\")\n","print(\"- Legal professionals rated answers as 'reliable' increased from 32% to 87%\")\n","\n","print_separator()"],"metadata":{"id":"XQecapFT4Vmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Section 7: Resources and Further Learning**"],"metadata":{"id":"suwv4W7u4fVY"}},{"cell_type":"code","source":["print(\"Section 7: Resources and Further Learning\")\n","\n","resources = {\n","    \"Research Papers\": [\n","        {\n","            \"title\": \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\",\n","            \"authors\": \"Wei et al.\",\n","            \"url\": \"https://arxiv.org/abs/2201.11903\",\n","            \"relevance\": \"Foundational work on prompting for explicit reasoning\"\n","        },\n","        {\n","            \"title\": \"Calibrate Before Use: Improving Few-Shot Performance of Language Models\",\n","            \"authors\": \"Zhao et al.\",\n","            \"url\": \"https://arxiv.org/abs/2102.09690\",\n","            \"relevance\": \"Techniques for optimizing few-shot examples in prompts\"\n","        },\n","        {\n","            \"title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n","            \"authors\": \"Lewis et al.\",\n","            \"url\": \"https://arxiv.org/abs/2005.11401\",\n","            \"relevance\": \"Original RAG paper with prompting insights\"\n","        }\n","    ],\n","    \"Open Source Libraries\": [\n","        {\n","            \"name\": \"LangChain\",\n","            \"url\": \"https://github.com/hwchase17/langchain\",\n","            \"relevance\": \"Comprehensive library with templates and chains for RAG\"\n","        },\n","        {\n","            \"name\": \"LlamaIndex\",\n","            \"url\": \"https://github.com/jerryjliu/llama_index\",\n","            \"relevance\": \"Specialized library for RAG with template utilities\"\n","        },\n","        {\n","            \"name\": \"Guidance\",\n","            \"url\": \"https://github.com/microsoft/guidance\",\n","            \"relevance\": \"Microsoft's library for structured prompting\"\n","        }\n","    ],\n","    \"Interactive Tools\": [\n","        {\n","            \"name\": \"OpenAI Playground\",\n","            \"url\": \"https://platform.openai.com/playground\",\n","            \"relevance\": \"Test templates with various OpenAI models\"\n","        },\n","        {\n","            \"name\": \"Anthropic Claude Console\",\n","            \"url\": \"https://console.anthropic.com/\",\n","            \"relevance\": \"Test templates with Claude models\"\n","        },\n","        {\n","            \"name\": \"PromptTools\",\n","            \"url\": \"https://github.com/hegelai/prompttools\",\n","            \"relevance\": \"Open-source toolkit for prompt testing and evaluation\"\n","        }\n","    ],\n","    \"Blogs and Guides\": [\n","        {\n","            \"name\": \"Anthropic's Prompt Engineering Guide\",\n","            \"url\": \"https://docs.anthropic.com/claude/docs/introduction-to-prompting\",\n","            \"relevance\": \"Best practices for prompting Claude models\"\n","        },\n","        {\n","            \"name\": \"OpenAI Cookbook\",\n","            \"url\": \"https://github.com/openai/openai-cookbook\",\n","            \"relevance\": \"Collection of prompting techniques and templates\"\n","        },\n","        {\n","            \"name\": \"Prompt Engineering Guide\",\n","            \"url\": \"https://www.promptingguide.ai/\",\n","            \"relevance\": \"Comprehensive guide to prompt engineering techniques\"\n","        }\n","    ]\n","}\n","\n","print(\"Resources for Further Learning on Prompt Templates:\")\n","\n","for category, items in resources.items():\n","    print(f\"\\n{category}:\")\n","    for item in items:\n","        if \"title\" in item:\n","            print(f\"- {item['title']} ({item['authors']})\")\n","            print(f\"  URL: {item['url']}\")\n","            print(f\"  Relevance: {item['relevance']}\")\n","        else:\n","            print(f\"- {item['name']}\")\n","            print(f\"  URL: {item['url']}\")\n","            print(f\"  Relevance: {item['relevance']}\")\n","\n","print(\"\\nRecommended Learning Path:\")\n","print(\"1. Start with the OpenAI and Anthropic guides for foundational understanding\")\n","print(\"2. Explore the Chain-of-Thought and RAG research papers for theoretical background\")\n","print(\"3. Experiment with the interactive tools to test different template approaches\")\n","print(\"4. Implement templates using LangChain or LlamaIndex libraries\")\n","print(\"5. Use the template evaluation framework in this notebook to assess your templates\")\n","print(\"6. Iterate based on performance metrics and user feedback\")\n","\n","print_separator()"],"metadata":{"id":"J5LMbko14feG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Final Summary**"],"metadata":{"id":"OSkW7Ahf4nm9"}},{"cell_type":"code","source":["print(\"Final Summary: Template Best Practices\")\n","\n","print(\"\"\"\n","Key Takeaways:\n","\n","1. Structure and Clarity\n","   - Use clear section headers (CONTEXT, QUESTION, ANSWER)\n","   - Provide explicit, unambiguous instructions\n","   - Maintain logical organization of template components\n","\n","2. Constraints and Guardrails\n","   - Explicitly limit responses to retrieved information\n","   - Provide guidance for handling insufficient information\n","   - Include instructions for acknowledging uncertainty\n","\n","3. Citation and Attribution\n","   - Require specific citation format ([Document X])\n","   - Provide clear guidelines for when citations are needed\n","   - Include instructions for handling conflicting sources\n","\n","4. Token Efficiency\n","   - Eliminate redundant instructions\n","   - Use concise language\n","   - Allocate tokens strategically between instructions and context\n","\n","5. Error Handling\n","   - Plan for edge cases like missing information\n","   - Include guidance for contradictory content\n","   - Provide strategies for handling ambiguity\n","\n","6. Template Management\n","   - Implement semantic versioning\n","   - Document template changes and rationale\n","   - Establish clear criteria for evaluation\n","\n","7. Continuous Improvement\n","   - Analyze error patterns systematically\n","   - Test template variations methodically\n","   - Collect and incorporate user feedback\n","   - Iterate based on performance metrics\n","\"\"\")\n","\n","print(\"\\nSuccessful templates balance comprehensiveness and efficiency, providing clear guidance while maximizing the context available for retrieved information. The techniques and tools in this notebook will help you develop templates that enhance the performance of your RAG system while delivering reliable, accurate, and helpful responses to users.\")\n","\n","print_separator()\n","\n","print(\"Notebook completed!\")"],"metadata":{"id":"ik9WgXQv4nxz"},"execution_count":null,"outputs":[]}]}