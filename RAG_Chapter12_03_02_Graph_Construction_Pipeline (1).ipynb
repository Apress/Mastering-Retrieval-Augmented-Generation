{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkJyaYrTkY7Op4OdfiYSh6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook demonstrates how to convert extracted entities and relationships into a populated Neo4j knowledge graph. You'll learn to:\n","- Convert extracted data to graph format\n","- Implement entity linking and deduplication\n","- Populate Neo4j database with nodes and relationships\n","- Validate graph construction quality\n","- Handle large-scale graph construction"],"metadata":{"id":"gfFD3BcMD7JW"}},{"cell_type":"markdown","source":["**## Setup**"],"metadata":{"id":"dqftxXKQEAZt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8XGtYThDfkD"},"outputs":[],"source":["# Install and import all packages in one cell\n","!pip install -q neo4j pandas numpy matplotlib seaborn networkx plotly json5\n","\n","# Import packages immediately after installation\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import networkx as nx\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from typing import List, Dict, Any, Tuple, Set\n","from collections import defaultdict, Counter\n","import hashlib\n","import re\n","from datetime import datetime\n","\n","# Neo4j imports with error handling\n","try:\n","    from neo4j import GraphDatabase\n","    print(\"‚úÖ Neo4j driver loaded successfully\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Neo4j driver issue: {e}\")\n","    print(\"Neo4j operations will be simulated\")\n","\n","print(\"üöÄ Setup complete! Ready for graph construction.\")"]},{"cell_type":"markdown","source":["**## Part 1: Load Extracted Knowledge**"],"metadata":{"id":"9NcspanyERpc"}},{"cell_type":"code","source":["### Load Data from Previous Notebook\n","\n","def load_extracted_knowledge():\n","    \"\"\"Load the knowledge extracted from the previous notebook.\"\"\"\n","\n","    # Try to load the saved data from previous notebook\n","    try:\n","        with open('processed_knowledge_for_graph.json', 'r') as f:\n","            data = json.load(f)\n","        print(\"‚úÖ Loaded extracted knowledge from previous notebook\")\n","        return data\n","    except FileNotFoundError:\n","        print(\"‚ö†Ô∏è Previous notebook data not found. Creating sample data...\")\n","        return create_sample_data()\n","\n","def create_sample_data():\n","    \"\"\"Create sample extracted knowledge for demonstration.\"\"\"\n","\n","    sample_knowledge = {\n","        'extracted_knowledge': [\n","            {\n","                'document_id': 'paper_1',\n","                'document_title': 'Attention Is All You Need',\n","                'document_metadata': {\n","                    'authors': ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar'],\n","                    'year': 2017,\n","                    'venue': 'NIPS'\n","                },\n","                'entities': [\n","                    {'text': 'Transformer', 'type': 'CONCEPT', 'context': 'network architecture'},\n","                    {'text': 'attention mechanisms', 'type': 'CONCEPT', 'context': 'based solely on'},\n","                    {'text': 'machine translation', 'type': 'CONCEPT', 'context': 'tasks'},\n","                    {'text': 'BLEU', 'type': 'METRIC', 'context': 'score'},\n","                    {'text': 'WMT 2014', 'type': 'DATASET', 'context': 'English-to-German'},\n","                    {'text': 'Ashish Vaswani', 'type': 'PERSON', 'context': 'author'},\n","                    {'text': 'P100 GPUs', 'type': 'TECHNOLOGY', 'context': 'eight P100 GPUs'}\n","                ],\n","                'relationships': [\n","                    {\n","                        'source': 'Transformer',\n","                        'target': 'attention mechanisms',\n","                        'relationship': 'BASED_ON',\n","                        'confidence': 0.9,\n","                        'evidence': 'based solely on attention mechanisms'\n","                    },\n","                    {\n","                        'source': 'Transformer',\n","                        'target': 'machine translation',\n","                        'relationship': 'EVALUATES_ON',\n","                        'confidence': 0.8,\n","                        'evidence': 'Experiments on two machine translation tasks'\n","                    },\n","                    {\n","                        'source': 'Transformer',\n","                        'target': 'BLEU',\n","                        'relationship': 'ACHIEVES',\n","                        'confidence': 0.9,\n","                        'evidence': 'achieves 28.4 BLEU'\n","                    },\n","                    {\n","                        'source': 'Ashish Vaswani',\n","                        'target': 'Attention Is All You Need',\n","                        'relationship': 'AUTHORED',\n","                        'confidence': 1.0,\n","                        'evidence': 'author of the paper'\n","                    }\n","                ]\n","            },\n","            {\n","                'document_id': 'paper_2',\n","                'document_title': 'BERT: Pre-training of Deep Bidirectional Transformers',\n","                'document_metadata': {\n","                    'authors': ['Jacob Devlin', 'Ming-Wei Chang'],\n","                    'year': 2018,\n","                    'venue': 'NAACL'\n","                },\n","                'entities': [\n","                    {'text': 'BERT', 'type': 'CONCEPT', 'context': 'language representation model'},\n","                    {'text': 'Bidirectional Encoder Representations', 'type': 'CONCEPT', 'context': 'full name'},\n","                    {'text': 'Transformers', 'type': 'CONCEPT', 'context': 'from Transformers'},\n","                    {'text': 'pre-training', 'type': 'CONCEPT', 'context': 'designed to pre-train'},\n","                    {'text': 'question answering', 'type': 'CONCEPT', 'context': 'tasks such as'},\n","                    {'text': 'Jacob Devlin', 'type': 'PERSON', 'context': 'author'}\n","                ],\n","                'relationships': [\n","                    {\n","                        'source': 'BERT',\n","                        'target': 'Transformers',\n","                        'relationship': 'BASED_ON',\n","                        'confidence': 0.9,\n","                        'evidence': 'Bidirectional Encoder Representations from Transformers'\n","                    },\n","                    {\n","                        'source': 'BERT',\n","                        'target': 'question answering',\n","                        'relationship': 'EVALUATES_ON',\n","                        'confidence': 0.8,\n","                        'evidence': 'tasks such as question answering'\n","                    },\n","                    {\n","                        'source': 'Jacob Devlin',\n","                        'target': 'BERT: Pre-training of Deep Bidirectional Transformers',\n","                        'relationship': 'AUTHORED',\n","                        'confidence': 1.0,\n","                        'evidence': 'author of the paper'\n","                    }\n","                ]\n","            }\n","        ],\n","        'extraction_metadata': {\n","            'total_entities': 13,\n","            'total_relationships': 7,\n","            'source_documents': 2\n","        }\n","    }\n","\n","    return sample_knowledge\n","\n","# Load the extracted knowledge\n","knowledge_data = load_extracted_knowledge()\n","extracted_knowledge = knowledge_data['extracted_knowledge']\n","\n","print(f\"üìä Loaded Knowledge Summary:\")\n","print(f\"   Documents: {len(extracted_knowledge)}\")\n","print(f\"   Total entities: {sum(len(doc.get('entities', [])) for doc in extracted_knowledge)}\")\n","print(f\"   Total relationships: {sum(len(doc.get('relationships', [])) for doc in extracted_knowledge)}\")"],"metadata":{"id":"9Mov94M4ERyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 2: Entity Linking and Deduplication**"],"metadata":{"id":"Hdi46KruEd4m"}},{"cell_type":"code","source":["### Entity Normalization and Linking\n","\n","class EntityLinker:\n","    \"\"\"Handle entity normalization, linking, and deduplication.\"\"\"\n","\n","    def __init__(self):\n","        self.entity_mapping = {}  # Maps variations to canonical form\n","        self.canonical_entities = {}  # Stores canonical entity information\n","        self.similarity_threshold = 0.8\n","\n","    def normalize_entity_text(self, text: str) -> str:\n","        \"\"\"Normalize entity text for comparison.\"\"\"\n","        # Convert to lowercase and remove extra whitespace\n","        normalized = re.sub(r'\\s+', ' ', text.lower().strip())\n","\n","        # Remove common suffixes/prefixes that don't affect meaning\n","        normalized = re.sub(r'\\b(the|a|an)\\b', '', normalized)\n","        normalized = normalized.strip()\n","\n","        # Handle acronyms and full forms\n","        # e.g., \"BERT\" and \"Bidirectional Encoder Representations from Transformers\"\n","        if len(normalized) <= 10 and normalized.isupper():\n","            return normalized  # Keep acronyms as-is\n","\n","        return normalized\n","\n","    def calculate_similarity(self, text1: str, text2: str) -> float:\n","        \"\"\"Calculate similarity between two entity texts.\"\"\"\n","        norm1 = self.normalize_entity_text(text1)\n","        norm2 = self.normalize_entity_text(text2)\n","\n","        # Exact match\n","        if norm1 == norm2:\n","            return 1.0\n","\n","        # Check if one is contained in the other (substring match)\n","        if norm1 in norm2 or norm2 in norm1:\n","            return 0.9\n","\n","        # Check for acronym matches\n","        if self._is_acronym_match(text1, text2):\n","            return 0.95\n","\n","        # Simple character-based similarity (Jaccard similarity)\n","        set1 = set(norm1.split())\n","        set2 = set(norm2.split())\n","\n","        if not set1 or not set2:\n","            return 0.0\n","\n","        intersection = len(set1.intersection(set2))\n","        union = len(set1.union(set2))\n","\n","        return intersection / union if union > 0 else 0.0\n","\n","    def _is_acronym_match(self, text1: str, text2: str) -> bool:\n","        \"\"\"Check if one text is an acronym of the other.\"\"\"\n","        short, long = (text1, text2) if len(text1) < len(text2) else (text2, text1)\n","\n","        if len(short) <= 10 and short.isupper() and len(long) > 10:\n","            # Extract first letters of each word in the long form\n","            words = long.split()\n","            if len(words) >= len(short):\n","                acronym = ''.join([word[0].upper() for word in words if word])\n","                return short == acronym\n","\n","        return False\n","\n","    def find_canonical_entity(self, entity: Dict[str, Any]) -> str:\n","        \"\"\"Find or create canonical form for an entity.\"\"\"\n","        entity_text = entity['text']\n","        entity_type = entity['type']\n","\n","        # Check if we already have a mapping for this exact text\n","        if entity_text in self.entity_mapping:\n","            return self.entity_mapping[entity_text]\n","\n","        # Look for similar entities of the same type\n","        best_match = None\n","        best_similarity = 0.0\n","\n","        for canonical_id, canonical_info in self.canonical_entities.items():\n","            if canonical_info['type'] == entity_type:\n","                similarity = self.calculate_similarity(entity_text, canonical_info['text'])\n","                if similarity > best_similarity and similarity >= self.similarity_threshold:\n","                    best_similarity = similarity\n","                    best_match = canonical_id\n","\n","        if best_match:\n","            # Map this entity to existing canonical entity\n","            self.entity_mapping[entity_text] = best_match\n","            # Update canonical entity with additional context\n","            self.canonical_entities[best_match]['variants'].add(entity_text)\n","            return best_match\n","        else:\n","            # Create new canonical entity\n","            canonical_id = f\"{entity_type.lower()}_{len(self.canonical_entities)}\"\n","            self.canonical_entities[canonical_id] = {\n","                'id': canonical_id,\n","                'text': entity_text,\n","                'type': entity_type,\n","                'variants': {entity_text},\n","                'contexts': [entity.get('context', '')]\n","            }\n","            self.entity_mapping[entity_text] = canonical_id\n","            return canonical_id\n","\n","    def link_entities(self, extracted_knowledge: List[Dict]) -> Dict[str, Any]:\n","        \"\"\"Process all entities and create canonical mappings.\"\"\"\n","\n","        print(\"üîó Starting entity linking process...\")\n","\n","        # First pass: create canonical entities\n","        for doc in extracted_knowledge:\n","            for entity in doc.get('entities', []):\n","                canonical_id = self.find_canonical_entity(entity)\n","                # Add document context\n","                if canonical_id in self.canonical_entities:\n","                    self.canonical_entities[canonical_id]['contexts'].append(\n","                        f\"From {doc['document_title']}: {entity.get('context', '')}\"\n","                    )\n","\n","        # Generate linking statistics\n","        total_entities = sum(len(doc.get('entities', [])) for doc in extracted_knowledge)\n","        unique_entities = len(self.canonical_entities)\n","\n","        linking_stats = {\n","            'total_entities': total_entities,\n","            'unique_entities': unique_entities,\n","            'deduplication_ratio': (total_entities - unique_entities) / total_entities if total_entities > 0 else 0,\n","            'entity_mappings': len(self.entity_mapping)\n","        }\n","\n","        print(f\"‚úÖ Entity linking complete:\")\n","        print(f\"   Original entities: {total_entities}\")\n","        print(f\"   Unique entities: {unique_entities}\")\n","        print(f\"   Deduplication ratio: {linking_stats['deduplication_ratio']:.2%}\")\n","\n","        return {\n","            'canonical_entities': self.canonical_entities,\n","            'entity_mapping': self.entity_mapping,\n","            'linking_stats': linking_stats\n","        }\n","\n","# Initialize entity linker and process entities\n","entity_linker = EntityLinker()\n","linking_results = entity_linker.link_entities(extracted_knowledge)\n","\n","# Display some examples of linked entities\n","print(\"\\nüîç Sample Canonical Entities:\")\n","for i, (canonical_id, entity_info) in enumerate(list(linking_results['canonical_entities'].items())[:5]):\n","    print(f\"   {i+1}. {canonical_id}: {entity_info['text']} ({entity_info['type']})\")\n","    if len(entity_info['variants']) > 1:\n","        print(f\"      Variants: {', '.join(entity_info['variants'])}\")"],"metadata":{"id":"jXEAvK5dEeBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Relationship Normalization**"],"metadata":{"id":"-ikA2X5VEp7t"}},{"cell_type":"code","source":["class RelationshipProcessor:\n","    \"\"\"Process and normalize relationships for graph construction.\"\"\"\n","\n","    def __init__(self, entity_mapping: Dict[str, str]):\n","        self.entity_mapping = entity_mapping\n","        self.processed_relationships = []\n","        self.relationship_stats = defaultdict(int)\n","\n","    def normalize_relationship(self, relationship: Dict[str, Any], doc_id: str) -> Dict[str, Any]:\n","        \"\"\"Normalize a relationship using canonical entity mappings.\"\"\"\n","\n","        source_text = relationship.get('source', '')\n","        target_text = relationship.get('target', '')\n","\n","        # Map to canonical entities\n","        source_canonical = self.entity_mapping.get(source_text, source_text)\n","        target_canonical = self.entity_mapping.get(target_text, target_text)\n","\n","        # Skip self-relationships\n","        if source_canonical == target_canonical:\n","            return None\n","\n","        normalized_rel = {\n","            'source': source_canonical,\n","            'target': target_canonical,\n","            'relationship': relationship.get('relationship', 'RELATED'),\n","            'confidence': relationship.get('confidence', 0.5),\n","            'evidence': relationship.get('evidence', ''),\n","            'source_document': doc_id,\n","            'original_source': source_text,\n","            'original_target': target_text\n","        }\n","\n","        return normalized_rel\n","\n","    def process_relationships(self, extracted_knowledge: List[Dict]) -> List[Dict]:\n","        \"\"\"Process all relationships from extracted knowledge.\"\"\"\n","\n","        print(\"üîÑ Processing relationships...\")\n","\n","        all_relationships = []\n","\n","        for doc in extracted_knowledge:\n","            doc_id = doc['document_id']\n","\n","            for relationship in doc.get('relationships', []):\n","                normalized = self.normalize_relationship(relationship, doc_id)\n","                if normalized:\n","                    all_relationships.append(normalized)\n","                    self.relationship_stats[normalized['relationship']] += 1\n","\n","        # Remove duplicate relationships (same source, target, relationship type)\n","        unique_relationships = []\n","        seen_relationships = set()\n","\n","        for rel in all_relationships:\n","            rel_key = (rel['source'], rel['target'], rel['relationship'])\n","            if rel_key not in seen_relationships:\n","                unique_relationships.append(rel)\n","                seen_relationships.add(rel_key)\n","            else:\n","                # Merge confidence scores for duplicates\n","                for existing_rel in unique_relationships:\n","                    if (existing_rel['source'], existing_rel['target'], existing_rel['relationship']) == rel_key:\n","                        existing_rel['confidence'] = max(existing_rel['confidence'], rel['confidence'])\n","                        break\n","\n","        print(f\"‚úÖ Relationship processing complete:\")\n","        print(f\"   Total relationships: {len(all_relationships)}\")\n","        print(f\"   Unique relationships: {len(unique_relationships)}\")\n","        print(f\"   Relationship types: {len(self.relationship_stats)}\")\n","\n","        return unique_relationships\n","\n","# Process relationships using the canonical entity mappings\n","relationship_processor = RelationshipProcessor(linking_results['entity_mapping'])\n","processed_relationships = relationship_processor.process_relationships(extracted_knowledge)\n","\n","# Display relationship statistics\n","print(f\"\\nüìä Relationship Type Distribution:\")\n","relationship_counter = Counter()\n","for rel_type, count in relationship_processor.relationship_stats.items():\n","    relationship_counter[rel_type] = count\n","\n","for rel_type, count in relationship_counter.most_common():\n","    print(f\"   {rel_type}: {count}\")"],"metadata":{"id":"N0TxSzfwEqES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 3: Neo4j Database Population**"],"metadata":{"id":"6fCV0_sNFMjk"}},{"cell_type":"code","source":["### Neo4j Connection Management\n","\n","class Neo4jGraphConstructor:\n","    \"\"\"Manage Neo4j database connection and graph construction.\"\"\"\n","\n","    def __init__(self, uri: str = None, user: str = None, password: str = None):\n","        self.uri = uri\n","        self.user = user\n","        self.password = password\n","        self.driver = None\n","        self.use_real_neo4j = False\n","\n","        # Try to connect to Neo4j\n","        if uri and user and password:\n","            try:\n","                self.driver = GraphDatabase.driver(uri, auth=(user, password))\n","                # Test connection\n","                with self.driver.session() as session:\n","                    session.run(\"RETURN 1\")\n","                self.use_real_neo4j = True\n","                print(\"‚úÖ Connected to Neo4j database\")\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Neo4j connection failed: {e}\")\n","                print(\"Will simulate Neo4j operations\")\n","        else:\n","            print(\"üìù No Neo4j credentials provided. Simulating operations.\")\n","\n","    def execute_query(self, query: str, parameters: Dict = None) -> List[Dict]:\n","        \"\"\"Execute a Cypher query.\"\"\"\n","        if self.use_real_neo4j and self.driver:\n","            try:\n","                with self.driver.session() as session:\n","                    result = session.run(query, parameters or {})\n","                    return [record.data() for record in result]\n","            except Exception as e:\n","                print(f\"‚ùå Query execution error: {e}\")\n","                return []\n","        else:\n","            # Simulate query execution\n","            print(f\"üîç Simulated Query: {query[:100]}...\")\n","            if parameters:\n","                print(f\"   Parameters: {list(parameters.keys())}\")\n","            return [{\"status\": \"simulated\"}]\n","\n","    def clear_database(self):\n","        \"\"\"Clear the Neo4j database.\"\"\"\n","        query = \"MATCH (n) DETACH DELETE n\"\n","        result = self.execute_query(query)\n","        print(\"üóëÔ∏è Database cleared\")\n","        return result\n","\n","    def create_indexes(self):\n","        \"\"\"Create indexes for better performance.\"\"\"\n","        indexes = [\n","            \"CREATE INDEX entity_id_index IF NOT EXISTS FOR (e:Entity) ON (e.id)\",\n","            \"CREATE INDEX document_id_index IF NOT EXISTS FOR (d:Document) ON (d.id)\",\n","            \"CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)\"\n","        ]\n","\n","        for index_query in indexes:\n","            self.execute_query(index_query)\n","\n","        print(\"üìä Created database indexes\")\n","\n","    def close(self):\n","        \"\"\"Close the database connection.\"\"\"\n","        if self.driver:\n","            self.driver.close()\n","            print(\"Connection closed\")\n","\n","# Initialize Neo4j constructor\n","# Replace with your actual Neo4j credentials\n","NEO4J_URI = \"neo4j+s://your-instance.databases.neo4j.io\"  # Your Neo4j URI\n","NEO4J_USERNAME = \"neo4j\"                                   # Usually 'neo4j'\n","NEO4J_PASSWORD = \"your-password-here\"                      # Your password\n","\n","# For demonstration, we'll use simulation mode\n","# Set these to your actual credentials to use real Neo4j\n","neo4j_constructor = Neo4jGraphConstructor()  # No credentials = simulation mode\n","\n","# Uncomment and set your credentials for real Neo4j usage:\n","# neo4j_constructor = Neo4jGraphConstructor(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n","\n","# Clear database and create indexes\n","neo4j_constructor.clear_database()\n","neo4j_constructor.create_indexes()"],"metadata":{"id":"QKgvKNcxFMrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Document Node Creation**"],"metadata":{"id":"_0Om1rZDFfa8"}},{"cell_type":"code","source":["def create_document_nodes(extracted_knowledge: List[Dict], neo4j_constructor: Neo4jGraphConstructor):\n","    \"\"\"Create document nodes in Neo4j.\"\"\"\n","\n","    print(\"üìÑ Creating document nodes...\")\n","\n","    document_creation_query = \"\"\"\n","    UNWIND $documents as doc\n","    CREATE (d:Document {\n","        id: doc.id,\n","        title: doc.title,\n","        authors: doc.authors,\n","        year: doc.year,\n","        venue: doc.venue,\n","        created_at: datetime()\n","    })\n","    \"\"\"\n","\n","    # Prepare document data\n","    documents = []\n","    for doc in extracted_knowledge:\n","        doc_data = {\n","            'id': doc['document_id'],\n","            'title': doc['document_title'],\n","            'authors': doc.get('document_metadata', {}).get('authors', []),\n","            'year': doc.get('document_metadata', {}).get('year', 0),\n","            'venue': doc.get('document_metadata', {}).get('venue', '')\n","        }\n","        documents.append(doc_data)\n","\n","    # Execute document creation\n","    result = neo4j_constructor.execute_query(document_creation_query, {'documents': documents})\n","\n","    print(f\"‚úÖ Created {len(documents)} document nodes\")\n","    return result\n","\n","# Create document nodes\n","doc_creation_result = create_document_nodes(extracted_knowledge, neo4j_constructor)"],"metadata":{"id":"VqhxbARxFflD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Entity Node Creation**"],"metadata":{"id":"iyWTBZL2FqMb"}},{"cell_type":"code","source":["def create_entity_nodes(canonical_entities: Dict[str, Dict], neo4j_constructor: Neo4jGraphConstructor):\n","    \"\"\"Create entity nodes in Neo4j.\"\"\"\n","\n","    print(\"üè∑Ô∏è Creating entity nodes...\")\n","\n","    entity_creation_query = \"\"\"\n","    UNWIND $entities as entity\n","    CREATE (e:Entity {\n","        id: entity.id,\n","        text: entity.text,\n","        type: entity.type,\n","        variants: entity.variants,\n","        contexts: entity.contexts,\n","        created_at: datetime()\n","    })\n","    \"\"\"\n","\n","    # Prepare entity data\n","    entities = []\n","    for canonical_id, entity_info in canonical_entities.items():\n","        entity_data = {\n","            'id': canonical_id,\n","            'text': entity_info['text'],\n","            'type': entity_info['type'],\n","            'variants': list(entity_info['variants']),\n","            'contexts': entity_info['contexts'][:5]  # Limit contexts to avoid too much data\n","        }\n","        entities.append(entity_data)\n","\n","    # Execute entity creation in batches to avoid memory issues\n","    batch_size = 100\n","    total_created = 0\n","\n","    for i in range(0, len(entities), batch_size):\n","        batch = entities[i:i + batch_size]\n","        result = neo4j_constructor.execute_query(entity_creation_query, {'entities': batch})\n","        total_created += len(batch)\n","        print(f\"   Created batch {i//batch_size + 1}: {len(batch)} entities\")\n","\n","    print(f\"‚úÖ Created {total_created} entity nodes\")\n","    return total_created\n","\n","# Create entity nodes\n","entity_creation_result = create_entity_nodes(linking_results['canonical_entities'], neo4j_constructor)"],"metadata":{"id":"b_91NwBdFqTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Relationship Creation**"],"metadata":{"id":"FNZb3k3eFzmg"}},{"cell_type":"code","source":["def create_relationships(processed_relationships: List[Dict], neo4j_constructor: Neo4jGraphConstructor):\n","    \"\"\"Create relationships in Neo4j.\"\"\"\n","\n","    print(\"üîó Creating relationships...\")\n","\n","    # Group relationships by type for efficient creation\n","    relationships_by_type = defaultdict(list)\n","    for rel in processed_relationships:\n","        relationships_by_type[rel['relationship']].append(rel)\n","\n","    total_created = 0\n","\n","    for rel_type, relationships in relationships_by_type.items():\n","        print(f\"   Creating {len(relationships)} {rel_type} relationships...\")\n","\n","        # Create relationship query\n","        relationship_query = f\"\"\"\n","        UNWIND $relationships as rel\n","        MATCH (source:Entity {{id: rel.source}})\n","        MATCH (target:Entity {{id: rel.target}})\n","        CREATE (source)-[r:{rel_type} {{\n","            confidence: rel.confidence,\n","            evidence: rel.evidence,\n","            source_document: rel.source_document,\n","            created_at: datetime()\n","        }}]->(target)\n","        \"\"\"\n","\n","        # Execute in batches\n","        batch_size = 50\n","        for i in range(0, len(relationships), batch_size):\n","            batch = relationships[i:i + batch_size]\n","            try:\n","                neo4j_constructor.execute_query(relationship_query, {'relationships': batch})\n","                total_created += len(batch)\n","            except Exception as e:\n","                print(f\"   ‚ö†Ô∏è Error creating {rel_type} relationships: {e}\")\n","\n","    print(f\"‚úÖ Created {total_created} relationships\")\n","    return total_created\n","\n","# Create relationships\n","relationship_creation_result = create_relationships(processed_relationships, neo4j_constructor)"],"metadata":{"id":"ri2uqdxPFzu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Document-Entity Connections**"],"metadata":{"id":"FZddkczFF7T_"}},{"cell_type":"code","source":["def create_document_entity_connections(extracted_knowledge: List[Dict],\n","                                     entity_mapping: Dict[str, str],\n","                                     neo4j_constructor: Neo4jGraphConstructor):\n","    \"\"\"Create CONTAINS relationships between documents and entities.\"\"\"\n","\n","    print(\"üìã Creating document-entity connections...\")\n","\n","    connection_query = \"\"\"\n","    UNWIND $connections as conn\n","    MATCH (doc:Document {id: conn.document_id})\n","    MATCH (entity:Entity {id: conn.entity_id})\n","    CREATE (doc)-[:CONTAINS {\n","        context: conn.context,\n","        created_at: datetime()\n","    }]->(entity)\n","    \"\"\"\n","\n","    # Prepare connection data\n","    connections = []\n","    for doc in extracted_knowledge:\n","        doc_id = doc['document_id']\n","\n","        for entity in doc.get('entities', []):\n","            canonical_id = entity_mapping.get(entity['text'], entity['text'])\n","\n","            connection_data = {\n","                'document_id': doc_id,\n","                'entity_id': canonical_id,\n","                'context': entity.get('context', '')\n","            }\n","            connections.append(connection_data)\n","\n","    # Execute in batches\n","    batch_size = 100\n","    total_created = 0\n","\n","    for i in range(0, len(connections), batch_size):\n","        batch = connections[i:i + batch_size]\n","        neo4j_constructor.execute_query(connection_query, {'connections': batch})\n","        total_created += len(batch)\n","\n","    print(f\"‚úÖ Created {total_created} document-entity connections\")\n","    return total_created\n","\n","# Create document-entity connections\n","connection_result = create_document_entity_connections(\n","    extracted_knowledge,\n","    linking_results['entity_mapping'],\n","    neo4j_constructor\n",")"],"metadata":{"id":"un5ew4slF7bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 4: Graph Validation and Quality Assessment**"],"metadata":{"id":"sUUK-u4YGD4p"}},{"cell_type":"code","source":["### Graph Statistics and Validation\n","\n","class GraphValidator:\n","    \"\"\"Validate the constructed knowledge graph.\"\"\"\n","\n","    def __init__(self, neo4j_constructor: Neo4jGraphConstructor):\n","        self.neo4j = neo4j_constructor\n","\n","    def get_graph_statistics(self) -> Dict[str, Any]:\n","        \"\"\"Get comprehensive graph statistics.\"\"\"\n","\n","        print(\"üìä Collecting graph statistics...\")\n","\n","        # Basic node and relationship counts\n","        node_count_query = \"MATCH (n) RETURN labels(n)[0] as label, count(n) as count\"\n","        node_counts = self.neo4j.execute_query(node_count_query)\n","\n","        rel_count_query = \"MATCH ()-[r]->() RETURN type(r) as type, count(r) as count\"\n","        rel_counts = self.neo4j.execute_query(rel_count_query)\n","\n","        # Entity type distribution\n","        entity_type_query = \"MATCH (e:Entity) RETURN e.type as type, count(e) as count\"\n","        entity_types = self.neo4j.execute_query(entity_type_query)\n","\n","        # Connectivity statistics\n","        connectivity_query = \"\"\"\n","        MATCH (n)\n","        OPTIONAL MATCH (n)-[r]-()\n","        RETURN labels(n)[0] as node_type,\n","               count(DISTINCT n) as nodes,\n","               count(r) as total_relationships,\n","               count(r) * 1.0 / count(DISTINCT n) as avg_degree\n","        \"\"\"\n","        connectivity_stats = self.neo4j.execute_query(connectivity_query)\n","\n","        # Most connected entities\n","        top_entities_query = \"\"\"\n","        MATCH (e:Entity)\n","        OPTIONAL MATCH (e)-[r]-()\n","        RETURN e.text as entity, e.type as type, count(r) as degree\n","        ORDER BY degree DESC\n","        LIMIT 10\n","        \"\"\"\n","        top_entities = self.neo4j.execute_query(top_entities_query)\n","\n","        # Isolated nodes (nodes with no relationships)\n","        isolated_nodes_query = \"\"\"\n","        MATCH (n)\n","        WHERE NOT (n)-[]-()\n","        RETURN labels(n)[0] as type, count(n) as count\n","        \"\"\"\n","        isolated_nodes = self.neo4j.execute_query(isolated_nodes_query)\n","\n","        stats = {\n","            'node_counts': node_counts,\n","            'relationship_counts': rel_counts,\n","            'entity_types': entity_types,\n","            'connectivity': connectivity_stats,\n","            'top_entities': top_entities,\n","            'isolated_nodes': isolated_nodes\n","        }\n","\n","        return stats\n","\n","    def validate_graph_quality(self) -> Dict[str, Any]:\n","        \"\"\"Validate graph construction quality.\"\"\"\n","\n","        print(\"üîç Validating graph quality...\")\n","\n","        validation_results = {\n","            'issues': [],\n","            'warnings': [],\n","            'quality_score': 0.0\n","        }\n","\n","        stats = self.get_graph_statistics()\n","\n","        # Check for isolated nodes\n","        isolated_count = sum(item.get('count', 0) for item in stats.get('isolated_nodes', []))\n","        total_nodes = sum(item.get('count', 0) for item in stats.get('node_counts', []))\n","\n","        if isolated_count > 0:\n","            isolation_ratio = isolated_count / total_nodes if total_nodes > 0 else 0\n","            if isolation_ratio > 0.3:\n","                validation_results['issues'].append(f\"High isolation ratio: {isolation_ratio:.2%} of nodes are isolated\")\n","            elif isolation_ratio > 0.1:\n","                validation_results['warnings'].append(f\"Moderate isolation ratio: {isolation_ratio:.2%} of nodes are isolated\")\n","\n","        # Check relationship distribution\n","        rel_counts = {item.get('type', 'UNKNOWN'): item.get('count', 0) for item in stats.get('relationship_counts', [])}\n","        total_rels = sum(rel_counts.values())\n","\n","        if total_rels == 0:\n","            validation_results['issues'].append(\"No relationships found in graph\")\n","        else:\n","            # Check for relationship diversity\n","            rel_types = len(rel_counts)\n","            if rel_types < 3:\n","                validation_results['warnings'].append(f\"Low relationship diversity: only {rel_types} relationship types\")\n","\n","        # Check entity type distribution\n","        entity_types = {item.get('type', 'UNKNOWN'): item.get('count', 0) for item in stats.get('entity_types', [])}\n","        if len(entity_types) < 3:\n","            validation_results['warnings'].append(f\"Low entity type diversity: only {len(entity_types)} entity types\")\n","\n","        # Calculate quality score\n","        quality_factors = []\n","\n","        # Factor 1: Connection density (lower isolation = higher quality)\n","        if total_nodes > 0:\n","            connection_factor = 1.0 - (isolated_count / total_nodes)\n","            quality_factors.append(connection_factor * 0.3)\n","\n","        # Factor 2: Relationship diversity\n","        if total_rels > 0:\n","            diversity_factor = min(len(rel_counts) / 5.0, 1.0)  # Normalize to max 5 types\n","            quality_factors.append(diversity_factor * 0.3)\n","\n","        # Factor 3: Entity type diversity\n","        entity_diversity_factor = min(len(entity_types) / 6.0, 1.0)  # Normalize to max 6 types\n","        quality_factors.append(entity_diversity_factor * 0.2)\n","\n","        # Factor 4: Graph size (more nodes = potentially higher quality, up to a point)\n","        size_factor = min(total_nodes / 100.0, 1.0)  # Normalize to 100 nodes\n","        quality_factors.append(size_factor * 0.2)\n","\n","        validation_results['quality_score'] = sum(quality_factors)\n","\n","        return validation_results\n","\n","    def print_validation_report(self, stats: Dict, validation: Dict):\n","        \"\"\"Print a comprehensive validation report.\"\"\"\n","\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"üìã GRAPH CONSTRUCTION VALIDATION REPORT\")\n","        print(\"=\"*60)\n","\n","        # Basic statistics\n","        print(\"\\nüìä Graph Statistics:\")\n","        total_nodes = sum(item.get('count', 0) for item in stats.get('node_counts', []))\n","        total_rels = sum(item.get('count', 0) for item in stats.get('relationship_counts', []))\n","\n","        print(f\"   Total Nodes: {total_nodes}\")\n","        print(f\"   Total Relationships: {total_rels}\")\n","\n","        print(\"\\n   Node Distribution:\")\n","        for item in stats.get('node_counts', []):\n","            print(f\"      {item.get('label', 'Unknown')}: {item.get('count', 0)}\")\n","\n","        print(\"\\n   Relationship Distribution:\")\n","        for item in stats.get('relationship_counts', []):\n","            print(f\"      {item.get('type', 'Unknown')}: {item.get('count', 0)}\")\n","\n","        print(\"\\n   Entity Type Distribution:\")\n","        for item in stats.get('entity_types', []):\n","            print(f\"      {item.get('type', 'Unknown')}: {item.get('count', 0)}\")\n","\n","        # Top connected entities\n","        print(\"\\nüîó Most Connected Entities:\")\n","        for item in stats.get('top_entities', [])[:5]:\n","            print(f\"      {item.get('entity', 'Unknown')} ({item.get('type', 'Unknown')}): {item.get('degree', 0)} connections\")\n","\n","        # Quality assessment\n","        print(f\"\\nüéØ Quality Assessment:\")\n","        print(f\"   Overall Quality Score: {validation['quality_score']:.2f}/1.0\")\n","\n","        if validation['issues']:\n","            print(f\"\\n‚ùå Issues Found:\")\n","            for issue in validation['issues']:\n","                print(f\"      ‚Ä¢ {issue}\")\n","\n","        if validation['warnings']:\n","            print(f\"\\n‚ö†Ô∏è Warnings:\")\n","            for warning in validation['warnings']:\n","                print(f\"      ‚Ä¢ {warning}\")\n","\n","        if not validation['issues'] and not validation['warnings']:\n","            print(f\"   ‚úÖ No issues detected!\")\n","\n","        print(\"\\n\" + \"=\"*60)\n","\n","# Validate the constructed graph\n","validator = GraphValidator(neo4j_constructor)\n","graph_stats = validator.get_graph_statistics()\n","validation_results = validator.validate_graph_quality()\n","\n","# Print validation report\n","validator.print_validation_report(graph_stats, validation_results)"],"metadata":{"id":"9Wl6FuUmGEA1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 5: Graph Visualization and Analysis**"],"metadata":{"id":"-UF6CtyMGQmS"}},{"cell_type":"code","source":["### NetworkX Graph Creation for Visualization\n","\n","def create_networkx_graph(canonical_entities: Dict, processed_relationships: List[Dict]) -> nx.Graph:\n","    \"\"\"Create a NetworkX graph for visualization.\"\"\"\n","\n","    print(\"üé® Creating NetworkX graph for visualization...\")\n","\n","    G = nx.Graph()\n","\n","    # Add nodes\n","    for entity_id, entity_info in canonical_entities.items():\n","        G.add_node(entity_id,\n","                  text=entity_info['text'],\n","                  type=entity_info['type'],\n","                  variants=len(entity_info['variants']))\n","\n","    # Add edges\n","    for rel in processed_relationships:\n","        if rel['source'] in G.nodes and rel['target'] in G.nodes:\n","            G.add_edge(rel['source'], rel['target'],\n","                      relationship=rel['relationship'],\n","                      confidence=rel['confidence'])\n","\n","    print(f\"‚úÖ Created NetworkX graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n","    return G\n","\n","def visualize_graph_structure(G: nx.Graph):\n","    \"\"\"Visualize the graph structure using matplotlib.\"\"\"\n","\n","    plt.figure(figsize=(15, 10))\n","\n","    # Create layout\n","    pos = nx.spring_layout(G, k=2, iterations=50)\n","\n","    # Color nodes by type\n","    entity_types = list(set(nx.get_node_attributes(G, 'type').values()))\n","    colors = plt.cm.Set3(np.linspace(0, 1, len(entity_types)))\n","    type_color_map = dict(zip(entity_types, colors))\n","\n","    node_colors = [type_color_map.get(G.nodes[node].get('type', 'UNKNOWN'), 'gray')\n","                   for node in G.nodes()]\n","\n","    # Draw the graph\n","    nx.draw(G, pos,\n","            node_color=node_colors,\n","            node_size=300,\n","            font_size=8,\n","            font_weight='bold',\n","            edge_color='gray',\n","            alpha=0.7,\n","            with_labels=False)\n","\n","    # Add node labels\n","    labels = {node: G.nodes[node].get('text', node)[:15] for node in G.nodes()}\n","    nx.draw_networkx_labels(G, pos, labels, font_size=6)\n","\n","    # Create legend\n","    from matplotlib.patches import Patch\n","    legend_elements = [Patch(facecolor=type_color_map[entity_type], label=entity_type)\n","                      for entity_type in entity_types]\n","    plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))\n","\n","    plt.title(\"Knowledge Graph Structure\", size=16, weight='bold')\n","    plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def analyze_graph_metrics(G: nx.Graph):\n","    \"\"\"Analyze graph metrics and properties.\"\"\"\n","\n","    print(\"üìà GRAPH ANALYSIS METRICS\")\n","    print(\"=\"*40)\n","\n","    # Basic metrics\n","    print(f\"Nodes: {G.number_of_nodes()}\")\n","    print(f\"Edges: {G.number_of_edges()}\")\n","    print(f\"Density: {nx.density(G):.4f}\")\n","\n","    # Connectivity\n","    if G.number_of_nodes() > 0:\n","        if nx.is_connected(G):\n","            print(\"Graph is connected\")\n","            print(f\"Diameter: {nx.diameter(G)}\")\n","            print(f\"Average shortest path length: {nx.average_shortest_path_length(G):.2f}\")\n","        else:\n","            components = list(nx.connected_components(G))\n","            print(f\"Graph has {len(components)} connected components\")\n","            largest_component_size = max(len(comp) for comp in components)\n","            print(f\"Largest component size: {largest_component_size}\")\n","\n","    # Centrality measures\n","    if G.number_of_nodes() > 0:\n","        degree_centrality = nx.degree_centrality(G)\n","        betweenness_centrality = nx.betweenness_centrality(G)\n","\n","        print(f\"\\nTop 5 nodes by degree centrality:\")\n","        sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n","        for node, centrality in sorted_degree[:5]:\n","            node_text = G.nodes[node].get('text', node)\n","            print(f\"   {node_text}: {centrality:.3f}\")\n","\n","        print(f\"\\nTop 5 nodes by betweenness centrality:\")\n","        sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n","        for node, centrality in sorted_betweenness[:5]:\n","            node_text = G.nodes[node].get('text', node)\n","            print(f\"   {node_text}: {centrality:.3f}\")\n","\n","# Create and analyze NetworkX graph\n","nx_graph = create_networkx_graph(linking_results['canonical_entities'], processed_relationships)\n","visualize_graph_structure(nx_graph)\n","analyze_graph_metrics(nx_graph)"],"metadata":{"id":"-oMBM8RkGQuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**### Interactive Graph Visualization**"],"metadata":{"id":"9u69xy5OGdRN"}},{"cell_type":"code","source":["def create_interactive_graph_visualization(G: nx.Graph):\n","    \"\"\"Create an interactive graph visualization using Plotly.\"\"\"\n","\n","    print(\"üñ•Ô∏è Creating interactive graph visualization...\")\n","\n","    # Create layout\n","    pos = nx.spring_layout(G, k=2, iterations=50)\n","\n","    # Prepare edge traces\n","    edge_x = []\n","    edge_y = []\n","    edge_info = []\n","\n","    for edge in G.edges():\n","        x0, y0 = pos[edge[0]]\n","        x1, y1 = pos[edge[1]]\n","        edge_x.extend([x0, x1, None])\n","        edge_y.extend([y0, y1, None])\n","\n","        # Get edge information\n","        edge_data = G.edges[edge]\n","        relationship = edge_data.get('relationship', 'RELATED')\n","        confidence = edge_data.get('confidence', 0.5)\n","        edge_info.append(f\"{relationship} (confidence: {confidence:.2f})\")\n","\n","    edge_trace = go.Scatter(x=edge_x, y=edge_y,\n","                            line=dict(width=1, color='gray'),\n","                            hoverinfo='none',\n","                            mode='lines')\n","\n","    # Prepare node traces by type\n","    entity_types = list(set(nx.get_node_attributes(G, 'type').values()))\n","    colors = px.colors.qualitative.Set3[:len(entity_types)]\n","    type_color_map = dict(zip(entity_types, colors))\n","\n","    node_traces = []\n","\n","    for entity_type in entity_types:\n","        # Get nodes of this type\n","        type_nodes = [node for node in G.nodes()\n","                     if G.nodes[node].get('type') == entity_type]\n","\n","        if not type_nodes:\n","            continue\n","\n","        node_x = [pos[node][0] for node in type_nodes]\n","        node_y = [pos[node][1] for node in type_nodes]\n","\n","        # Create hover text\n","        hover_text = []\n","        for node in type_nodes:\n","            node_data = G.nodes[node]\n","            text = node_data.get('text', node)\n","            variants = node_data.get('variants', 1)\n","            degree = G.degree[node]\n","\n","            hover_info = f\"<b>{text}</b><br>\"\n","            hover_info += f\"Type: {entity_type}<br>\"\n","            hover_info += f\"Variants: {variants}<br>\"\n","            hover_info += f\"Connections: {degree}\"\n","            hover_text.append(hover_info)\n","\n","        # Create trace for this entity type\n","        node_trace = go.Scatter(\n","            x=node_x, y=node_y,\n","            mode='markers+text',\n","            hoverinfo='text',\n","            text=[G.nodes[node].get('text', node)[:10] for node in type_nodes],\n","            textposition=\"middle center\",\n","            hovertext=hover_text,\n","            marker=dict(\n","                size=10,\n","                color=type_color_map[entity_type],\n","                line=dict(width=2, color='white')\n","            ),\n","            name=entity_type\n","        )\n","        node_traces.append(node_trace)\n","\n","    # Create the figure\n","    fig = go.Figure(data=[edge_trace] + node_traces,\n","                   layout=go.Layout(\n","                        title='Interactive Knowledge Graph',\n","                        titlefont_size=16,\n","                        showlegend=True,\n","                        hovermode='closest',\n","                        margin=dict(b=20,l=5,r=5,t=40),\n","                        annotations=[ dict(\n","                            text=\"Hover over nodes for details\",\n","                            showarrow=False,\n","                            xref=\"paper\", yref=\"paper\",\n","                            x=0.005, y=-0.002,\n","                            xanchor=\"left\", yanchor=\"bottom\",\n","                            font=dict(color=\"gray\", size=12)\n","                        )],\n","                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n","                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n","                   )\n","\n","    fig.show()\n","    print(\"‚úÖ Interactive visualization created!\")\n","\n","# Create interactive visualization\n","if nx_graph.number_of_nodes() > 0:\n","    create_interactive_graph_visualization(nx_graph)\n","else:\n","    print(\"‚ö†Ô∏è Graph is empty, skipping interactive visualization\")"],"metadata":{"id":"CpOja0V3GdZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 6: Large-Scale Graph Construction Considerations**"],"metadata":{"id":"OQDmqZh3Gmx0"}},{"cell_type":"code","source":["### Batch Processing for Large Datasets\n","\n","class LargeScaleGraphConstructor:\n","    \"\"\"Handle large-scale graph construction with batching and optimization.\"\"\"\n","\n","    def __init__(self, neo4j_constructor: Neo4jGraphConstructor, batch_size: int = 1000):\n","        self.neo4j = neo4j_constructor\n","        self.batch_size = batch_size\n","        self.construction_stats = {\n","            'entities_processed': 0,\n","            'relationships_processed': 0,\n","            'batches_completed': 0,\n","            'errors': []\n","        }\n","\n","    def construct_graph_in_batches(self, extracted_knowledge: List[Dict],\n","                                  entity_linker: EntityLinker) -> Dict[str, Any]:\n","        \"\"\"Construct graph in batches for large datasets.\"\"\"\n","\n","        print(f\"üèóÔ∏è Starting large-scale graph construction (batch size: {self.batch_size})\")\n","\n","        # Step 1: Process entities in batches\n","        all_entities = []\n","        for doc in extracted_knowledge:\n","            for entity in doc.get('entities', []):\n","                all_entities.append((entity, doc['document_id']))\n","\n","        print(f\"üìä Total entities to process: {len(all_entities)}\")\n","\n","        # Process entities in batches\n","        entity_mapping = {}\n","        canonical_entities = {}\n","\n","        for i in range(0, len(all_entities), self.batch_size):\n","            batch = all_entities[i:i + self.batch_size]\n","            print(f\"   Processing entity batch {i//self.batch_size + 1}\")\n","\n","            # Process this batch through entity linker\n","            batch_docs = [{'entities': [entity for entity, _ in batch]}]\n","            batch_linking = entity_linker.link_entities(batch_docs)\n","\n","            # Merge results\n","            entity_mapping.update(batch_linking['entity_mapping'])\n","            canonical_entities.update(batch_linking['canonical_entities'])\n","\n","            self.construction_stats['entities_processed'] += len(batch)\n","            self.construction_stats['batches_completed'] += 1\n","\n","        # Step 2: Create nodes in batches\n","        self._create_nodes_in_batches(canonical_entities)\n","\n","        # Step 3: Process relationships in batches\n","        all_relationships = []\n","        for doc in extracted_knowledge:\n","            for rel in doc.get('relationships', []):\n","                all_relationships.append((rel, doc['document_id']))\n","\n","        print(f\"üìä Total relationships to process: {len(all_relationships)}\")\n","\n","        # Process relationships in batches\n","        relationship_processor = RelationshipProcessor(entity_mapping)\n","        processed_rels = []\n","\n","        for i in range(0, len(all_relationships), self.batch_size):\n","            batch = all_relationships[i:i + self.batch_size]\n","            print(f\"   Processing relationship batch {i//self.batch_size + 1}\")\n","\n","            for rel, doc_id in batch:\n","                normalized = relationship_processor.normalize_relationship(rel, doc_id)\n","                if normalized:\n","                    processed_rels.append(normalized)\n","\n","            self.construction_stats['relationships_processed'] += len(batch)\n","\n","        # Step 4: Create relationships in batches\n","        self._create_relationships_in_batches(processed_rels)\n","\n","        print(f\"‚úÖ Large-scale construction complete!\")\n","        print(f\"   Entities processed: {self.construction_stats['entities_processed']}\")\n","        print(f\"   Relationships processed: {self.construction_stats['relationships_processed']}\")\n","        print(f\"   Batches completed: {self.construction_stats['batches_completed']}\")\n","\n","        return {\n","            'entity_mapping': entity_mapping,\n","            'canonical_entities': canonical_entities,\n","            'processed_relationships': processed_rels,\n","            'construction_stats': self.construction_stats\n","        }\n","\n","    def _create_nodes_in_batches(self, canonical_entities: Dict):\n","        \"\"\"Create entity nodes in batches.\"\"\"\n","        entities_list = list(canonical_entities.items())\n","\n","        for i in range(0, len(entities_list), self.batch_size):\n","            batch = dict(entities_list[i:i + self.batch_size])\n","            create_entity_nodes(batch, self.neo4j)\n","\n","    def _create_relationships_in_batches(self, relationships: List[Dict]):\n","        \"\"\"Create relationships in batches.\"\"\"\n","        for i in range(0, len(relationships), self.batch_size):\n","            batch = relationships[i:i + self.batch_size]\n","            create_relationships(batch, self.neo4j)\n","\n","# Demonstrate large-scale construction (on our sample data)\n","large_scale_constructor = LargeScaleGraphConstructor(neo4j_constructor, batch_size=10)\n","\n","# Note: For demonstration with small sample data\n","print(\"üìù Large-scale construction demo (with small sample data):\")\n","print(\"   In production, this would handle thousands of entities efficiently\")\n","print(\"   Key benefits: Memory management, progress tracking, error recovery\")"],"metadata":{"id":"ifDqHacqGm6B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 7: Export and Integration**"],"metadata":{"id":"t4-24D4MGzcB"}},{"cell_type":"code","source":["### Export Functions for Other Tools\n","\n","class GraphExporter:\n","    \"\"\"Export constructed knowledge graph to various formats.\"\"\"\n","\n","    def __init__(self, neo4j_constructor: Neo4jGraphConstructor):\n","        self.neo4j = neo4j_constructor\n","\n","    def export_to_csv(self, output_prefix: str = \"knowledge_graph\"):\n","        \"\"\"Export graph data to CSV files.\"\"\"\n","\n","        print(f\"üì§ Exporting graph to CSV files...\")\n","\n","        # Export entities\n","        entities_query = \"\"\"\n","        MATCH (e:Entity)\n","        RETURN e.id as id, e.text as text, e.type as type,\n","               e.variants as variants, size(e.contexts) as context_count\n","        \"\"\"\n","        entities_data = self.neo4j.execute_query(entities_query)\n","\n","        if entities_data and entities_data[0].get('status') != 'simulated':\n","            entities_df = pd.DataFrame(entities_data)\n","            entities_df.to_csv(f\"{output_prefix}_entities.csv\", index=False)\n","            print(f\"   ‚úÖ Exported {len(entities_df)} entities to {output_prefix}_entities.csv\")\n","\n","        # Export relationships\n","        relationships_query = \"\"\"\n","        MATCH (s)-[r]->(t)\n","        RETURN s.text as source, t.text as target, type(r) as relationship,\n","               r.confidence as confidence, r.source_document as source_document\n","        \"\"\"\n","        relationships_data = self.neo4j.execute_query(relationships_query)\n","\n","        if relationships_data and relationships_data[0].get('status') != 'simulated':\n","            relationships_df = pd.DataFrame(relationships_data)\n","            relationships_df.to_csv(f\"{output_prefix}_relationships.csv\", index=False)\n","            print(f\"   ‚úÖ Exported {len(relationships_df)} relationships to {output_prefix}_relationships.csv\")\n","\n","        # Export documents\n","        documents_query = \"\"\"\n","        MATCH (d:Document)\n","        RETURN d.id as id, d.title as title, d.authors as authors,\n","               d.year as year, d.venue as venue\n","        \"\"\"\n","        documents_data = self.neo4j.execute_query(documents_query)\n","\n","        if documents_data and documents_data[0].get('status') != 'simulated':\n","            documents_df = pd.DataFrame(documents_data)\n","            documents_df.to_csv(f\"{output_prefix}_documents.csv\", index=False)\n","            print(f\"   ‚úÖ Exported {len(documents_df)} documents to {output_prefix}_documents.csv\")\n","\n","    def export_for_graph_rag(self, filename: str = \"graph_rag_data.json\"):\n","        \"\"\"Export graph data specifically formatted for Graph RAG systems.\"\"\"\n","\n","        print(f\"üéØ Exporting data for Graph RAG system...\")\n","\n","        # Get all entities with their connections\n","        entities_with_connections_query = \"\"\"\n","        MATCH (e:Entity)\n","        OPTIONAL MATCH (e)-[r]-(connected)\n","        RETURN e.id as id, e.text as text, e.type as type,\n","               count(r) as connection_count,\n","               collect(DISTINCT type(r)) as relationship_types\n","        \"\"\"\n","        entities_data = self.neo4j.execute_query(entities_with_connections_query)\n","\n","        # Get relationship paths for multi-hop reasoning\n","        paths_query = \"\"\"\n","        MATCH path = (s:Entity)-[*1..3]-(t:Entity)\n","        WHERE s <> t\n","        RETURN s.text as start, t.text as end,\n","               [rel in relationships(path) | type(rel)] as path_types,\n","               length(path) as path_length\n","        LIMIT 100\n","        \"\"\"\n","        paths_data = self.neo4j.execute_query(paths_query)\n","\n","        graph_rag_export = {\n","            'entities': entities_data if entities_data[0].get('status') != 'simulated' else [],\n","            'reasoning_paths': paths_data if paths_data[0].get('status') != 'simulated' else [],\n","            'export_metadata': {\n","                'export_timestamp': datetime.now().isoformat(),\n","                'graph_type': 'knowledge_graph',\n","                'intended_use': 'graph_rag_retrieval'\n","            }\n","        }\n","\n","        with open(filename, 'w') as f:\n","            json.dump(graph_rag_export, f, indent=2)\n","\n","        print(f\"‚úÖ Graph RAG data exported to {filename}\")\n","        return graph_rag_export\n","\n","# Export the constructed graph\n","exporter = GraphExporter(neo4j_constructor)\n","exporter.export_to_csv(\"constructed_knowledge_graph\")\n","graph_rag_data = exporter.export_for_graph_rag()"],"metadata":{"id":"8_rMPuKZGzj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**## Part 8: Summary and Next Steps**"],"metadata":{"id":"Mjf-yHaXG-MX"}},{"cell_type":"code","source":["### Construction Pipeline Summary\n","\n","def summarize_graph_construction():\n","    \"\"\"Summarize the graph construction pipeline and results.\"\"\"\n","\n","    print(\"üìã GRAPH CONSTRUCTION PIPELINE SUMMARY\")\n","    print(\"=\"*60)\n","\n","    pipeline_steps = [\n","        \"1. Load extracted knowledge from previous notebook\",\n","        \"2. Entity linking and deduplication\",\n","        \"3. Relationship normalization and processing\",\n","        \"4. Neo4j database population (documents, entities, relationships)\",\n","        \"5. Graph validation and quality assessment\",\n","        \"6. Visualization and analysis\",\n","        \"7. Export for Graph RAG integration\"\n","    ]\n","\n","    print(\"üîÑ Pipeline Steps:\")\n","    for step in pipeline_steps:\n","        print(f\"   {step}\")\n","\n","    print(f\"\\nüìä Construction Results:\")\n","    if linking_results:\n","        print(f\"   Unique Entities: {len(linking_results['canonical_entities'])}\")\n","        print(f\"   Entity Deduplication Ratio: {linking_results['linking_stats']['deduplication_ratio']:.2%}\")\n","\n","    if processed_relationships:\n","        print(f\"   Processed Relationships: {len(processed_relationships)}\")\n","\n","    if validation_results:\n","        print(f\"   Graph Quality Score: {validation_results['quality_score']:.2f}/1.0\")\n","\n","    print(f\"\\nüéØ Key Achievements:\")\n","    achievements = [\n","        \"‚úÖ Automated entity linking and deduplication\",\n","        \"‚úÖ Normalized relationships for consistent graph structure\",\n","        \"‚úÖ Populated Neo4j knowledge graph with validation\",\n","        \"‚úÖ Quality assessment and graph analytics\",\n","        \"‚úÖ Multiple export formats for downstream use\",\n","        \"‚úÖ Scalable pipeline for large document collections\"\n","    ]\n","\n","    for achievement in achievements:\n","        print(f\"   {achievement}\")\n","\n","    print(f\"\\nüöÄ Ready for Next Steps:\")\n","    next_steps = [\n","        \"‚Ä¢ Graph-Enhanced Retrieval (Notebook 12.4)\",\n","        \"‚Ä¢ Multi-hop reasoning implementation\",\n","        \"‚Ä¢ Graph traversal for question answering\",\n","        \"‚Ä¢ End-to-End Graph RAG System (Notebook 12.5)\"\n","    ]\n","\n","    for step in next_steps:\n","        print(f\"   {step}\")\n","\n","    print(\"\\nüí° Production Considerations:\")\n","    considerations = [\n","        \"‚Ä¢ Implement incremental graph updates for new documents\",\n","        \"‚Ä¢ Add graph versioning for reproducibility\",\n","        \"‚Ä¢ Scale Neo4j infrastructure for large datasets\",\n","        \"‚Ä¢ Implement graph backup and recovery procedures\",\n","        \"‚Ä¢ Monitor graph quality metrics over time\"\n","    ]\n","\n","    for consideration in considerations:\n","        print(f\"   {consideration}\")\n","\n","# Run pipeline summary\n","summarize_graph_construction()\n","\n","# Clean up connections\n","neo4j_constructor.close()\n","\n","print(\"\\nüéâ Graph Construction Complete!\")\n","print(\"The knowledge graph is now ready for Graph RAG retrieval in the next notebook.\")"],"metadata":{"id":"2eSf0uAkG-TB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Key Takeaways\n","\n","‚úÖ **Entity Deduplication**: Automated linking reduces redundancy and improves graph quality\n","‚úÖ **Scalable Construction**: Batch processing handles large document collections efficiently  \n","‚úÖ **Quality Validation**: Systematic assessment ensures reliable knowledge graphs\n","‚úÖ **Multiple Exports**: Flexible output formats for various downstream applications\n","‚úÖ **Production Ready**: Designed for real-world deployment with proper error handling\n","\n","Continue to Notebook 12.4 to learn how to implement graph-enhanced retrieval using this constructed knowledge graph!"],"metadata":{"id":"bKDhFMioHIo8"}}]}