{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHLoMGfEoLO4HLtS9/jBbt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7b4df7edbcf34dde886d1d3a1c9e2d02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fffb209100c04086a3708526e7630d52","IPY_MODEL_5dfc514509bf4b13aad0910b37e09d43","IPY_MODEL_c30ace05c8df42e3845493dc5b12c205"],"layout":"IPY_MODEL_8024883fde2c493fa35cc1c4cffc5863"}},"fffb209100c04086a3708526e7630d52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90d02d107c254297b241db3ad7126835","placeholder":"​","style":"IPY_MODEL_5f33174d84ef481ba5bb31aed6815142","value":"README.md: 100%"}},"5dfc514509bf4b13aad0910b37e09d43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23fd66382edf499c926d187844f3ed57","max":7254,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25b46730d55f4fdcaf561dcd5296f9b3","value":7254}},"c30ace05c8df42e3845493dc5b12c205":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4391cf4db6748a8b8998030511c9f15","placeholder":"​","style":"IPY_MODEL_f3da154ca5c54b3d8c14afe7683a3639","value":" 7.25k/7.25k [00:00&lt;00:00, 789kB/s]"}},"8024883fde2c493fa35cc1c4cffc5863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90d02d107c254297b241db3ad7126835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f33174d84ef481ba5bb31aed6815142":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23fd66382edf499c926d187844f3ed57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25b46730d55f4fdcaf561dcd5296f9b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4391cf4db6748a8b8998030511c9f15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3da154ca5c54b3d8c14afe7683a3639":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2752a71b253748559c3e17af75630233":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02ee07e1256e4ea29ebd00b74c85205d","IPY_MODEL_e2bbdf02da4e46ea86c1b62e585d9dbc","IPY_MODEL_a3704cea0492408d97809dfc42142b37"],"layout":"IPY_MODEL_faa478da89af48d28e63828f3c1fff1b"}},"02ee07e1256e4ea29ebd00b74c85205d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad544a6db88f41e69d9b5305f7ff02a6","placeholder":"​","style":"IPY_MODEL_e0d937b665e94330b917a1a0c5dee24d","value":"arxiv_dataset.py: 100%"}},"e2bbdf02da4e46ea86c1b62e585d9dbc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be1723ad1ea1492dbbca65ec5718ff05","max":4672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbf1bf3f4fc34ce58f5b15061874925d","value":4672}},"a3704cea0492408d97809dfc42142b37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd5833f8ec34a87936379922236c197","placeholder":"​","style":"IPY_MODEL_b9a8ce7916a5455ab64fbcf1123a9bb7","value":" 4.67k/4.67k [00:00&lt;00:00, 299kB/s]"}},"faa478da89af48d28e63828f3c1fff1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad544a6db88f41e69d9b5305f7ff02a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0d937b665e94330b917a1a0c5dee24d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be1723ad1ea1492dbbca65ec5718ff05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf1bf3f4fc34ce58f5b15061874925d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfd5833f8ec34a87936379922236c197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9a8ce7916a5455ab64fbcf1123a9bb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["**# Setup and Installation**"],"metadata":{"id":"Zlmq6O12f2gB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5YfehSPfw4K"},"outputs":[],"source":["!pip install langchain langchain_openai faiss-cpu chromadb qdrant-client transformers sentence-transformers datasets matplotlib numpy tqdm\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from tqdm import tqdm\n","from contextlib import contextmanager\n","\n","# Set up for timing measurements\n","@contextmanager\n","def timing(label):\n","    start_time = time.time()\n","    try:\n","        yield\n","    finally:\n","        end_time = time.time()\n","        print(f\"{label}: {end_time - start_time:.3f} seconds\")\n","\n","# Import necessary packages\n","import faiss\n","from langchain.vectorstores import FAISS, Chroma\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.schema import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor\n","# Note: If using a newer version of LangChain, we'll create a custom reranker\n","from langchain.retrievers import MultiQueryRetriever\n","from langchain_openai import OpenAI\n","from datasets import load_dataset\n","\n","# Set up OpenAI API key - You'll need to provide your own API key\n","os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"  # Replace with your API key\n","\n","print(\"Setup complete!\")"]},{"cell_type":"markdown","source":["**Part 1: Generate Sample Data**"],"metadata":{"id":"GKQtW7EDgE-h"}},{"cell_type":"code","source":["# =================================================================\n","# Part 1: Generate Sample Data for Experimentation\n","# =================================================================\n","\n","# We'll use a subset of the arxiv dataset for academic papers\n","print(\"Loading sample dataset...\")\n","\n","try:\n","    # Load a small sample of ArXiv papers\n","    dataset = load_dataset(\"arxiv_dataset\", split=\"train[:100]\")\n","\n","    # Create documents from the dataset\n","    documents = []\n","    for item in dataset:\n","        doc = Document(\n","            page_content=f\"{item['title']}\\n\\n{item['abstract']}\",\n","            metadata={\"title\": item[\"title\"],\n","                     \"authors\": item[\"authors\"],\n","                     \"categories\": item[\"categories\"],\n","                     \"update_date\": item[\"update_date\"]}\n","        )\n","        documents.append(doc)\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    print(\"Creating synthetic dataset instead...\")\n","\n","    # Create synthetic documents about renewable energy\n","    documents = [\n","        Document(\n","            page_content=\"Recent Advances in Perovskite Solar Cells: A Comprehensive Review\\n\\nThis paper presents the latest developments in perovskite solar cell technology, including improvements in efficiency, stability, and manufacturing processes. We discuss how the efficiency of perovskite cells has increased from 3.8% in 2009 to over 25.7% in recent demonstrations, surpassing many conventional silicon technologies. The paper also addresses remaining challenges including long-term stability, lead toxicity concerns, and scalable manufacturing techniques.\",\n","            metadata={\"title\": \"Recent Advances in Perovskite Solar Cells\",\n","                     \"authors\": \"Zhang, J., Williams, R., Johnson, T.\",\n","                     \"categories\": \"Physics, Materials Science\",\n","                     \"update_date\": \"2023-02-15\"}\n","        ),\n","        # Add more synthetic documents (20+ documents for meaningful indexing)\n","    ]\n","\n","    # Generate synthetic documents if needed\n","    topics = [\"Solar Energy\", \"Wind Power\", \"Hydroelectric Systems\", \"Nuclear Fusion\",\n","             \"Battery Storage\", \"Smart Grids\", \"Hydrogen Fuel Cells\", \"Bioenergy\",\n","             \"Geothermal Power\", \"Quantum Computing\", \"Artificial Intelligence\",\n","             \"Climate Modeling\", \"Carbon Capture\", \"Renewable Integration\"]\n","\n","    for i in range(80):\n","        topic = topics[i % len(topics)]\n","        doc = Document(\n","            page_content=f\"Advances in {topic}: New Frontiers and Opportunities\\n\\n\" +\n","                         f\"This research explores the latest developments in {topic} technology, \" +\n","                         f\"with a focus on improving efficiency and reducing costs. \" +\n","                         f\"We present novel approaches that have demonstrated significant \" +\n","                         f\"improvements over traditional methods and discuss potential \" +\n","                         f\"applications in both industrial and consumer settings.\",\n","            metadata={\"title\": f\"Advances in {topic}\",\n","                     \"authors\": f\"Author{i+1}, A., Author{i+2}, B.\",\n","                     \"categories\": \"Energy, Technology\",\n","                     \"update_date\": f\"2023-{(i%12)+1:02d}-{(i%28)+1:02d}\"}\n","        )\n","        documents.append(doc)\n","\n","print(f\"Created dataset with {len(documents)} documents\")\n","\n","# Choose an embedding model\n","# Option A: OpenAI Embeddings (requires API key)\n","try:\n","    embeddings = OpenAIEmbeddings()\n","    print(\"Using OpenAI embeddings\")\n","except Exception as e:\n","    # Option B: Local Hugging Face embeddings (free alternative)\n","    print(f\"Error with OpenAI embeddings: {e}\")\n","    print(\"Falling back to local Hugging Face embeddings\")\n","    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":583,"referenced_widgets":["7b4df7edbcf34dde886d1d3a1c9e2d02","fffb209100c04086a3708526e7630d52","5dfc514509bf4b13aad0910b37e09d43","c30ace05c8df42e3845493dc5b12c205","8024883fde2c493fa35cc1c4cffc5863","90d02d107c254297b241db3ad7126835","5f33174d84ef481ba5bb31aed6815142","23fd66382edf499c926d187844f3ed57","25b46730d55f4fdcaf561dcd5296f9b3","a4391cf4db6748a8b8998030511c9f15","f3da154ca5c54b3d8c14afe7683a3639","2752a71b253748559c3e17af75630233","02ee07e1256e4ea29ebd00b74c85205d","e2bbdf02da4e46ea86c1b62e585d9dbc","a3704cea0492408d97809dfc42142b37","faa478da89af48d28e63828f3c1fff1b","ad544a6db88f41e69d9b5305f7ff02a6","e0d937b665e94330b917a1a0c5dee24d","be1723ad1ea1492dbbca65ec5718ff05","bbf1bf3f4fc34ce58f5b15061874925d","dfd5833f8ec34a87936379922236c197","b9a8ce7916a5455ab64fbcf1123a9bb7"]},"id":"0x7swWdkgFNq","executionInfo":{"status":"ok","timestamp":1740644951809,"user_tz":-330,"elapsed":45967,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"150355fe-d4a4-4812-e22e-aca4a7e95b01"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading sample dataset...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.25k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b4df7edbcf34dde886d1d3a1c9e2d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["arxiv_dataset.py:   0%|          | 0.00/4.67k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2752a71b253748559c3e17af75630233"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The repository for arxiv_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/arxiv_dataset.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","Error loading dataset:                 The dataset arxiv_dataset with config default requires manual data.\n","                Please follow the manual download instructions:\n","                     You need to go to https://www.kaggle.com/Cornell-University/arxiv,\n","and manually download the dataset. Once it is completed,\n","a zip folder named archive.zip will be appeared in your Downloads folder\n","or whichever folder your browser chooses to save files to. Extract that folder\n","and you would get a arxiv-metadata-oai-snapshot.json file\n","You can then move that file under <path/to/folder>.\n","The <path/to/folder> can e.g. be \"~/manual_data\".\n","arxiv_dataset can then be loaded using the following command `datasets.load_dataset(\"arxiv_dataset\", data_dir=\"<path/to/folder>\")`.\n","\n","                Manual data can be loaded with:\n","                 datasets.load_dataset(\"arxiv_dataset\", data_dir=\"<path/to/manual/data>\")\n","Creating synthetic dataset instead...\n","Created dataset with 81 documents\n","Using OpenAI embeddings\n"]}]},{"cell_type":"markdown","source":["**Part 2: Indexing Strategies - HNSW**"],"metadata":{"id":"9Um9-baQgTY_"}},{"cell_type":"code","source":["# =================================================================\n","# Part 2: Indexing Strategies - HNSW\n","# =================================================================\n","print(\"\\n=== Exploring HNSW Indexing ===\")\n","\n","# Create a simple function to generate document vectors for experimenting with FAISS directly\n","def get_vectors(documents, embedding_model):\n","    vectors = []\n","    for doc in tqdm(documents, desc=\"Creating embeddings\"):\n","        vector = embedding_model.embed_query(doc.page_content)\n","        vectors.append(vector)\n","    return np.array(vectors, dtype=np.float32)\n","\n","# Generate vectors\n","try:\n","    vectors = get_vectors(documents[:50], embeddings)  # Using a subset for speed\n","    dimension = vectors.shape[1]\n","    print(f\"Generated {len(vectors)} vectors of dimension {dimension}\")\n","except Exception as e:\n","    print(f\"Error generating vectors: {e}\")\n","    # Create random vectors for demonstration if real embeddings fail\n","    dimension = 384  # Default for many models\n","    vectors = np.random.random((50, dimension)).astype(np.float32)\n","    print(f\"Created random vectors for demonstration: {vectors.shape}\")\n","\n","# Experiment with HNSW parameters\n","print(\"\\nTesting HNSW with different parameters...\")\n","\n","# Define a sample query vector\n","query_vector = vectors[0]  # Using the first vector as a query for demonstration\n","\n","# Test different M values (number of connections)\n","m_values = [4, 16, 32, 64]\n","ef_search_values = [16, 64, 128, 256]\n","\n","# Results storage\n","results = []\n","\n","for m in m_values:\n","    for ef_search in ef_search_values:\n","        try:\n","            # Create an HNSW index\n","            index = faiss.IndexHNSWFlat(dimension, m)\n","            index.hnsw.efConstruction = 200  # Fixed for comparison\n","            index.hnsw.efSearch = ef_search\n","\n","            # Add vectors to the index\n","            with timing(f\"HNSW Build (M={m})\"):\n","                index.add(vectors)\n","\n","            # Test search performance\n","            with timing(f\"HNSW Search (M={m}, efSearch={ef_search})\"):\n","                D, I = index.search(query_vector.reshape(1, -1), 5)\n","\n","            results.append({\n","                'M': m,\n","                'efSearch': ef_search,\n","                'top_indices': I[0],\n","                'distances': D[0]\n","            })\n","\n","            print(f\"  M={m}, efSearch={ef_search}: Top indices: {I[0]}, Distances: {D[0]}\")\n","\n","        except Exception as e:\n","            print(f\"Error with HNSW (M={m}, efSearch={ef_search}): {e}\")"],"metadata":{"id":"jvCLbr5ogTvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 3: Indexing Strategies - IVF**"],"metadata":{"id":"I64emZ2EkK4T"}},{"cell_type":"code","source":["# =================================================================\n","# Part 3: Indexing Strategies - IVF\n","# =================================================================\n","print(\"\\n=== Exploring IVF Indexing ===\")\n","\n","# Test different numbers of clusters\n","n_clusters_values = [4, 10, 20, 50]\n","nprobe_values = [1, 5, 10, 20]\n","\n","for n_clusters in n_clusters_values:\n","    try:\n","        # Create quantizer\n","        quantizer = faiss.IndexFlatL2(dimension)\n","\n","        # Create an IVF index\n","        index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)\n","\n","        # Train the index\n","        with timing(f\"IVF Training (clusters={n_clusters})\"):\n","            index.train(vectors)\n","\n","        # Add vectors to the index\n","        with timing(f\"IVF Add (clusters={n_clusters})\"):\n","            index.add(vectors)\n","\n","        # Test different nprobe values\n","        for nprobe in nprobe_values:\n","            index.nprobe = nprobe\n","\n","            # Search\n","            with timing(f\"IVF Search (clusters={n_clusters}, nprobe={nprobe})\"):\n","                D, I = index.search(query_vector.reshape(1, -1), 5)\n","\n","            print(f\"  Clusters={n_clusters}, nprobe={nprobe}: Top indices: {I[0]}, Distances: {D[0]}\")\n","\n","    except Exception as e:\n","        print(f\"Error with IVF (clusters={n_clusters}): {e}\")"],"metadata":{"id":"-HBOopV0kLD_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 4: Vector Compression Techniques**"],"metadata":{"id":"Sjpzj0FhkWR8"}},{"cell_type":"code","source":["# =================================================================\n","# Part 4: Vector Compression Techniques\n","# =================================================================\n","print(\"\\n=== Exploring Vector Compression ===\")\n","\n","try:\n","    # Original size benchmark\n","    original_size = vectors.nbytes / 1024\n","    print(f\"Original vectors size: {original_size:.2f} KB\")\n","\n","    # Scalar Quantization (8-bit)\n","    index_8bit = faiss.IndexScalarQuantizer(dimension, faiss.ScalarQuantizer.QT_8bit)\n","    with timing(\"8-bit Scalar Quantization\"):\n","        index_8bit.add(vectors)\n","\n","    # Scalar Quantization (4-bit)\n","    index_4bit = faiss.IndexScalarQuantizer(dimension, faiss.ScalarQuantizer.QT_4bit)\n","    with timing(\"4-bit Scalar Quantization\"):\n","        index_4bit.add(vectors)\n","\n","    # Product Quantization\n","    # m must divide dimension evenly, adjust if needed\n","    m = 8\n","    while dimension % m != 0:\n","        m -= 1\n","\n","    index_pq = faiss.IndexPQ(dimension, m, 8)  # m subquantizers with 8 bits each\n","    with timing(f\"Product Quantization (m={m})\"):\n","        index_pq.train(vectors)\n","        index_pq.add(vectors)\n","\n","    # Compare accuracy of different compression methods\n","    print(\"\\nAccuracy Comparison:\")\n","\n","    # Ground truth (using exact search)\n","    index_flat = faiss.IndexFlatL2(dimension)\n","    index_flat.add(vectors)\n","    D_ref, I_ref = index_flat.search(query_vector.reshape(1, -1), 5)\n","\n","    # Results with 8-bit quantization\n","    D_8bit, I_8bit = index_8bit.search(query_vector.reshape(1, -1), 5)\n","\n","    # Results with 4-bit quantization\n","    D_4bit, I_4bit = index_4bit.search(query_vector.reshape(1, -1), 5)\n","\n","    # Results with PQ\n","    D_pq, I_pq = index_pq.search(query_vector.reshape(1, -1), 5)\n","\n","    print(f\"Reference results: {I_ref[0]}\")\n","    print(f\"8-bit quant results: {I_8bit[0]} (matching: {np.sum(np.isin(I_8bit[0], I_ref[0]))})\")\n","    print(f\"4-bit quant results: {I_4bit[0]} (matching: {np.sum(np.isin(I_4bit[0], I_ref[0]))})\")\n","    print(f\"PQ results: {I_pq[0]} (matching: {np.sum(np.isin(I_pq[0], I_ref[0]))})\")\n","\n","    # Estimated size comparison\n","    print(\"\\nMemory Usage Comparison:\")\n","    print(f\"Original (32-bit float): {original_size:.2f} KB\")\n","    print(f\"8-bit quantization: ~{original_size/4:.2f} KB\")\n","    print(f\"4-bit quantization: ~{original_size/8:.2f} KB\")\n","    print(f\"Product Quantization: ~{len(vectors) * m:.2f} KB\")\n","\n","except Exception as e:\n","    print(f\"Error in compression experiments: {e}\")"],"metadata":{"id":"FQv8ztkCkWaw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 5: Semantic Chunking**"],"metadata":{"id":"3htWnMiWkgPJ"}},{"cell_type":"code","source":["# =================================================================\n","# Part 5: Semantic Chunking\n","# =================================================================\n","print(\"\\n=== Exploring Semantic Chunking ===\")\n","\n","# Create a sample document with clear structure for demonstration\n","sample_document = Document(\n","    page_content=\"\"\"# Introduction to Renewable Energy\n","\n","## Solar Power\n","Solar power is a clean, renewable energy source that harnesses energy from the sun.\n","Photovoltaic cells convert sunlight directly into electricity.\n","Solar thermal systems use the sun's heat for water heating or power generation.\n","\n","## Wind Energy\n","Wind turbines convert the kinetic energy of wind into mechanical power.\n","This mechanical power can be used for specific tasks or converted into electricity.\n","Offshore wind farms can take advantage of strong, consistent ocean winds.\n","\n","## Hydroelectric Power\n","Hydroelectric power captures energy from flowing water.\n","Large dams create reservoirs, releasing water through turbines to generate electricity.\n","Run-of-river systems use natural river flow with minimal environmental impact.\n","\n","### Micro-Hydro Systems\n","Smaller hydroelectric systems can power individual communities.\n","These systems have minimal environmental impact and are suitable for remote areas.\n","\n","## Conclusion\n","Renewable energy sources provide sustainable alternatives to fossil fuels.\n","Continued innovation will increase efficiency and reduce costs.\n","\"\"\",\n","    metadata={\"title\": \"Introduction to Renewable Energy\"}\n",")\n","\n","# Compare different chunking strategies\n","print(\"\\nComparing different chunking approaches:\")\n","\n","# Basic character splitter\n","basic_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=200,\n","    chunk_overlap=0,\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",")\n","\n","# Semantic splitter with hierarchy\n","semantic_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=200,\n","    chunk_overlap=50,\n","    separators=[\"# \", \"## \", \"### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",")\n","\n","# Apply both splitting strategies\n","basic_chunks = basic_splitter.split_documents([sample_document])\n","semantic_chunks = semantic_splitter.split_documents([sample_document])\n","\n","print(f\"\\nBasic splitting created {len(basic_chunks)} chunks\")\n","print(f\"Semantic splitting created {len(semantic_chunks)} chunks\")\n","\n","# Display examples of both chunking methods\n","print(\"\\nBasic Chunking Example:\")\n","for i, chunk in enumerate(basic_chunks[:3]):  # Show first 3 chunks\n","    print(f\"Chunk {i+1}: {chunk.page_content[:100]}...\")\n","\n","print(\"\\nSemantic Chunking Example:\")\n","for i, chunk in enumerate(semantic_chunks[:3]):  # Show first 3 chunks\n","    print(f\"Chunk {i+1}: {chunk.page_content[:100]}...\")"],"metadata":{"id":"h2UC5Eyekgbj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 6: Re-ranking with Contextual Compression**"],"metadata":{"id":"1kbOxnAqkpBz"}},{"cell_type":"code","source":["# =================================================================\n","# Part 6: Re-ranking with Contextual Compression\n","# =================================================================\n","print(\"\\n=== Exploring Re-ranking ===\")\n","\n","# Create a simple vector store with our documents for retrieval experiments\n","try:\n","    vectorstore = FAISS.from_documents(documents, embeddings)\n","    print(f\"Created FAISS vectorstore with {len(documents)} documents\")\n","\n","    # Define a test query\n","    test_query = \"advances in solar cell efficiency and materials\"\n","\n","    # Standard retrieval\n","    with timing(\"Standard retrieval\"):\n","        standard_results = vectorstore.similarity_search(test_query, k=5)\n","\n","    print(\"\\nStandard Retrieval Results:\")\n","    for i, doc in enumerate(standard_results):\n","        print(f\"{i+1}. {doc.metadata['title']}\")\n","\n","    # Create LLM re-ranker\n","    try:\n","        llm = OpenAI(temperature=0)\n","        reranker = LLMRerank.from_llm(\n","            llm=llm,\n","            k=5  # Number of documents to return after re-ranking\n","        )\n","\n","        rerank_retriever = ContextualCompressionRetriever(\n","            base_compressor=reranker,\n","            base_retriever=vectorstore.as_retriever(\n","                search_kwargs={\"k\": 10}  # Retrieve more docs initially for re-ranking\n","            )\n","        )\n","\n","        # Get re-ranked documents\n","        with timing(\"LLM re-ranking\"):\n","            reranked_docs = compression_retriever.get_relevant_documents(test_query)\n","\n","        print(\"\\nContextually Compressed/Extracted Results:\")\n","        for i, doc in enumerate(reranked_docs):\n","            print(f\"{i+1}. {doc.metadata['title']}\")\n","\n","    except Exception as e:\n","        print(f\"Error with LLM re-ranking (may require API key): {e}\")\n","        print(\"Skipping LLM-based re-ranking demonstration\")\n","\n","    # Multi-Query Retrieval demonstration\n","    try:\n","        # Create a retriever that generates multiple query variations\n","        multi_query_retriever = MultiQueryRetriever.from_llm(\n","            retriever=vectorstore.as_retriever(),\n","            llm=OpenAI()\n","        )\n","\n","        # The retriever will generate variations of the query,\n","        # perform separate searches, and combine the results\n","        with timing(\"Multi-query retrieval\"):\n","            multi_query_results = multi_query_retriever.get_relevant_documents(\n","                \"How do newer types of solar cells compare to silicon?\"\n","            )\n","\n","        print(\"\\nMulti-Query Retrieval Results:\")\n","        for i, doc in enumerate(multi_query_results[:5]):  # Display top 5\n","            print(f\"{i+1}. {doc.metadata['title']}\")\n","\n","    except Exception as e:\n","        print(f\"Error with multi-query retrieval (may require API key): {e}\")\n","        print(\"Skipping multi-query retrieval demonstration\")\n","\n","except Exception as e:\n","    print(f\"Error in retrieval experiments: {e}\")\n"],"metadata":{"id":"b3kwW3E0kpIo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Part 7: Performance Analysis and Visualization**"],"metadata":{"id":"5BxHCizCkxzt"}},{"cell_type":"code","source":["# =================================================================\n","# Part 7: Performance Analysis and Visualization\n","# =================================================================\n","print(\"\\n=== Performance Analysis and Visualization ===\")\n","\n","try:\n","    # Prepare data for visualization from our experiments\n","    print(\"Generating performance visualizations...\")\n","\n","    # Example 1: HNSW parameter impact\n","    plt.figure(figsize=(10, 6))\n","\n","    # Create synthetic data based on theoretical performance\n","    m_values_extended = [4, 8, 16, 32, 64, 128]\n","    times_relative = [0.8, 0.85, 1.0, 1.3, 1.8, 2.5]  # Relative search times\n","    accuracy_relative = [0.6, 0.75, 0.88, 0.95, 0.98, 0.99]  # Relative accuracy\n","\n","    plt.plot(m_values_extended, times_relative, 'o-', label='Relative Search Time')\n","    plt.plot(m_values_extended, accuracy_relative, 's-', label='Relative Accuracy')\n","    plt.xlabel('M (connections per layer)')\n","    plt.ylabel('Relative Performance')\n","    plt.title('Impact of HNSW Parameters on Search Performance')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('hnsw_performance.png')\n","\n","    # Example 2: Vector Compression Methods\n","    plt.figure(figsize=(10, 6))\n","\n","    # Theoretical data\n","    compression_methods = ['32-bit Float', '8-bit Quant', '4-bit Quant', 'PQ (m=8)']\n","    memory_usage = [100, 25, 12.5, 3]  # Percentage of original memory\n","    accuracy = [100, 95, 85, 75]  # Percentage of original accuracy\n","\n","    x = range(len(compression_methods))\n","    width = 0.35\n","\n","    plt.bar(x, memory_usage, width, label='Memory Usage (%)')\n","    plt.bar([i + width for i in x], accuracy, width, label='Accuracy (%)')\n","\n","    plt.xlabel('Compression Method')\n","    plt.ylabel('Percentage')\n","    plt.title('Trade-off: Memory Usage vs. Accuracy')\n","    plt.xticks([i + width/2 for i in x], compression_methods)\n","    plt.legend()\n","    plt.grid(True, axis='y')\n","    plt.savefig('compression_tradeoff.png')\n","\n","    print(\"Visualizations created: hnsw_performance.png and compression_tradeoff.png\")\n","\n","except Exception as e:\n","    print(f\"Error creating visualizations: {e}\")\n","\n","print(\"\\nNotebook execution complete!\")\n","\n","# Note: Some sections of this notebook require API keys and may be skipped if they are not provided.\n","# The core concepts and techniques are demonstrated regardless of API availability."],"metadata":{"id":"rn25q9wSkx8q"},"execution_count":null,"outputs":[]}]}