{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18xAJGtdZMnZp4KKMhwBV0NV9gwDpYrNH","timestamp":1743601581133}],"authorship_tag":"ABX9TyPB7o+p5R7xEJ2UQK7cY8k8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"4ALfxbtRqpZW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxeIjFubqLt0"},"outputs":[],"source":["# Install necessary packages\n","!pip install langchain langchain-openai langchain-community chromadb\n","\n","import os\n","import time\n","from typing import List\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set your OpenAI API key (in Colab, you should use secrets or environment variables)\n","# Option 1: Use this if you want to enter your key interactively\n","from getpass import getpass\n","OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n","\n","# Option 2: Uncomment and use this if you prefer (less secure but more convenient)\n","# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"  # Replace with your actual key\n","\n","# Import necessary components\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_community.vectorstores import Chroma\n","from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n","from langchain.memory import ConversationBufferMemory\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_core.documents import Document\n","from langchain.prompts import ChatPromptTemplate, PromptTemplate\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnablePassthrough\n","\n","print(\"Setup complete!\")"]},{"cell_type":"markdown","source":["**Create a Sample Knowledge Base**"],"metadata":{"id":"9fdJ4VKKrn8j"}},{"cell_type":"code","source":["documents = [\n","    Document(\n","        page_content=\"Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches in natural language processing. It enhances large language models by retrieving relevant information from external knowledge sources before generating a response.\",\n","        metadata={\"source\": \"introduction_to_rag.pdf\", \"page\": 1}\n","    ),\n","    Document(\n","        page_content=\"The key components of a RAG system include document loaders, text splitters, embedding models, vector stores, retrievers, and prompt templates. Document loaders ingest content from various sources. Text splitters segment documents into manageable chunks for embedding and retrieval.\",\n","        metadata={\"source\": \"rag_components.pdf\", \"page\": 5}\n","    ),\n","    Document(\n","        page_content=\"Embedding models in RAG convert text into mathematical vectors that capture semantic meaning. These models transform words and documents into numerical representations that enable semantic search and retrieval based on meaning rather than keywords.\",\n","        metadata={\"source\": \"embedding_models.pdf\", \"page\": 12}\n","    ),\n","    Document(\n","        page_content=\"Vector stores organize embedded documents for efficient retrieval. They index vector representations and support different search algorithms, from exact nearest neighbors to approximate methods that trade some accuracy for faster query speed.\",\n","        metadata={\"source\": \"vector_stores.pdf\", \"page\": 18}\n","    ),\n","    Document(\n","        page_content=\"Retrievers in RAG systems are responsible for finding the most relevant information from a knowledge base. They can use various strategies from simple vector similarity to hybrid approaches combining semantic and keyword search.\",\n","        metadata={\"source\": \"retrievers.pdf\", \"page\": 24}\n","    ),\n","    Document(\n","        page_content=\"Prompt templates structure interactions with language models in RAG systems. They provide a consistent format for combining user queries with retrieved context to generate accurate and contextually appropriate responses.\",\n","        metadata={\"source\": \"prompt_templates.pdf\", \"page\": 31}\n","    ),\n","    Document(\n","        page_content=\"RAG offers several advantages over traditional LLMs. It provides more accurate and up-to-date information, reduces hallucinations, enables source attribution, and allows for domain-specific knowledge without retraining the base model.\",\n","        metadata={\"source\": \"rag_advantages.pdf\", \"page\": 7}\n","    ),\n","    Document(\n","        page_content=\"Challenges in RAG implementation include retrieval quality, context window limitations, maintaining freshness of knowledge, and balancing retrieval with generation. Effective RAG systems require careful tuning of both retrieval and generation components.\",\n","        metadata={\"source\": \"rag_challenges.pdf\", \"page\": 9}\n","    )\n","]"],"metadata":{"id":"oTK5i4oYroGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create embeddings and vector store**"],"metadata":{"id":"Dcxt9nYOrzJU"}},{"cell_type":"code","source":["embeddings = OpenAIEmbeddings()\n","vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n","\n","print(f\"Created vector store with {len(documents)} documents\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02vBrlMJrzTF","executionInfo":{"status":"ok","timestamp":1743600505559,"user_tz":-330,"elapsed":5611,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"150b49f5-5919-4919-f0e0-15b1d01c3751"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created vector store with 8 documents\n"]}]},{"cell_type":"markdown","source":["**10.2.1 Incorporating Chat History for Contextual Understanding**\n","\n","---\n","\n","\n","**Basic Implementation with ConversationBufferMemory**\n","\n"],"metadata":{"id":"vxruvxGcsJmP"}},{"cell_type":"code","source":["# Initialize memory to store conversation history\n","memory = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","# Create a conversational retrieval chain with memory\n","qa_chain = ConversationalRetrievalChain.from_llm(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    retriever=vectorstore.as_retriever(),\n","    memory=memory\n",")\n","\n","# First question with no prior context\n","first_response = qa_chain.invoke({\"question\": \"What is RAG?\"})\n","print(\"First Question: What is RAG?\")\n","print(f\"Response: {first_response['answer']}\\n\")\n","\n","# Follow-up question that relies on conversation history\n","follow_up_response = qa_chain.invoke({\"question\": \"What are its advantages?\"})\n","print(\"Follow-up Question: What are its advantages?\")\n","print(f\"Response: {follow_up_response['answer']}\\n\")"],"metadata":{"id":"QS_yqqnwsgV_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Fixed-size Window for Chat History**"],"metadata":{"id":"iaOs-0HJsrUn"}},{"cell_type":"code","source":["# Initialize memory with a fixed-size window\n","windowed_memory = ConversationBufferWindowMemory(\n","    memory_key=\"chat_history\",\n","    k=3,  # Number of exchanges to keep\n","    return_messages=True\n",")\n","\n","# Create a conversation chain with windowed memory\n","windowed_qa_chain = ConversationalRetrievalChain.from_llm(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    retriever=vectorstore.as_retriever(),\n","    memory=windowed_memory\n",")\n","\n","# Demonstrate a conversation with memory window\n","print(\"Using ConversationBufferWindowMemory (keeping last 3 exchanges):\")\n","\n","questions = [\n","    \"What are the key components of RAG?\",\n","    \"Can you tell me more about embedding models?\",\n","    \"How do vector stores work?\",\n","    \"What about retrievers?\",\n","    \"And what role do prompt templates play?\",\n","    \"What challenges exist with these systems?\"\n","]\n","\n","for i, question in enumerate(questions):\n","    response = windowed_qa_chain.invoke({\"question\": question})\n","    print(f\"Q{i+1}: {question}\")\n","    print(f\"A{i+1}: {response['answer'][:150]}...\\n\")\n","\n","    # Show current memory size after each exchange\n","    memory_messages = windowed_memory.load_memory_variables({})[\"chat_history\"]\n","    print(f\"Memory now contains {len(memory_messages)//2} exchanges\\n\")"],"metadata":{"id":"KJNTX35Asrh_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summarization Approach for Memory**"],"metadata":{"id":"C875-DzotAgV"}},{"cell_type":"code","source":["# Memory that uses an LLM to summarize conversation history\n","summary_memory = ConversationSummaryMemory(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")\n","\n","# Create a conversation chain with summary memory\n","summary_qa_chain = ConversationalRetrievalChain.from_llm(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    retriever=vectorstore.as_retriever(),\n","    memory=summary_memory\n",")\n","\n","# Demonstrate a conversation with summarized memory\n","print(\"Using ConversationSummaryMemory:\")\n","summary_questions = [\n","    \"What is RAG and what are its main components?\",\n","    \"How do these components work together in a complete system?\",\n","    \"What are the advantages of RAG over traditional LLMs?\",\n","    \"What challenges do RAG systems face?\"\n","]\n","\n","for i, question in enumerate(summary_questions):\n","    response = summary_qa_chain.invoke({\"question\": question})\n","    print(f\"Q{i+1}: {question}\")\n","    print(f\"A{i+1}: {response['answer'][:150]}...\\n\")\n","\n","    # Access the current summary\n","    if i < len(summary_questions) - 1:  # Skip after last question to avoid extra API call\n","        current_summary = summary_memory.predict_new_summary(\n","            summary_memory.chat_memory.messages,\n","            \"\"\n","        )\n","        print(f\"Current conversation summary: {current_summary[:200]}...\\n\")"],"metadata":{"id":"WkaO9jOhtApC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.2.2 Streaming Responses for Better User Experience**\n","\n","---\n","\n","**Basic Streaming Implementation**\n"],"metadata":{"id":"e9IrTql9tRkp"}},{"cell_type":"code","source":["# Initialize a streaming-enabled LLM\n","streaming_llm = ChatOpenAI(\n","    model=\"gpt-3.5-turbo\",\n","    temperature=0,\n","    streaming=True,\n","    callbacks=[StreamingStdOutCallbackHandler()]\n",")\n","\n","# Create a template for RAG\n","template = \"\"\"Answer the question based on the following context:\n","Context: {context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# Define function to format documents into context string\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","# Create a streaming RAG chain\n","streaming_chain = (\n","    {\"context\": vectorstore.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | streaming_llm\n",")\n","\n","print(\"Streaming response (you'll see the answer appear word by word):\")\n","# Invoke with streaming output (will print to console)\n","_ = streaming_chain.invoke(\"What are the advantages of RAG systems?\")\n","print(\"\\nStreaming complete!\")"],"metadata":{"id":"y3YBfe0BtR0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Phased Streaming with Retrieval Feedback**"],"metadata":{"id":"ggxypd_utdEq"}},{"cell_type":"code","source":["def stream_retrieval_and_generation(query: str, retriever, llm, prompt):\n","    \"\"\"\n","    Demonstrate a phased approach to streaming that shows the retrieval process.\n","    \"\"\"\n","    # Phase 1: Notify user about search\n","    print(\"Searching knowledge base...\\n\")\n","\n","    # Phase 2: Retrieve documents with visual feedback\n","    start_time = time.time()\n","    retrieved_docs = retriever.get_relevant_documents(query)\n","    retrieval_time = time.time() - start_time\n","\n","    print(f\"Found {len(retrieved_docs)} relevant documents in {retrieval_time:.2f} seconds:\")\n","    for i, doc in enumerate(retrieved_docs, 1):\n","        preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n","        print(f\"Document {i}: {preview}\\n\")\n","\n","    # Phase 3: Generate and stream the response\n","    print(\"\\nGenerating response:\")\n","\n","    # Prepare context from retrieved documents\n","    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n","\n","    # Create and invoke the streaming chain\n","    chain = prompt | llm\n","    _ = chain.invoke({\"context\": context, \"question\": query})\n","\n","    return \"Response generation complete!\"\n","\n","# Test the phased streaming approach\n","query = \"How do embedding models and vector stores work together in RAG?\"\n","prompt_template = ChatPromptTemplate.from_template(template)\n","\n","print(\"Demonstrating phased streaming with retrieval feedback:\")\n","stream_retrieval_and_generation(\n","    query,\n","    vectorstore.as_retriever(),\n","    streaming_llm,\n","    prompt_template\n",")"],"metadata":{"id":"YDUl4Yw2tdOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.2.3 Source Attribution and Transparency**\n","\n","---\n","\n","**Basic Source Attribution**"],"metadata":{"id":"9UVoKihLtoJ9"}},{"cell_type":"code","source":["# Create a QA chain that returns source documents\n","qa_with_sources = RetrievalQA.from_chain_type(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    chain_type=\"stuff\",\n","    retriever=vectorstore.as_retriever(),\n","    return_source_documents=True\n",")\n","\n","# Query the system\n","result = qa_with_sources({\"query\": \"What are the components of a RAG system?\"})\n","\n","# Access both the answer and the sources\n","answer = result[\"result\"]\n","source_documents = result[\"source_documents\"]\n","\n","# Display answer and sources\n","print(f\"Answer: {answer}\\n\")\n","print(\"Sources:\")\n","for i, doc in enumerate(source_documents, 1):\n","    print(f\"Source {i}: {doc.metadata.get('source', 'Unknown')}, Page {doc.metadata.get('page', 'Unknown')}\")\n","    print(f\"Content snippet: {doc.page_content[:150]}...\\n\")"],"metadata":{"id":"0kHBk52jtoTk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In-text Citations**"],"metadata":{"id":"7gTDB7NCt25X"}},{"cell_type":"code","source":["citation_prompt = \"\"\"\n","Answer the question based solely on the following context.\n","Use [doc1], [doc2], etc. to indicate which document supports each part of your answer.\n","\n","Context:\n","{context}\n","\n","Question: {question}\n","\n","Answer with citations:\n","\"\"\"\n","\n","# Modify the documents to include their index\n","def add_index_to_docs(docs):\n","    doc_string = \"\"\n","    for i, doc in enumerate(docs, 1):\n","        doc_string += f\"[doc{i}]: {doc.page_content}\\n\\n\"\n","    return doc_string\n","\n","# Create the chain with citation instructions\n","citation_chain = (\n","    {\"context\": vectorstore.as_retriever() | add_index_to_docs, \"question\": RunnablePassthrough()}\n","    | ChatPromptTemplate.from_template(citation_prompt)\n","    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",")\n","\n","# Query with citations\n","print(\"Response with in-text citations:\")\n","response = citation_chain.invoke(\"What are the advantages of RAG?\")\n","print(response.content)"],"metadata":{"id":"94b9QoOqt3B0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Building a Citation Lookup System**"],"metadata":{"id":"mb3xZcHzuFvO"}},{"cell_type":"code","source":["def create_response_with_clickable_citations(query, response, source_docs):\n","    \"\"\"\n","    Create a response with clickable citations that map to source documents.\n","    This simulates what would happen in a web interface.\n","    \"\"\"\n","    import re\n","\n","    # Extract citations like [doc1], [doc2] from the response\n","    citations = re.findall(r'\\[doc(\\d+)\\]', response)\n","\n","    # Create a mapping of citation numbers to documents\n","    citation_map = {}\n","    for citation in citations:\n","        doc_num = int(citation)\n","        if doc_num <= len(source_docs):\n","            doc = source_docs[doc_num-1]\n","            citation_map[f\"doc{doc_num}\"] = {\n","                \"content\": doc.page_content,\n","                \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n","                \"page\": doc.metadata.get(\"page\", \"\")\n","            }\n","\n","    # In a real application, you'd return this for the frontend to use\n","    return {\n","        \"response\": response,\n","        \"citations\": citation_map\n","    }\n","\n","# Get documents from the retriever\n","docs = vectorstore.as_retriever().get_relevant_documents(\"What are the key components and advantages of RAG?\")\n","\n","# Get a response with citations\n","raw_response = citation_chain.invoke(\"What are the key components and advantages of RAG?\")\n","\n","# Create clickable citations\n","result = create_response_with_clickable_citations(\n","    \"What are the key components and advantages of RAG?\",\n","    raw_response.content,\n","    docs\n",")\n","\n","# Simulate how this would work in a web interface\n","print(\"RESPONSE WITH CLICKABLE CITATIONS:\")\n","print(result[\"response\"])\n","print(\"\\nWhen a citation is clicked, it would show:\")\n","for citation, source in result[\"citations\"].items():\n","    print(f\"\\n--- {citation} ---\")\n","    print(f\"Source: {source['source']}, Page: {source['page']}\")\n","    print(f\"Content: {source['content']}\")"],"metadata":{"id":"7SBKwIjnuF48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.2.4 User-Specific Retrieval Patterns**\n","\n","---\n","\n","**Query Augmentation with User Context**"],"metadata":{"id":"_2I7yHb8uQa2"}},{"cell_type":"code","source":["def create_personalized_query(query: str, user_profile: dict) -> str:\n","    \"\"\"Augment the query with relevant user context.\"\"\"\n","    # Extract relevant user attributes based on the query topic\n","    relevant_attributes = []\n","\n","    if \"investment\" in query.lower() or \"financial\" in query.lower():\n","        if \"age\" in user_profile:\n","            relevant_attributes.append(f\"age {user_profile['age']}\")\n","        if \"risk_tolerance\" in user_profile:\n","            relevant_attributes.append(f\"risk tolerance: {user_profile['risk_tolerance']}\")\n","\n","    elif \"health\" in query.lower() or \"medical\" in query.lower():\n","        if \"medical_conditions\" in user_profile:\n","            conditions = user_profile[\"medical_conditions\"]\n","            relevant_attributes.append(f\"medical conditions: {', '.join(conditions)}\")\n","\n","    # If we have relevant attributes, include them in the query\n","    if relevant_attributes:\n","        augmented_query = f\"{query} [For a person with {'; '.join(relevant_attributes)}]\"\n","        return augmented_query\n","\n","    # If no relevant attributes, return the original query\n","    return query\n","\n","# Example usage\n","user_profile = {\n","    \"age\": 65,\n","    \"risk_tolerance\": \"conservative\",\n","    \"medical_conditions\": [\"hypertension\", \"type 2 diabetes\"]\n","}\n","\n","original_query = \"What investment strategies should I consider?\"\n","personalized_query = create_personalized_query(original_query, user_profile)\n","\n","print(f\"Original: {original_query}\")\n","print(f\"Personalized: {personalized_query}\")\n","\n","# Create another example with health-related query\n","health_query = \"What exercises are recommended for maintaining health?\"\n","personalized_health_query = create_personalized_query(health_query, user_profile)\n","\n","print(f\"\\nOriginal: {health_query}\")\n","print(f\"Personalized: {personalized_health_query}\")"],"metadata":{"id":"Ag2UO2OOuQjU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Metadata Filtering by User Attributes**"],"metadata":{"id":"-S0CCJYvuf5x"}},{"cell_type":"code","source":["# In a real application, documents would have metadata like age_group, risk_profile, etc.\n","# For demonstration purposes, we'll add this metadata\n","\n","# Add more detailed metadata to our documents\n","documents_with_metadata = [\n","    Document(\n","        page_content=\"For young investors (20-35), a growth-focused strategy with higher equity allocation is often recommended. This typically involves 80-90% stocks with an emphasis on emerging markets and innovative sectors.\",\n","        metadata={\"source\": \"investment_strategies.pdf\", \"page\": 15, \"age_group\": \"young_adult\", \"risk_profile\": \"aggressive\"}\n","    ),\n","    Document(\n","        page_content=\"Middle-aged investors (35-60) should consider a balanced approach with a moderate allocation to both stocks and bonds. A typical allocation might be 60% stocks and 40% bonds, adjusting based on personal risk tolerance.\",\n","        metadata={\"source\": \"investment_strategies.pdf\", \"page\": 18, \"age_group\": \"middle_aged\", \"risk_profile\": \"moderate\"}\n","    ),\n","    Document(\n","        page_content=\"Senior investors (60+) with a conservative risk profile should focus on capital preservation and income generation. A portfolio with 30-40% stocks and 60-70% bonds, with an emphasis on dividend stocks and high-quality bonds is often appropriate.\",\n","        metadata={\"source\": \"investment_strategies.pdf\", \"page\": 22, \"age_group\": \"senior\", \"risk_profile\": \"conservative\"}\n","    ),\n","    Document(\n","        page_content=\"For those with high risk tolerance regardless of age, a higher allocation to small cap stocks and emerging markets can potentially yield greater returns, though with increased volatility.\",\n","        metadata={\"source\": \"investment_strategies.pdf\", \"page\": 25, \"age_group\": \"any\", \"risk_profile\": \"aggressive\"}\n","    ),\n","    Document(\n","        page_content=\"Conservative investors should prioritize high-quality corporate bonds, Treasury securities, and blue-chip dividend stocks to minimize volatility while generating stable income.\",\n","        metadata={\"source\": \"investment_strategies.pdf\", \"page\": 27, \"age_group\": \"any\", \"risk_profile\": \"conservative\"}\n","    ),\n","]\n","\n","# Create a new vector store with the metadata-rich documents\n","metadata_vectorstore = Chroma.from_documents(documents=documents_with_metadata, embedding=embeddings)\n","\n","def create_user_specific_filters(user_profile: dict) -> dict:\n","    \"\"\"Create metadata filters based on user profile.\"\"\"\n","    # For Chroma with the basic configuration, we need a simpler filter structure\n","    # We'll handle one filter at a time for compatibility\n","\n","    # Add age range filter if user age is available\n","    if \"age\" in user_profile:\n","        age = user_profile[\"age\"]\n","        if age < 35:\n","            return {\"age_group\": \"young_adult\"}\n","        elif age < 60:\n","            return {\"age_group\": \"middle_aged\"}\n","        else:\n","            return {\"age_group\": \"senior\"}\n","\n","    # Add risk profile filter if available\n","    if \"risk_tolerance\" in user_profile:\n","        return {\"risk_profile\": user_profile[\"risk_tolerance\"]}\n","\n","    return {}\n","\n","# Create a filtered retriever based on age (senior)\n","age_filter = {\"age_group\": {\"$eq\": \"senior\"}}\n","print(f\"Using age filter: {age_filter}\")\n","\n","age_filtered_retriever = metadata_vectorstore.as_retriever(\n","    search_kwargs={\"k\": 2, \"filter\": age_filter}\n",")\n","\n","# Create a filtered retriever based on risk profile\n","risk_filter = {\"risk_profile\": {\"$eq\": \"conservative\"}}\n","print(f\"Using risk profile filter: {risk_filter}\")\n","\n","risk_filtered_retriever = metadata_vectorstore.as_retriever(\n","    search_kwargs={\"k\": 2, \"filter\": risk_filter}\n",")\n","\n","# Retrieve documents matching the age filter\n","print(\"\\nRetrieving documents with AGE filtering (senior):\")\n","age_results = age_filtered_retriever.get_relevant_documents(\"investment strategies\")\n","for i, doc in enumerate(age_results, 1):\n","    print(f\"Document {i}:\")\n","    print(f\"Content: {doc.page_content}\")\n","    print(f\"Metadata: {doc.metadata}\\n\")\n","\n","# Retrieve documents matching the risk filter\n","print(\"\\nRetrieving documents with RISK PROFILE filtering (conservative):\")\n","risk_results = risk_filtered_retriever.get_relevant_documents(\"investment strategies\")\n","for i, doc in enumerate(risk_results, 1):\n","    print(f\"Document {i}:\")\n","    print(f\"Content: {doc.page_content}\")\n","    print(f\"Metadata: {doc.metadata}\\n\")\n","\n","# For comparison, retrieve without filtering\n","print(\"Retrieving without filtering:\")\n","unfiltered_results = metadata_vectorstore.as_retriever(search_kwargs={\"k\": 2}).get_relevant_documents(\"investment strategies\")\n","for i, doc in enumerate(unfiltered_results, 1):\n","    print(f\"Document {i}:\")\n","    print(f\"Content: {doc.page_content}\")\n","    print(f\"Metadata: {doc.metadata}\\n\")"],"metadata":{"id":"vtLJIxOZugWb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Personalized Vector Stores (Simulated)**"],"metadata":{"id":"H_qmwRSovPud"}},{"cell_type":"code","source":["# Create some additional personal documents for the user\n","personal_documents = [\n","    Document(\n","        page_content=\"Based on our last portfolio review, we agreed to allocate 40% to bonds, 30% to dividend stocks, 20% to REITs, and 10% to cash reserves, given your conservative approach and income needs in retirement.\",\n","        metadata={\"source\": \"personal_financial_plan.pdf\", \"type\": \"personal\", \"user_id\": \"user_123\"}\n","    ),\n","    Document(\n","        page_content=\"Your recent health check showed improved blood pressure (130/82) following medication adjustment, but continued monitoring of blood glucose levels is recommended for your type 2 diabetes management.\",\n","        metadata={\"source\": \"health_summary.pdf\", \"type\": \"personal\", \"user_id\": \"user_123\"}\n","    )\n","]\n","\n","# Simulate a user-specific vector store by combining general and personal documents\n","def create_personalized_vectorstore(user_id, general_docs, personal_docs):\n","    \"\"\"Create a personalized vector store for a specific user.\"\"\"\n","    print(f\"Creating vector store for user {user_id}\")\n","\n","    # Combine general documents with user-specific ones\n","    all_docs = general_docs + [\n","        doc for doc in personal_docs\n","        if doc.metadata.get(\"user_id\") == user_id\n","    ]\n","\n","    # Create a new vector store\n","    user_vectorstore = Chroma.from_documents(\n","        documents=all_docs,\n","        embedding=embeddings\n","    )\n","\n","    print(f\"Created personalized vector store with {len(all_docs)} documents\")\n","    return user_vectorstore\n","\n","# Create a personalized vector store\n","user_id = \"user_123\"\n","user_vectorstore = create_personalized_vectorstore(\n","    user_id,\n","    documents_with_metadata,\n","    personal_documents\n",")\n","\n","# Create a retriever from the user's store\n","user_retriever = user_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","\n","# Test personalized retrieval\n","print(\"\\nPersonalized retrieval results:\")\n","personal_results = user_retriever.get_relevant_documents(\"What investment strategy should I follow based on my profile?\")\n","for i, doc in enumerate(personal_results, 1):\n","    print(f\"Document {i}:\")\n","    print(f\"Content: {doc.page_content}\")\n","    print(f\"Metadata: {doc.metadata}\\n\")\n","\n","# Demonstrate how this affects RAG responses\n","personalized_qa = RetrievalQA.from_chain_type(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n","    chain_type=\"stuff\",\n","    retriever=user_retriever\n",")\n","\n","print(\"Using a personalized RAG system:\")\n","personalized_response = personalized_qa.invoke({\"query\": \"What investment strategy should I follow?\"})\n","print(personalized_response[\"result\"])"],"metadata":{"id":"NBhhIc-fvP4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Best Practices and Considerations**"],"metadata":{"id":"LlD8o_Dzvdzh"}},{"cell_type":"code","source":["print(\"\"\"\n","## Best Practices for Interactive Q&A Systems with RAG\n","\n","### Chat History Management\n","1. Limit history size to fit within context windows (typically keep 5-10 exchanges)\n","2. Consider summary approaches for longer conversations\n","3. Store timestamps for multi-session interactions\n","4. Prioritize recent exchanges over older ones\n","5. Consider user privacy when storing conversation history\n","\n","### Streaming Implementation\n","1. Always provide visual feedback during document retrieval\n","2. Monitor connection status to detect disconnections\n","3. Implement timeouts for long-running queries\n","4. Include progress indicators for multi-step processes\n","5. Consider chunking very long responses\n","\n","### Source Attribution\n","1. Make citations clickable or expandable in user interfaces\n","2. Include complete metadata for verification\n","3. Balance citation density with readability\n","4. Handle conflicting information transparently\n","5. Consider confidence levels for different sources\n","\n","### Personalization\n","1. Start with simple query augmentation before complex personalization\n","2. Always get explicit consent for using personal data\n","3. Implement strict access controls for user-specific data\n","4. Be transparent about how personalization affects results\n","5. Provide options to disable personalization\n","\n","### Performance Considerations\n","1. Cache common query results to improve response time\n","2. Use async operations for resource-intensive processes\n","3. Implement fallback mechanisms for when retrieval or generation fails\n","4. Consider rate limiting to prevent resource exhaustion\n","5. Monitor token usage to stay within API limits\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4IqN25rvd83","executionInfo":{"status":"ok","timestamp":1743601417203,"user_tz":-330,"elapsed":5,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}},"outputId":"667000d8-893b-41d4-9374-0b96a6a36d06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","## Best Practices for Interactive Q&A Systems with RAG\n","\n","### Chat History Management\n","1. Limit history size to fit within context windows (typically keep 5-10 exchanges)\n","2. Consider summary approaches for longer conversations\n","3. Store timestamps for multi-session interactions\n","4. Prioritize recent exchanges over older ones\n","5. Consider user privacy when storing conversation history\n","\n","### Streaming Implementation\n","1. Always provide visual feedback during document retrieval\n","2. Monitor connection status to detect disconnections\n","3. Implement timeouts for long-running queries\n","4. Include progress indicators for multi-step processes\n","5. Consider chunking very long responses\n","\n","### Source Attribution\n","1. Make citations clickable or expandable in user interfaces\n","2. Include complete metadata for verification\n","3. Balance citation density with readability\n","4. Handle conflicting information transparently\n","5. Consider confidence levels for different sources\n","\n","### Personalization\n","1. Start with simple query augmentation before complex personalization\n","2. Always get explicit consent for using personal data\n","3. Implement strict access controls for user-specific data\n","4. Be transparent about how personalization affects results\n","5. Provide options to disable personalization\n","\n","### Performance Considerations\n","1. Cache common query results to improve response time\n","2. Use async operations for resource-intensive processes\n","3. Implement fallback mechanisms for when retrieval or generation fails\n","4. Consider rate limiting to prevent resource exhaustion\n","5. Monitor token usage to stay within API limits\n","\n"]}]}]}