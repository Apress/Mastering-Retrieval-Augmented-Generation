{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNScmKexac5rpSLyr3uBq7l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"h29Gtx6XFQ9x"},"outputs":[],"source":["# Install required packages\n","!pip install pandas numpy chardet xmltodict\n","\n","# Import necessary libraries\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import chardet\n","import xmltodict\n","from typing import List, Dict, Any\n","from pathlib import Path\n","from datetime import datetime\n","\n","print(\"Structured data processing environment setup complete!\")"]},{"cell_type":"markdown","source":["**CSV Files: The Workhorses of Data Analysis**"],"metadata":{"id":"1_6b3QpSJrjk"}},{"cell_type":"code","source":["class EnhancedCSVLoader:\n","    \"\"\"\n","    A comprehensive CSV loader for RAG systems.\n","    Handles encoding detection, delimiter inference, and data validation.\n","    \"\"\"\n","    def __init__(self, file_path: str):\n","        self.file_path = file_path\n","        self.metadata = {}\n","        self.encoding = None\n","        self.delimiter = None\n","\n","    def detect_file_properties(self) -> dict:\n","        \"\"\"\n","        Automatically detect CSV file properties including\n","        encoding and delimiter.\n","        \"\"\"\n","        with open(self.file_path, 'rb') as file:\n","            # Read a sample of the file\n","            raw_data = file.read(10000)  # Read first 10KB\n","\n","            # Detect encoding\n","            result = chardet.detect(raw_data)\n","            self.encoding = result['encoding']\n","\n","            # Detect delimiter\n","            sample_text = raw_data.decode(self.encoding)\n","            delimiters = [',', ';', '\\t', '|']\n","            delimiter_counts = {d: sample_text.count(d) for d in delimiters}\n","            self.delimiter = max(delimiter_counts.items(), key=lambda x: x[1])[0]\n","\n","            return {\n","                'encoding': self.encoding,\n","                'delimiter': self.delimiter,\n","                'confidence': result['confidence']\n","            }\n","\n","    def validate_data(self, df: pd.DataFrame) -> dict:\n","       \"\"\"\n","       Perform validation checks on the loaded data.\n","       Returns a dictionary of validation results.\n","       \"\"\"\n","       validation = {\n","           'total_rows': len(df),\n","           'total_columns': len(df.columns),\n","           'missing_values': df.isnull().sum().to_dict(),\n","           'column_types': df.dtypes.astype(str).to_dict(),\n","           'duplicate_rows': df.duplicated().sum()\n","       }\n","\n","       # Check for common data quality issues\n","       validation['warnings'] = []\n","\n","       # Check for high percentage of missing values\n","       missing_percentages = (df.isnull().sum() / len(df)) * 100\n","       for column, pct in missing_percentages.items():\n","           if pct > 20:\n","               validation['warnings'].append(\n","                   f\"Column '{column}' has {pct:.1f}% missing values\"\n","               )\n","\n","       # Check for mixed data types\n","       for column in df.columns:\n","           if df[column].dtype == 'object':\n","               try:\n","                   pd.to_numeric(df[column], errors='raise')\n","                   validation['warnings'].append(\n","                       f\"Column '{column}' contains mixed numeric and non-numeric values\"\n","                   )\n","               except:\n","                   pass\n","\n","       return validation\n","\n","    def load(self) -> tuple[pd.DataFrame, dict]:\n","       \"\"\"\n","       Load and process the CSV file.\n","       Returns both the data and metadata about the file and its contents.\n","       \"\"\"\n","       try:\n","           # Detect file properties\n","           properties = self.detect_file_properties()\n","\n","           # Read the CSV file\n","           df = pd.read_csv(\n","               self.file_path,\n","               encoding=self.encoding,\n","               delimiter=self.delimiter,\n","               on_bad_lines='warn'\n","           )\n","\n","           # Validate the data\n","           validation_results = self.validate_data(df)\n","\n","           # Collect metadata\n","           self.metadata = {\n","               'file_properties': properties,\n","               'validation': validation_results,\n","               'file_size': os.path.getsize(self.file_path),\n","               'last_modified': os.path.getmtime(self.file_path)\n","           }\n","\n","           return df, self.metadata\n","\n","       except Exception as e:\n","           print(f\"Error loading CSV: {str(e)}\")\n","           return None, None"],"metadata":{"id":"-ic9oCgtF7PI","executionInfo":{"status":"ok","timestamp":1739164809562,"user_tz":-330,"elapsed":97,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def test_csv_loader():\n","    \"\"\"Test the CSV loader with sample data\"\"\"\n","\n","    # Create a test CSV file with various data scenarios\n","    sample_data = \"\"\"\n","name,age,city,salary\n","John Doe,30,New York,75000\n","Jane Smith,25,Los Angeles,82000\n","Bob Johnson,,Chicago,68000\n","Alice Brown,35,Houston,91000.5\n","\"\"\"\n","\n","    # Ensure we have a directory for our samples\n","    os.makedirs('rag_sample_data', exist_ok=True)\n","\n","    # Write the sample data to a file\n","    with open('rag_sample_data/sample.csv', 'w') as f:\n","        f.write(sample_data.strip())\n","\n","    # Test our loader with the sample file\n","    loader = EnhancedCSVLoader('rag_sample_data/sample.csv')\n","    df, metadata = loader.load()\n","\n","    # Convert numpy types to Python native types for JSON serialization\n","    metadata['validation']['total_rows'] = int(metadata['validation']['total_rows'])\n","    metadata['validation']['total_columns'] = int(metadata['validation']['total_columns'])\n","    metadata['validation']['duplicate_rows'] = int(metadata['validation']['duplicate_rows'])\n","\n","    # Display the results\n","    print(\"Loaded Data Preview:\")\n","    print(df.head())\n","\n","    print(\"\\nMetadata and Validation Results:\")\n","    print(json.dumps(metadata, indent=2))\n","\n","# Create sample directory and run the test\n","os.makedirs('rag_sample_data', exist_ok=True)\n","test_csv_loader()"],"metadata":{"id":"RHatngjmJJEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Directory Structures: Managing Document Collections**"],"metadata":{"id":"jYwM1ANEJucl"}},{"cell_type":"code","source":["class DocumentCollectionManager:\n","    \"\"\"\n","    Manages collections of documents organized in directories.\n","    Supports various file types and maintains a searchable index.\n","    \"\"\"\n","    def __init__(self, root_dir: str):\n","        self.root_dir = root_dir\n","        self.file_index = {}\n","        self.metadata = {}\n","\n","    def _count_file_types(self) -> dict:\n","        \"\"\"Count the number of files of each type.\"\"\"\n","        type_counts = {}\n","        for file_info in self.file_index.values():\n","            ext = file_info['extension']\n","            type_counts[ext] = type_counts.get(ext, 0) + 1\n","        return type_counts\n","\n","    def _calculate_max_depth(self) -> int:\n","        \"\"\"Calculate the maximum directory depth.\"\"\"\n","        max_depth = 0\n","        for file_path in self.file_index:\n","            relative_path = os.path.relpath(file_path, self.root_dir)\n","            depth = len(relative_path.split(os.sep))\n","            max_depth = max(max_depth, depth)\n","        return max_depth\n","\n","    def scan_directory(self) -> dict:\n","        \"\"\"\n","        Scan the directory structure and catalog all documents.\n","        Returns information about the document collection.\n","        \"\"\"\n","        for root, dirs, files in os.walk(self.root_dir):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                file_ext = os.path.splitext(file)[1].lower()\n","\n","                # Get file metadata\n","                stats = os.stat(file_path)\n","\n","                # Store file information\n","                self.file_index[file_path] = {\n","                    'extension': file_ext,\n","                    'size': stats.st_size,\n","                    'modified': datetime.fromtimestamp(stats.st_mtime).isoformat(),\n","                    'relative_path': os.path.relpath(file_path, self.root_dir)\n","                }\n","\n","        # Collect collection statistics\n","        self.metadata = {\n","            'total_files': len(self.file_index),\n","            'file_types': self._count_file_types(),\n","            'total_size': sum(f['size'] for f in self.file_index.values()),\n","            'directory_depth': self._calculate_max_depth()\n","        }\n","\n","        return self.metadata"],"metadata":{"id":"aY0SRe-ULAZo","executionInfo":{"status":"ok","timestamp":1739164831036,"user_tz":-330,"elapsed":13,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def test_document_manager():\n","    \"\"\"Test the document collection manager\"\"\"\n","\n","    # Create a test directory structure\n","    base_dir = 'rag_sample_data/documents'\n","    os.makedirs(f'{base_dir}/texts', exist_ok=True)\n","    os.makedirs(f'{base_dir}/pdfs', exist_ok=True)\n","\n","    # Create some sample files\n","    with open(f'{base_dir}/texts/doc1.txt', 'w') as f:\n","        f.write(\"Sample text document\")\n","    with open(f'{base_dir}/pdfs/doc2.pdf', 'w') as f:\n","        f.write(\"Sample PDF content\")\n","\n","    # Test the manager\n","    manager = DocumentCollectionManager(base_dir)\n","    metadata = manager.scan_directory()\n","\n","    print(\"Directory Analysis:\")\n","    print(json.dumps(metadata, indent=2))\n","\n","    print(\"\\nFile Index:\")\n","    for path, info in manager.file_index.items():\n","        print(f\"\\nFile: {path}\")\n","        print(json.dumps(info, indent=2))\n","\n","# Run the test\n","test_document_manager()"],"metadata":{"id":"Jqjgcpa4LB-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**JSON and XML: Handling Hierarchical Data**"],"metadata":{"id":"fiS7Hz7HM8qE"}},{"cell_type":"code","source":["import json\n","import xmltodict\n","from typing import Union, Any\n","\n","class HierarchicalDataLoader:\n","    \"\"\"\n","    Handles both JSON and XML files with support for schema validation\n","    and data transformation.\n","    \"\"\"\n","    def __init__(self, file_path: str):\n","        self.file_path = file_path\n","        self.file_type = os.path.splitext(file_path)[1].lower()\n","        self.metadata = {}\n","\n","    def _load_json(self) -> tuple[dict, dict]:\n","        \"\"\"Load and parse JSON files with error handling.\"\"\"\n","        try:\n","            with open(self.file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","\n","            # Extract metadata about the JSON structure\n","            self.metadata = {\n","                'depth': self._calculate_json_depth(data),\n","                'keys': self._extract_json_keys(data),\n","                'size': os.path.getsize(self.file_path)\n","            }\n","\n","            return data, self.metadata\n","\n","        except json.JSONDecodeError as e:\n","            print(f\"Error parsing JSON: {str(e)}\")\n","            return None, None\n","\n","    def _calculate_json_depth(self, obj: Any, current_depth: int = 1) -> int:\n","        \"\"\"Calculate the maximum depth of a JSON object.\"\"\"\n","        if isinstance(obj, dict):\n","            if not obj:\n","                return current_depth\n","            return max(\n","                self._calculate_json_depth(value, current_depth + 1)\n","                for value in obj.values()\n","            )\n","        elif isinstance(obj, list):\n","            if not obj:\n","                return current_depth\n","            return max(\n","                self._calculate_json_depth(item, current_depth + 1)\n","                for item in obj\n","            )\n","        return current_depth\n","\n","    def _load_xml(self) -> tuple[dict, dict]:\n","        \"\"\"Load and parse XML files with error handling.\"\"\"\n","        try:\n","            with open(self.file_path, 'r', encoding='utf-8') as f:\n","                data = xmltodict.parse(f.read())\n","\n","            # Extract metadata about the XML structure\n","            self.metadata = {\n","                'root_tag': list(data.keys())[0],\n","                'size': os.path.getsize(self.file_path),\n","                'depth': self._calculate_xml_depth(data)\n","            }\n","\n","            return data, self.metadata\n","\n","        except Exception as e:\n","            print(f\"Error parsing XML: {str(e)}\")\n","            return None, None\n","\n","    def _extract_json_keys(self, obj: Any, prefix: str = '') -> list:\n","        \"\"\"Extract all keys from a JSON object with their full paths.\"\"\"\n","        keys = []\n","        if isinstance(obj, dict):\n","            for key, value in obj.items():\n","                full_key = f\"{prefix}.{key}\" if prefix else key\n","                keys.append(full_key)\n","                if isinstance(value, (dict, list)):\n","                    keys.extend(self._extract_json_keys(value, full_key))\n","        elif isinstance(obj, list):\n","            for i, item in enumerate(obj):\n","                full_key = f\"{prefix}[{i}]\"\n","                if isinstance(item, (dict, list)):\n","                    keys.extend(self._extract_json_keys(item, full_key))\n","        return keys"],"metadata":{"id":"u1L99cuFM-pe","executionInfo":{"status":"ok","timestamp":1739164994148,"user_tz":-330,"elapsed":7,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def test_hierarchical_loader():\n","    \"\"\"Test the hierarchical data loader\"\"\"\n","\n","    # Create sample JSON file\n","    json_data = {\n","        \"users\": [\n","            {\n","                \"id\": 1,\n","                \"name\": \"John Doe\",\n","                \"address\": {\n","                    \"city\": \"New York\",\n","                    \"country\": \"USA\"\n","                }\n","            }\n","        ]\n","    }\n","\n","    with open('rag_sample_data/sample.json', 'w') as f:\n","        json.dump(json_data, f)\n","\n","    # Create sample XML file\n","    xml_data = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n","    <root>\n","        <users>\n","            <user>\n","                <id>1</id>\n","                <name>John Doe</name>\n","                <address>\n","                    <city>New York</city>\n","                    <country>USA</country>\n","                </address>\n","            </user>\n","        </users>\n","    </root>\n","    \"\"\"\n","\n","    with open('rag_sample_data/sample.xml', 'w') as f:\n","        f.write(xml_data)\n","\n","    # Test both formats\n","    json_loader = HierarchicalDataLoader('rag_sample_data/sample.json')\n","    json_content, json_metadata = json_loader._load_json()\n","\n","    print(\"JSON Processing Results:\")\n","    print(\"\\nContent:\")\n","    print(json.dumps(json_content, indent=2))\n","    print(\"\\nMetadata:\")\n","    print(json.dumps(json_metadata, indent=2))\n","\n","    xml_loader = HierarchicalDataLoader('rag_sample_data/sample.xml')\n","    xml_content, xml_metadata = xml_loader._load_xml()\n","\n","    print(\"\\nXML Processing Results:\")\n","    print(\"\\nContent:\")\n","    print(json.dumps(xml_content, indent=2))\n","    print(\"\\nMetadata:\")\n","    print(json.dumps(xml_metadata, indent=2))\n","\n","# Run the test\n","test_hierarchical_loader()"],"metadata":{"id":"FsBUb-JoODOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Bringing It All Together**"],"metadata":{"id":"rnWmuh7iTBaz"}},{"cell_type":"code","source":["class UnifiedDataProcessor:\n","    \"\"\"\n","    A comprehensive processor that handles CSV, JSON, XML,\n","    and directory structures in a unified way.\n","    \"\"\"\n","    def __init__(self, base_dir: str):\n","        self.base_dir = base_dir\n","        self.collection_manager = DocumentCollectionManager(base_dir)\n","        self.data_cache = {}\n","\n","    def process_document(self, file_path: str) -> tuple[Any, dict]:\n","        \"\"\"\n","        Process any supported document type and return its content\n","        and metadata.\n","        \"\"\"\n","        file_ext = os.path.splitext(file_path)[1].lower()\n","\n","        if file_ext == '.csv':\n","            loader = EnhancedCSVLoader(file_path)\n","            return loader.load()\n","\n","        elif file_ext in ['.json', '.xml']:\n","            loader = HierarchicalDataLoader(file_path)\n","            if file_ext == '.json':\n","                return loader._load_json()\n","            else:\n","                return loader._load_xml()\n","        else:\n","            raise ValueError(f\"Unsupported file type: {file_ext}\")\n","\n","    def build_knowledge_base(self) -> dict:\n","        \"\"\"\n","        Scan the directory and process all supported documents\n","        into a unified knowledge base.\n","        \"\"\"\n","        # Scan directory structure\n","        collection_metadata = self.collection_manager.scan_directory()\n","\n","        # Process each file\n","        for file_path in self.collection_manager.file_index:\n","            try:\n","                content, metadata = self.process_document(file_path)\n","                if content is not None:\n","                    self.data_cache[file_path] = {\n","                        'content': content,\n","                        'metadata': metadata\n","                    }\n","            except Exception as e:\n","                print(f\"Error processing {file_path}: {str(e)}\")\n","\n","        return {\n","            'collection_metadata': collection_metadata,\n","            'processed_files': len(self.data_cache),\n","            'total_files': len(self.collection_manager.file_index)\n","        }"],"metadata":{"id":"jcFvZiJkTDep","executionInfo":{"status":"ok","timestamp":1739165010710,"user_tz":-330,"elapsed":3,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from json import JSONEncoder\n","\n","class CustomJSONEncoder(JSONEncoder):\n","    \"\"\"Custom JSON encoder to handle numpy and other special types.\"\"\"\n","    def default(self, obj):\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        return super().default(obj)\n","\n","def test_unified_processor():\n","    \"\"\"Test the unified data processor with multiple file types\"\"\"\n","\n","    # Create test files of different types\n","    os.makedirs('rag_sample_data/unified_test', exist_ok=True)\n","\n","    # Create a CSV file\n","    with open('rag_sample_data/unified_test/data.csv', 'w') as f:\n","        f.write(\"name,age\\nJohn,30\\nJane,25\")\n","\n","    # Create a JSON file\n","    with open('rag_sample_data/unified_test/config.json', 'w') as f:\n","        json.dump({\"setting1\": \"value1\", \"setting2\": 42}, f)\n","\n","    # Create an XML file\n","    with open('rag_sample_data/unified_test/data.xml', 'w') as f:\n","        f.write('<root><item>Test</item></root>')\n","\n","    # Test the unified processor\n","    processor = UnifiedDataProcessor('rag_sample_data/unified_test')\n","    results = processor.build_knowledge_base()\n","\n","    print(\"Unified Processing Results:\")\n","    print(json.dumps(results, indent=2, cls=CustomJSONEncoder))\n","\n","    print(\"\\nProcessed Documents:\")\n","    for path, data in processor.data_cache.items():\n","        print(f\"\\nFile: {path}\")\n","        print(\"Metadata:\", json.dumps(data['metadata'], indent=2, cls=CustomJSONEncoder))\n","\n","# Run the test\n","test_unified_processor()"],"metadata":{"id":"qDiIaJUdUpXV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Best Practices and Common Challenges**"],"metadata":{"id":"7UR197QqU8Ya"}},{"cell_type":"code","source":["def validate_structured_data(data: Any, format_type: str) -> dict:\n","    \"\"\"\n","    Comprehensive validation for structured data.\n","    Returns validation results and recommendations.\n","    \"\"\"\n","    validation_results = {\n","        'passed_checks': [],\n","        'warnings': [],\n","        'errors': [],\n","        'recommendations': []\n","    }\n","\n","    if format_type == 'csv':\n","        # Check for missing values\n","        if data.isnull().any().any():\n","            validation_results['warnings'].append(\n","                'Missing values detected. Consider imputation strategy.'\n","            )\n","\n","        # Check for data type consistency\n","        for column in data.columns:\n","            unique_types = data[column].apply(type).unique()\n","            if len(unique_types) > 1:\n","                validation_results['warnings'].append(\n","                    f'Mixed data types in column {column}'\n","                )\n","\n","    elif format_type in ['json', 'xml']:\n","        # Check for structural consistency\n","        validation_results.update(\n","            check_hierarchical_structure(data)\n","        )\n","\n","    return validation_results"],"metadata":{"id":"LmGCXQa6U-j8","executionInfo":{"status":"ok","timestamp":1739165274198,"user_tz":-330,"elapsed":6,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class StreamingDataProcessor:\n","    \"\"\"\n","    Process large structured data files in chunks to manage memory usage.\n","    \"\"\"\n","    def __init__(self, chunk_size: int = 1000):\n","        self.chunk_size = chunk_size\n","\n","    def process_large_csv(self, file_path: str, processor_func):\n","        \"\"\"Process a large CSV file in chunks.\"\"\"\n","        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):\n","            processed_chunk = processor_func(chunk)\n","            yield processed_chunk"],"metadata":{"id":"ZadOXw9DVCZs","executionInfo":{"status":"ok","timestamp":1739165289929,"user_tz":-330,"elapsed":3,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class StructuredDataLogger:\n","    \"\"\"Logging system for structured data processing.\"\"\"\n","    def __init__(self, log_path: str):\n","        self.log_path = log_path\n","        self.error_counts = {}\n","\n","    def log_processing_error(self, error_type: str, details: str):\n","        \"\"\"Log processing errors with context for debugging.\"\"\"\n","        timestamp = pd.Timestamp.now()\n","        with open(self.log_path, 'a') as f:\n","            f.write(f\"{timestamp} - {error_type}: {details}\\n\")\n","\n","        # Track error frequencies\n","        self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1"],"metadata":{"id":"AWmIpkXuVFkn","executionInfo":{"status":"ok","timestamp":1739165302225,"user_tz":-330,"elapsed":6,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":17,"outputs":[]}]}