{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBnEZEXcQo2547U3dzEbTh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Setup and Installation**"],"metadata":{"id":"BgTYo4XW-wFy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbPRqM50-fs-"},"outputs":[],"source":["!pip install -q langchain\n","!pip install -q langchain_community\n","!pip install -q langchain-openai\n","!pip install -q sentence-transformers\n","!pip install -q numpy\n","!pip install -q python-dotenv"]},{"cell_type":"markdown","source":["**Import required libraries**"],"metadata":{"id":"hth4Xhls-7lj"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from getpass import getpass\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.embeddings import HuggingFaceEmbeddings"],"metadata":{"id":"DHVOd6nt-60p","executionInfo":{"status":"ok","timestamp":1740233439329,"user_tz":-330,"elapsed":26,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Setting up API Keys**"],"metadata":{"id":"NrDtiQi0_0L9"}},{"cell_type":"code","source":["# Securely input your OpenAI API key\n","openai_api_key = getpass('Enter your OpenAI API key: ')\n","os.environ['OPENAI_API_KEY'] = openai_api_key"],"metadata":{"id":"UVekyhYK-62t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Working with OpenAI Embeddings**"],"metadata":{"id":"eRWZ1F_A__WG"}},{"cell_type":"code","source":["#Let's start by initializing and using OpenAI's embedding model:\n","\n","try:\n","    # Initialize the embedding model\n","    embeddings_model = OpenAIEmbeddings(\n","        model=\"text-embedding-3-large\",\n","        openai_api_key=openai_api_key\n","    )\n","    print(\"OpenAI Embeddings model initialized successfully!\")\n","except Exception as e:\n","    print(f\"Error initializing OpenAI Embeddings: {str(e)}\")\n","\n","\"\"\"### 2.1 Single Query Embedding\n","\n","Let's embed a single piece of text and examine its properties:\n","\"\"\"\n","\n","try:\n","    # Embed a single query\n","    query = \"What is machine learning?\"\n","    query_embedding = embeddings_model.embed_query(query)\n","\n","    # Examine the embedding\n","    print(f\"Query text: '{query}'\")\n","    print(f\"Embedding dimension: {len(query_embedding)}\")\n","    print(f\"First 5 values: {query_embedding[:5]}\")\n","    print(f\"Vector magnitude: {np.linalg.norm(query_embedding):.4f}\")\n","except Exception as e:\n","    print(f\"Error generating query embedding: {str(e)}\")\n","\n","# Multiple Document Embeddings: Now let's embed multiple documents:\n","\n","# Sample documents\n","documents = [\n","    \"Machine learning is a subset of artificial intelligence.\",\n","    \"Deep learning is a type of machine learning.\",\n","    \"Neural networks are used in deep learning.\",\n","    \"Python is a popular programming language.\"\n","]\n","\n","try:\n","    # Generate embeddings for all documents\n","    doc_embeddings = embeddings_model.embed_documents(documents)\n","\n","    print(f\"Number of documents: {len(doc_embeddings)}\")\n","    print(f\"Embedding dimension: {len(doc_embeddings[0])}\")\n","\n","    # Display first document and its embedding shape\n","    print(f\"\\nFirst document: '{documents[0]}'\")\n","    print(f\"Its embedding shape: {np.array(doc_embeddings[0]).shape}\")\n","except Exception as e:\n","    print(f\"Error generating document embeddings: {str(e)}\")\n","\n","# Computing Similarities:Let's implement cosine similarity to compare embeddings:\n","\n","def cosine_similarity(vec1, vec2):\n","    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n","    dot_product = np.dot(vec1, vec2)\n","    norm1 = np.linalg.norm(vec1)\n","    norm2 = np.linalg.norm(vec2)\n","    return dot_product / (norm1 * norm2)\n","\n","# Compare similarities between documents\n","try:\n","    print(\"Computing similarities between documents:\\n\")\n","    for i in range(len(documents)):\n","        for j in range(i+1, len(documents)):\n","            sim = cosine_similarity(doc_embeddings[i], doc_embeddings[j])\n","            print(f\"Similarity between:\\n'{documents[i]}' and\\n'{documents[j]}':\\n{sim:.4f}\\n\")\n","except Exception as e:\n","    print(f\"Error computing similarities: {str(e)}\")"],"metadata":{"id":"ki6TvfkwAD14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Using HuggingFace Embeddings**"],"metadata":{"id":"po_uG0aPAory"}},{"cell_type":"code","source":["# Now let's try using a different embedding model from HuggingFace:\n","\n","try:\n","    # Initialize HuggingFace embeddings\n","    hf_embeddings = HuggingFaceEmbeddings(\n","        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n","    )\n","    print(\"HuggingFace Embeddings model initialized successfully!\")\n","\n","    # Generate embeddings using HuggingFace\n","    hf_query_embedding = hf_embeddings.embed_query(query)\n","    hf_doc_embeddings = hf_embeddings.embed_documents(documents)\n","\n","    print(f\"\\nHuggingFace embedding dimension: {len(hf_query_embedding)}\")\n","except Exception as e:\n","    print(f\"Error working with HuggingFace embeddings: {str(e)}\")\n","\n","# Comparing Different Models: Let's compare the similarities computed by different models:\n","\n","def compare_similarities(doc1, doc2, model1, model2):\n","    \"\"\"Compare similarities computed by two different embedding models.\"\"\"\n","    try:\n","        # Get embeddings from both models\n","        emb1_1 = model1.embed_query(doc1)\n","        emb1_2 = model1.embed_query(doc2)\n","        emb2_1 = model2.embed_query(doc1)\n","        emb2_2 = model2.embed_query(doc2)\n","\n","        # Compute similarities\n","        sim1 = cosine_similarity(emb1_1, emb1_2)\n","        sim2 = cosine_similarity(emb2_1, emb2_2)\n","\n","        return sim1, sim2\n","    except Exception as e:\n","        print(f\"Error comparing similarities: {str(e)}\")\n","        return None, None\n","\n","# Compare two similar documents\n","doc1 = \"Machine learning is fascinating.\"\n","doc2 = \"I love studying artificial intelligence.\"\n","\n","try:\n","    openai_sim, hf_sim = compare_similarities(doc1, doc2, embeddings_model, hf_embeddings)\n","\n","    if openai_sim is not None and hf_sim is not None:\n","        print(f\"Documents being compared:\")\n","        print(f\"Doc1: '{doc1}'\")\n","        print(f\"Doc2: '{doc2}'\\n\")\n","        print(f\"OpenAI similarity: {openai_sim:.4f}\")\n","        print(f\"HuggingFace similarity: {hf_sim:.4f}\")\n","except Exception as e:\n","    print(f\"Error in comparison: {str(e)}\")"],"metadata":{"id":"hofmZ78NAupV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Best Practices Demo**"],"metadata":{"id":"kX6uYMw6AtC0"}},{"cell_type":"code","source":["#Let's demonstrate some best practices for working with embeddings:\n","\n","def batch_embed_documents(documents, embeddings_model, batch_size=5):\n","    \"\"\"Embed documents in batches with error handling.\"\"\"\n","    all_embeddings = []\n","\n","    for i in range(0, len(documents), batch_size):\n","        batch = documents[i:i + batch_size]\n","        try:\n","            batch_embeddings = embeddings_model.embed_documents(batch)\n","            all_embeddings.extend(batch_embeddings)\n","            print(f\"Processed batch {i//batch_size + 1}\")\n","        except Exception as e:\n","            print(f\"Error processing batch {i//batch_size + 1}: {str(e)}\")\n","            continue\n","\n","    return all_embeddings\n","\n","# Generate some sample documents\n","sample_docs = [f\"This is sample document number {i}\" for i in range(12)]\n","\n","try:\n","    # Process in batches\n","    print(\"\\nProcessing documents in batches:\")\n","    batch_embeddings = batch_embed_documents(sample_docs, embeddings_model)\n","    print(f\"\\nTotal number of embeddings generated: {len(batch_embeddings)}\")\n","except Exception as e:\n","    print(f\"Error in batch processing: {str(e)}\")\n","\n","\"\"\"## 5. Saving and Loading Embeddings\n","\n","Let's demonstrate how to save and load embeddings:\n","\"\"\"\n","\n","import pickle\n","import tempfile\n","import os\n","\n","def save_embeddings(embeddings, filepath):\n","    \"\"\"Save embeddings to a file.\"\"\"\n","    try:\n","        with open(filepath, 'wb') as f:\n","            pickle.dump(embeddings, f)\n","        print(f\"Embeddings saved to {filepath}\")\n","    except Exception as e:\n","        print(f\"Error saving embeddings: {str(e)}\")\n","\n","def load_embeddings(filepath):\n","    \"\"\"Load embeddings from a file.\"\"\"\n","    try:\n","        with open(filepath, 'rb') as f:\n","            embeddings = pickle.load(f)\n","        print(f\"Embeddings loaded from {filepath}\")\n","        return embeddings\n","    except Exception as e:\n","        print(f\"Error loading embeddings: {str(e)}\")\n","        return None\n","\n","# Create a temporary file to save embeddings\n","temp_filepath = os.path.join(tempfile.gettempdir(), 'embeddings.pkl')\n","\n","try:\n","    # Save embeddings\n","    save_embeddings(doc_embeddings, temp_filepath)\n","\n","    # Load embeddings\n","    loaded_embeddings = load_embeddings(temp_filepath)\n","\n","    if loaded_embeddings is not None:\n","        print(f\"\\nVerifying loaded embeddings:\")\n","        print(f\"Original embeddings shape: {np.array(doc_embeddings).shape}\")\n","        print(f\"Loaded embeddings shape: {np.array(loaded_embeddings).shape}\")\n","\n","        # Clean up\n","        os.remove(temp_filepath)\n","        print(f\"\\nTemporary file removed\")\n","except Exception as e:\n","    print(f\"Error in save/load demo: {str(e)}\")"],"metadata":{"id":"0aL0TF1MBN_q"},"execution_count":null,"outputs":[]}]}