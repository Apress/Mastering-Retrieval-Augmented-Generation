{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtVBr79IMcfi6f2xo61uJU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**## Setup**"],"metadata":{"id":"GjuMA0eO53xv"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"09tplQZw5sQN","executionInfo":{"status":"ok","timestamp":1747650137345,"user_tz":-330,"elapsed":4304,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"outputs":[],"source":["# Install required libraries\n","!pip install -q langchain langchain_openai pandas matplotlib openai faiss-cpu tiktoken tabulate langchain-community\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","from typing import Dict, List, Any, Optional\n","\n","# Set your OpenAI API key\n","os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your actual key\n","\n","# Import LangChain components\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.vectorstores import FAISS\n","from langchain_core.documents import Document\n","from langchain_core.output_parsers import StrOutputParser"]},{"cell_type":"markdown","source":["**## 11.5.1 Architectural Adaptations for CSV Data**"],"metadata":{"id":"Eym97Hxf6XxY"}},{"cell_type":"code","source":["class CSVQuestionAnsweringSystem:\n","    \"\"\"A Q&A system for CSV files.\"\"\"\n","\n","    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n","        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n","        self.dataframes = {}\n","        self.df_info = {}\n","\n","    def add_csv(self, file_path: str, name: str = None):\n","        if name is None:\n","            name = os.path.basename(file_path).split('.')[0]\n","\n","        df = pd.read_csv(file_path)\n","        self.dataframes[name] = df\n","\n","        info = self._create_dataframe_info(df)\n","        self.df_info[name] = info\n","\n","        print(f\"Added CSV '{name}' with {len(df)} rows and {len(df.columns)} columns\")\n","        return True\n","\n","    def _create_dataframe_info(self, df: pd.DataFrame) -> str:\n","        info = f\"Columns: {', '.join(df.columns.tolist())}\\n\"\n","        info += f\"Rows: {len(df)}\\n\"\n","        info += \"Sample data:\\n\"\n","        info += df.head(3).to_string()\n","        return info\n","\n","    def generate_answer(self, question: str, csv_names: List[str] = None) -> str:\n","        if csv_names is None:\n","            csv_names = list(self.dataframes.keys())\n","\n","        context = self._prepare_context(csv_names)\n","\n","        prompt = PromptTemplate.from_template(\n","            \"\"\"You are a data analyst. Answer this question using the CSV data provided.\n","\n","            DATA:\n","            {context}\n","\n","            QUESTION:\n","            {question}\n","\n","            ANSWER:\"\"\"\n","        )\n","\n","        chain = prompt | self.llm | StrOutputParser()\n","        return chain.invoke({\"context\": context, \"question\": question})\n","\n","    def _prepare_context(self, csv_names: List[str]) -> str:\n","        context = \"\"\n","        for name in csv_names:\n","            if name in self.dataframes:\n","                context += f\"=== {name} Dataset ===\\n{self.df_info[name]}\\n\\n\"\n","        return context"],"metadata":{"id":"zekp2JE-6X5d","executionInfo":{"status":"ok","timestamp":1747649886656,"user_tz":-330,"elapsed":13,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**## 11.5.2 Loading and Processing CSV Files Efficiently**"],"metadata":{"id":"EzDWzqke6fiy"}},{"cell_type":"code","source":["def load_csv_efficiently(file_path: str,\n","                       chunk_size: Optional[int] = None,\n","                       optimize_dtypes: bool = True) -> pd.DataFrame:\n","    \"\"\"Load a CSV file with performance optimizations.\"\"\"\n","    start_time = time.time()\n","\n","    # Sample the file to determine data types\n","    dtypes = None\n","    if optimize_dtypes:\n","        try:\n","            sample = pd.read_csv(file_path, nrows=1000)\n","\n","            # Optimize dtypes based on the sample\n","            dtypes = {}\n","            for col in sample.columns:\n","                # Convert object columns with few unique values to category\n","                if sample[col].dtype == 'object':\n","                    unique_pct = sample[col].nunique() / len(sample)\n","                    if unique_pct < 0.5:  # If less than 50% unique values\n","                        dtypes[col] = 'category'\n","                # Downcast integers and floats\n","                elif sample[col].dtype == 'int64':\n","                    dtypes[col] = 'int32'  # Use smaller integer type\n","                elif sample[col].dtype == 'float64':\n","                    dtypes[col] = 'float32'  # Use smaller float type\n","        except:\n","            dtypes = None\n","\n","    # Load the data\n","    if chunk_size is not None:\n","        # Load in chunks to reduce memory pressure\n","        chunks = []\n","        for chunk in pd.read_csv(file_path, chunksize=chunk_size, dtype=dtypes):\n","            chunks.append(chunk)\n","        df = pd.concat(chunks)\n","    else:\n","        # Load all at once\n","        df = pd.read_csv(file_path, dtype=dtypes)\n","\n","    print(f\"Loaded CSV in {time.time() - start_time:.2f} seconds\")\n","    return df\n","\n","def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Optimize a dataframe's memory usage by changing data types.\"\"\"\n","    # Start with a copy of the dataframe\n","    result = df.copy()\n","\n","    # Optimize integers\n","    for col in result.select_dtypes(include=['int']).columns:\n","        result[col] = pd.to_numeric(result[col], downcast='integer')\n","\n","    # Optimize floats\n","    for col in result.select_dtypes(include=['float']).columns:\n","        result[col] = pd.to_numeric(result[col], downcast='float')\n","\n","    # Convert string columns with low cardinality to category\n","    for col in result.select_dtypes(include=['object']).columns:\n","        num_unique = result[col].nunique()\n","        num_total = len(result)\n","        if num_unique / num_total < 0.5:  # If < 50% are unique values\n","            result[col] = result[col].astype('category')\n","\n","    # Calculate memory savings\n","    original_mem = df.memory_usage(deep=True).sum() / 1024**2  # MB\n","    optimized_mem = result.memory_usage(deep=True).sum() / 1024**2  # MB\n","\n","    print(f\"Memory reduced from {original_mem:.2f} MB to {optimized_mem:.2f} MB\")\n","    print(f\"Saved {original_mem - optimized_mem:.2f} MB ({(1 - optimized_mem/original_mem) * 100:.1f}%)\")\n","\n","    return result"],"metadata":{"id":"1HkUcBW86fs4","executionInfo":{"status":"ok","timestamp":1747649911905,"user_tz":-330,"elapsed":9,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**## 11.5.3 Implementing Search Mechanisms**"],"metadata":{"id":"MQTysGDD6wZ5"}},{"cell_type":"code","source":["class CSVSearchSystem:\n","    \"\"\"Search system for CSV data supporting both keyword and semantic search.\"\"\"\n","\n","    def __init__(self, embedding_model=\"text-embedding-3-small\"):\n","        \"\"\"Initialize the search system.\"\"\"\n","        self.embeddings = OpenAIEmbeddings(model=embedding_model)\n","        self.vector_store = None\n","        self.df = None\n","\n","    def index_dataframe(self, df: pd.DataFrame, chunk_size: int = 20) -> bool:\n","        \"\"\"Index a dataframe for searching.\"\"\"\n","        self.df = df\n","\n","        # Create text representations of chunks of the dataframe\n","        documents = []\n","\n","        for i in range(0, len(df), chunk_size):\n","            end_idx = min(i + chunk_size, len(df))\n","            chunk = df.iloc[i:end_idx]\n","\n","            # Create text representation of this chunk\n","            text = f\"Rows {i} to {end_idx-1}:\\n\"\n","            for _, row in chunk.iterrows():\n","                text += str(dict(row)) + \"\\n\"\n","\n","            # Create a document with metadata tracking the row indices\n","            doc = Document(\n","                page_content=text,\n","                metadata={\"start_idx\": i, \"end_idx\": end_idx-1}\n","            )\n","            documents.append(doc)\n","\n","        # Create vector store\n","        self.vector_store = FAISS.from_documents(documents, self.embeddings)\n","        print(f\"Indexed {len(documents)} chunks from dataframe\")\n","        return True\n","\n","    def semantic_search(self, query: str, k: int = 3) -> pd.DataFrame:\n","        \"\"\"Perform a semantic search on the dataframe.\"\"\"\n","        if self.vector_store is None or self.df is None:\n","            return pd.DataFrame({\"error\": [\"No dataframe has been indexed\"]})\n","\n","        # Perform the search\n","        results = self.vector_store.similarity_search(query, k=k)\n","\n","        # Gather all row indices from the results\n","        all_indices = []\n","        for doc in results:\n","            start_idx = doc.metadata[\"start_idx\"]\n","            end_idx = doc.metadata[\"end_idx\"]\n","            all_indices.extend(range(start_idx, end_idx + 1))\n","\n","        # Return the relevant rows\n","        return self.df.iloc[all_indices].copy()\n","\n","    def keyword_search(self, query: str, k: int = 10) -> pd.DataFrame:\n","        \"\"\"Perform a keyword-based search on the dataframe.\"\"\"\n","        if self.df is None:\n","            return pd.DataFrame({\"error\": [\"No dataframe has been indexed\"]})\n","\n","        # Split the query into keywords\n","        keywords = query.lower().split()\n","\n","        # Initialize match scores\n","        scores = pd.Series(0, index=self.df.index)\n","\n","        # Search for each keyword in each column\n","        for col in self.df.columns:\n","            # Convert column to string for searching\n","            col_str = self.df[col].astype(str).str.lower()\n","\n","            for keyword in keywords:\n","                # Increase score for each match\n","                matches = col_str.str.contains(keyword, na=False)\n","                scores = scores + matches\n","\n","        # Get the top k matches\n","        top_indices = scores.nlargest(k).index\n","        return self.df.loc[top_indices].copy()"],"metadata":{"id":"1Z9Eb4ha6wj_","executionInfo":{"status":"ok","timestamp":1747649939700,"user_tz":-330,"elapsed":2,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**## 11.5.4 Handling Large CSV Datasets**"],"metadata":{"id":"XxR1KWpi64jl"}},{"cell_type":"code","source":["def convert_csv_to_parquet(csv_path: str, parquet_path: str) -> Dict:\n","    \"\"\"Convert a CSV file to Parquet format for better performance.\"\"\"\n","    import pyarrow as pa\n","    import pyarrow.csv as csv\n","    import pyarrow.parquet as pq\n","\n","    start_time = time.time()\n","\n","    # Load CSV into a PyArrow Table\n","    table = csv.read_csv(csv_path)\n","\n","    # Write to Parquet\n","    pq.write_table(table, parquet_path)\n","\n","    # Calculate metrics\n","    csv_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n","    parquet_size_mb = os.path.getsize(parquet_path) / (1024 * 1024)\n","\n","    return {\n","        \"csv_size_mb\": csv_size_mb,\n","        \"parquet_size_mb\": parquet_size_mb,\n","        \"compression_ratio\": csv_size_mb / parquet_size_mb,\n","        \"conversion_time_seconds\": time.time() - start_time\n","    }\n","\n","def process_large_csv_in_batches(file_path: str,\n","                               batch_size: int = 10000,\n","                               process_func = None) -> List[Any]:\n","    \"\"\"Process a large CSV file in batches to manage memory.\"\"\"\n","    results = []\n","\n","    # Process the CSV in chunks\n","    for chunk in pd.read_csv(file_path, chunksize=batch_size):\n","        if process_func:\n","            # Apply the processing function\n","            batch_result = process_func(chunk)\n","            results.append(batch_result)\n","        else:\n","            # Default processing: just count rows\n","            results.append(len(chunk))\n","\n","    return results"],"metadata":{"id":"ujMyUYj264sQ","executionInfo":{"status":"ok","timestamp":1747649964832,"user_tz":-330,"elapsed":2,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**## 11.5.5 Memory and Performance Considerations**"],"metadata":{"id":"rzJX9ax06_gx"}},{"cell_type":"code","source":["def profile_operation(func, *args, **kwargs):\n","    \"\"\"Profile the performance of a function.\"\"\"\n","    import time\n","    import psutil\n","\n","    # Record start stats\n","    start_time = time.time()\n","    process = psutil.Process(os.getpid())\n","    start_memory = process.memory_info().rss / 1024 / 1024  # MB\n","\n","    # Execute function\n","    result = func(*args, **kwargs)\n","\n","    # Record end stats\n","    end_time = time.time()\n","    end_memory = process.memory_info().rss / 1024 / 1024  # MB\n","\n","    # Calculate metrics\n","    execution_time = end_time - start_time\n","    memory_increase = end_memory - start_memory\n","\n","    print(f\"Execution time: {execution_time:.2f} seconds\")\n","    print(f\"Memory increase: {memory_increase:.2f} MB\")\n","\n","    return result\n","\n","from functools import lru_cache\n","\n","@lru_cache(maxsize=100)\n","def cached_embedding_generation(text):\n","    \"\"\"Generate embeddings with caching.\"\"\"\n","    embeddings = OpenAIEmbeddings()\n","    return embeddings.embed_query(text)\n","\n","def batch_process_with_memory_monitoring(df, batch_size=1000, func=None):\n","    \"\"\"Process a dataframe in batches with memory monitoring.\"\"\"\n","    import gc\n","\n","    results = []\n","\n","    for i in range(0, len(df), batch_size):\n","        # Process batch\n","        batch = df.iloc[i:min(i+batch_size, len(df))]\n","\n","        if func:\n","            batch_result = func(batch)\n","        else:\n","            batch_result = len(batch)\n","\n","        results.append(batch_result)\n","\n","        # Force garbage collection after each batch\n","        del batch\n","        gc.collect()\n","\n","    return results"],"metadata":{"id":"zmdMwc5-6_pt","executionInfo":{"status":"ok","timestamp":1747649996849,"user_tz":-330,"elapsed":4,"user":{"displayName":"ranajoy bose","userId":"06328792273740913169"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["**Complete Example: Titanic Dataset Q&A System**"],"metadata":{"id":"l2DIgsRXDrET"}},{"cell_type":"code","source":["# Download the Titanic dataset\n","!wget -q https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv -O titanic.csv\n","\n","# Let's build a Q&A system for the Titanic dataset\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 1. Create our CSV QA system\n","qa_system = CSVQuestionAnsweringSystem()\n","qa_system.add_csv(\"titanic.csv\")\n","\n","# 2. Optimize the dataframe for memory efficiency\n","original_df = qa_system.dataframes[\"titanic\"]\n","optimized_df = optimize_dataframe_memory(original_df)\n","qa_system.dataframes[\"titanic\"] = optimized_df\n","\n","# 3. Create a search system\n","search_system = CSVSearchSystem()\n","search_system.index_dataframe(optimized_df)\n","\n","# 4. Let's try some searches\n","print(\"\\nKeyword search for 'first class female':\")\n","keyword_results = search_system.keyword_search(\"first class female\")\n","print(f\"Found {len(keyword_results)} matches\")\n","print(keyword_results.head(3))\n","\n","print(\"\\nSemantic search for 'wealthy women passengers':\")\n","semantic_results = search_system.semantic_search(\"wealthy women passengers\")\n","print(f\"Found {len(semantic_results)} matches\")\n","print(semantic_results.head(3))\n","\n","# 5. Let's visualize some of the search results\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=semantic_results, x='Survived')\n","plt.title('Survival Counts for \"wealthy women passengers\" search')\n","plt.show()\n","\n","# 6. Ask questions using the QA system\n","questions = [\n","    \"What was the survival rate for women compared to men?\",\n","    \"How many passengers were in each class?\",\n","    \"Were children more likely to survive than adults?\"\n","]\n","\n","for question in questions:\n","    print(f\"\\nQuestion: {question}\")\n","    answer = qa_system.generate_answer(question)\n","    print(f\"Answer: {answer}\")\n","\n","print(\"\\nCSV Q&A System demonstration complete!\")"],"metadata":{"id":"wONX5kiaDrP8"},"execution_count":null,"outputs":[]}]}